{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "440c38da",
   "metadata": {},
   "source": [
    "\n",
    "# Pipeline de Otimização de Hiperparâmetros com Diagnóstico de Overtuning\n",
    "\n",
    "Este notebook implementa um pipeline completo para otimização de hiperparâmetros (HPO) com validação cruzada e diagnóstico de overtuning/overfitting em modelos de regressão. Utiliza `BayesSearchCV` da biblioteca `scikit-optimize` e múltiplos algoritmos de regressão.\n",
    "\n",
    "**Objetivos:**\n",
    "- Monitorar rigorosamente o processo de HPO\n",
    "- Detectar overtuning/overfitting\n",
    "- Comparar desempenho de múltiplos algoritmos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c963d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "import importlib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "CV_FOLDS = 5\n",
    "SPLIT = 0.2\n",
    "METRIC_TO_OPT = \"r2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e360d5ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ALGO_CONFIGS2.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALGO_CONFIGS2.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      2\u001b[0m     ALGO_CONFIGS \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[1;32mc:\\Users\\CTVZ\\OneDrive - PETROBRAS\\Documents\\Pessoal\\Projetos IA\\pucrj\\cda_fma\\mvp-machine-learning-e-analytics\\mvp_sprint_02_FMA_2025\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ALGO_CONFIGS2.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"ALGO_CONFIGS2.json\", \"r\") as f:\n",
    "    ALGO_CONFIGS = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90fd1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"r2\": r2_score(y_true, y_pred),\n",
    "        \"rmse\": mean_squared_error(y_true, y_pred, squared=False),\n",
    "        \"rme\": mean_absolute_error(y_true, y_pred)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807faf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OvertuningMonitor:\n",
    "    def __init__(self, X_dev, y_dev, X_test, y_test, metric):\n",
    "        self.X_dev = X_dev\n",
    "        self.y_dev = y_dev\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.metric = metric\n",
    "        self.history = []\n",
    "\n",
    "    def __call__(self, optim_result):\n",
    "        best_params = optim_result.best_params_\n",
    "        model = optim_result.estimator.set_params(**best_params)\n",
    "        model.fit(self.X_dev, self.y_dev)\n",
    "        y_pred_dev = model.predict(self.X_dev)\n",
    "        y_pred_test = model.predict(self.X_test)\n",
    "        metrics_dev = compute_metrics(self.y_dev, y_pred_dev)\n",
    "        metrics_test = compute_metrics(self.y_test, y_pred_test)\n",
    "        self.history.append({\n",
    "            \"params\": best_params,\n",
    "            \"dev\": metrics_dev,\n",
    "            \"test\": metrics_test\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3bae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=SPLIT, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {}\n",
    "\n",
    "for algo_name, config in ALGO_CONFIGS.items():\n",
    "    print(f\"Executando HPO para: {algo_name}\")\n",
    "    model_class = config[\"model_class\"]\n",
    "    module_name, class_name = model_class.rsplit(\".\", 1)\n",
    "    model_cls = getattr(importlib.import_module(module_name), class_name)\n",
    "    model = model_cls(**config[\"default_params\"])\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    search_space = {f\"model__{k}\": v for k, v in config[\"search_space\"].items()}\n",
    "\n",
    "    monitor = OvertuningMonitor(X_dev, y_dev, X_test, y_test, METRIC_TO_OPT)\n",
    "\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=pipe,\n",
    "        search_spaces=search_space,\n",
    "        n_iter=20,\n",
    "        cv=CV_FOLDS,\n",
    "        scoring=METRIC_TO_OPT,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    opt.fit(X_dev, y_dev, callback=monitor)\n",
    "    results[algo_name] = monitor.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b3a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "box_data = []\n",
    "test_scores = {}\n",
    "\n",
    "for algo, history in results.items():\n",
    "    fold_scores = [h[\"dev\"][METRIC_TO_OPT] for h in history]\n",
    "    test_score = history[-1][\"test\"][METRIC_TO_OPT]\n",
    "    box_data.append(pd.DataFrame({\"score\": fold_scores, \"algo\": algo}))\n",
    "    test_scores[algo] = test_score\n",
    "\n",
    "box_df = pd.concat(box_data)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(x=\"algo\", y=\"score\", data=box_df)\n",
    "sns.scatterplot(x=list(test_scores.keys()), y=list(test_scores.values()), color=\"red\", label=\"Teste Final\", s=100)\n",
    "plt.title(\"Comparação de Métricas de Validação Cruzada vs Teste Final\")\n",
    "plt.ylabel(METRIC_TO_OPT.upper())\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fdamata/pucrj-machinelearning-mvp-ml/blob/main/mvp_sprint_02_fma_2025_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmYX3PLx58Jg"
      },
      "source": [
        "#### MVP Machine Learning & Analytics\n",
        "\n",
        "**Nome:** Fabiano da Mata Almeida<br>\n",
        "**Matrícula:** 4052025000952<br>\n",
        "**Dataset:** Pressão de Vapor da nafta.\n",
        "\n",
        "**Nota sobre confidencialidade e descaracterização dos dados:**  \n",
        "> Para garantir a confidencialidade e o respeito à privacidade, todos os dados utilizados neste estudo foram devidamente descaracterizados, não permitindo a identificação na sua unidade de medida original ou informações sensíveis.<br>\n",
        "O uso desse dataset segue as boas práticas de ética em ciência de dados, assegurando que nenhuma informação pessoal ou confidencial seja exposta durante as análises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv_3tkX4x3xx"
      },
      "source": [
        "# 1. Escopo, Objetivo e Definição do Problema\n",
        "\n",
        "## 1.1. Contexto do problema e objetivo\n",
        "\n",
        "O conjunto de dados **Pressão de Vapor da Nafta** contempla observações de uma corrente do processo de fracionamento do petróleo, contendo variáveis físico-químicas do processo e propriedades da corrente.\n",
        "\n",
        "A variável alvo representa a pressão exercida pelo vapor em equilíbrio com sua fase líquida a uma temperatura específica. Essa propriedade caracteriza a volatilidade da corrente, impactando requisitos de armazenamento, segurança, transporte e conformidade regulatória.\n",
        "\n",
        "**Objetivo:** Desenvolver um modelo preditivo para estimar a pressão de vapor da nafta com precisão, permitindo operação próxima ao limite superior da especificação sem comprometer a segurança.\n",
        "\n",
        "*Nota: O conjunto de dados foi previamente analisado na SPRINT de Análise de Dados e Boas Práticas, permitindo EDA simplificada.*\n",
        "\n",
        "## 1.2. Tipo de tarefa\n",
        "\n",
        "Estudo de **regressão** com dados tabulares originados de sensores industriais e ensaios laboratoriais, aplicado à engenharia de processos na formulação de gasolina automotiva.\n",
        "\n",
        "A tarefa envolve desenvolvimento de modelo supervisionado para predição contínua de propriedade físico-química crítica.\n",
        "\n",
        "## 1.3. Valor para o negócio/usuário\n",
        "\n",
        "**Ganho econômico:** A predição confiável permite incorporar frações mais pesadas do GLP à nafta, agregando valor significativo (GLP vale ~50% da gasolina por volume).\n",
        "\n",
        "**Aplicações operacionais:**\n",
        "- Otimização de parâmetros operacionais em tempo real\n",
        "- Integração em sistemas de controle preditivo multivariável (MPC)\n",
        "- Automação da gestão da propriedade\n",
        "\n",
        "**Beneficiários:** Engenharia de Processos, Operações Industriais e Segurança Operacional, através de maior eficiência, previsibilidade e controle de qualidade.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4z0p0pNx3xy"
      },
      "source": [
        "# 2. Reprodutibilidade e ambiente\n",
        "\n",
        "Esta seção consolida todas as importações de bibliotecas necessárias, definições das funções utilizadas e algumas configurações iniciais globais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSgFMjhAx3xy"
      },
      "outputs": [],
      "source": [
        "# Bibliotecas básicas\n",
        "import time, ast, sys, joblib, math, os, json, importlib\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import Normalize\n",
        "from matplotlib.cm import viridis\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.patches import Patch\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualização\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Análise estatística\n",
        "from scipy.stats import norm, kstest\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Machine Learning - Model Selection and Evaluation\n",
        "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, RobustScaler, Normalizer\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error, root_mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import clone\n",
        "from sklearn.linear_model import LinearRegression,Ridge, Lasso, ElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "!pip install -q catboost\n",
        "!pip install -q scikit-optimize\n",
        "\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import xgboost as xgb\n",
        "from lightgbm import LGBMRegressor\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from skopt.callbacks import VerboseCallback, TimerCallback, DeltaYStopper\n",
        "from skopt.plots import plot_objective, plot_convergence, plot_evaluations, plot_histogram, plot_objective_2D, plot_regret, plot_gaussian_process\n",
        "\n",
        "\n",
        "\n",
        "# Configuração para não exibir os warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Configurações de exibição\n",
        "pd.options.display.float_format = '{:.4f}'.format  # Define o formato de exibição dos números float no pandas para quatro casas decimais\n",
        "pd.set_option('display.expand_frame_repr', False)  # Não quebra a representação do dataframe\n",
        "np.set_printoptions(precision=8, suppress=True, floatmode='maxprec') # Define o formato de exibição dos números float no numpy para oito casas decimais e sem notação científica\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp05MO4nx3xz"
      },
      "source": [
        "## 2.1 Definições prévias do problema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82kzmWnHFkb6"
      },
      "outputs": [],
      "source": [
        "# Definição do problema e inicialização de variáveis\n",
        "PROBLEM_TYPE = \"regressao\"\n",
        "SEED = 42\n",
        "SPLIT = 0.3\n",
        "CV_FOLDS = 10\n",
        "METRIC_TO_OPT = 'r2'  # Pode ser 'r2', 'rmse', ou 'mae'\n",
        "\n",
        "MINIMIZE_METRICS = {'rmse', 'mae', 'mse', 'msle', 'rmsle', 'mape', 'poisson', 'gamma', 'max_error', 'medae'}\n",
        "MAXIMIZE_METRICS = {'r2', 'var', 'd2', 'explained_variance'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc9c7nyPx3x0"
      },
      "source": [
        "## 2.2 Dependências\n",
        "Eventual instalação de pacotes extras. <br>\n",
        "*Manter o projeto enxuto*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVTbt69Hx3x0"
      },
      "outputs": [],
      "source": [
        "# Exemplo: descomente o que precisar\n",
        "# !pip install -q scikit-learn imbalanced-learn xgboost lightgbm catboost optuna\n",
        "# !pip install -q pandas-profiling ydata-profiling\n",
        "# !pip install -q matplotlib seaborn plotly\n",
        "# !pip install -q statsmodels pmdarima"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLA-s7f8x3x0"
      },
      "source": [
        "## 2.3. Funções Python\n",
        "Definição de funções em Python. <br>\n",
        "*Essa é uma boa prática de programação que facilita a leitura, manutenção e evolução do seu projeto.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgbYmMMlx3x0"
      },
      "outputs": [],
      "source": [
        "# Declaração de funções\n",
        "\n",
        "def teste_n(df, column_name, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Executa o teste de Kolmogorov-Smirnov para verificar se uma coluna do DataFrame segue uma distribuição normal.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados.\n",
        "    column_name : str\n",
        "        Nome da coluna a ser testada quanto à normalidade.\n",
        "    alpha : float, opcional\n",
        "        Nível de significância para o teste. O padrão é 0.05 (5%).\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Normaliza os dados da coluna (subtrai a média e divide pelo desvio padrão).\n",
        "    - Aplica o teste de Kolmogorov-Smirnov comparando com uma distribuição normal padrão.\n",
        "    - Interpreta os resultados com base no p-valor e o nível de significância especificado.\n",
        "    - Exibe uma mensagem informando se a distribuição pode ser considerada normal ou não.\n",
        "\n",
        "    Retorno:\n",
        "    --------\n",
        "    tuple\n",
        "        Uma tupla contendo (estatística do teste, p-valor).\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    stat, p_valor = teste_n(df, 'pv_nafta')\n",
        "    stat, p_valor = teste_n(df, 'pv_nafta', alpha=0.01)  # Usando significância de 1%\n",
        "    \"\"\"\n",
        "    # Executar o teste de Kolmogorov-Smirnov - nesse caso em relação a uma distribuição normal\n",
        "    stat, p_valor = kstest((df[column_name] - np.mean(df[column_name])) / np.std(df[column_name], ddof=1), 'norm')\n",
        "\n",
        "    # Interpretar os resultados\n",
        "    if p_valor > alpha:\n",
        "        print(\"A amostra parece vir de uma distribuição normal (não podemos rejeitar a hipótese nula) p-valor:\", f\"{p_valor:.5f}\")\n",
        "    else:\n",
        "        print(\"A amostra não parece vir de uma distribuição normal (rejeitamos a hipótese nula) p-valor:\", f\"{p_valor:.5f}\")\n",
        "    return float(stat), float(p_valor)\n",
        "\n",
        "def calcula_corr(df):\n",
        "    \"\"\"\n",
        "    Calcula e visualiza a matriz de correlação absoluta entre as variáveis de um DataFrame.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados para cálculo da correlação.\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Remove valores NaN do DataFrame antes de calcular as correlações.\n",
        "    - Calcula a matriz de correlação absoluta entre todas as variáveis.\n",
        "    - Cria uma visualização interativa usando Plotly Express.\n",
        "    - Aplica uma escala de cores Viridis para representar a intensidade das correlações.\n",
        "    - Mostra os valores numéricos das correlações com duas casas decimais.\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    calcula_corr(df)\n",
        "    calcula_corr(df[selected_columns])  # Para um subconjunto de colunas\n",
        "    \"\"\"\n",
        "    df_corr = df.dropna().corr().abs()\n",
        "\n",
        "    fig = px.imshow(\n",
        "        img    = df_corr,\n",
        "        color_continuous_scale='Viridis',\n",
        "        width  = 900, # caso não esteja visualizando todas as variáveis, altere esse valor\n",
        "        height = 900, # caso não esteja visualizando todas as variáveis, altere esse valor\n",
        "        text_auto = \".2f\"  # Mostra os valores com 2 casas decimais\n",
        "    )\n",
        "\n",
        "    fig.update_traces(textfont_size=12)  # Altere o valor conforme desejado\n",
        "    fig.show()\n",
        "\n",
        "def pairplot_corr_hm(df, figsize=(12, 12), hist_bins=30, s=10, alpha=0.6):\n",
        "    \"\"\"\n",
        "    Cria um pairplot onde a cor dos pontos é baseada na correlação absoluta entre variáveis\n",
        "    usando uma paleta de cores Viridis.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas DataFrame\n",
        "        O DataFrame contendo os dados a serem plotados\n",
        "    figsize : tuple, opcional\n",
        "        Tamanho da figura (largura, altura) em polegadas\n",
        "    hist_bins : int, opcional\n",
        "        Número de bins para os histogramas na diagonal\n",
        "    s : int, opcional\n",
        "        Tamanho dos pontos nos gráficos de dispersão\n",
        "    alpha : float, opcional\n",
        "        Nível de transparência dos pontos (0-1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Obter a matriz de correlação absoluta\n",
        "    corr_matrix = df.corr().abs()\n",
        "\n",
        "    # Configurar a normalização de cores para a escala viridis\n",
        "    norm = Normalize(vmin=0, vmax=1)\n",
        "\n",
        "    # Obter as variáveis e o número de variáveis\n",
        "    variables = df.columns\n",
        "    n_vars = len(variables)\n",
        "\n",
        "    # Criar a figura e os subplots\n",
        "    fig, axes = plt.subplots(n_vars, n_vars, figsize=figsize)\n",
        "\n",
        "    # Ajustar o espaçamento entre os subplots\n",
        "    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
        "\n",
        "    # Criar os gráficos para cada par de variáveis\n",
        "    for i, var1 in enumerate(variables):\n",
        "        for j, var2 in enumerate(variables):\n",
        "            ax = axes[i, j]\n",
        "\n",
        "            # Remover os ticks dos eixos internos\n",
        "            if i < n_vars - 1:\n",
        "                ax.set_xticks([])\n",
        "            if j > 0:\n",
        "                ax.set_yticks([])\n",
        "\n",
        "            # Se estamos na diagonal, plotar histograma\n",
        "            if i == j:\n",
        "                ax.hist(df[var1], bins=hist_bins, alpha=0.7, color='darkblue')\n",
        "                ax.set_title(var1, fontsize=10)\n",
        "            else:\n",
        "                # Obter a correlação absoluta entre as variáveis\n",
        "                corr_val = corr_matrix.loc[var1, var2]\n",
        "\n",
        "                # Determinar a cor com base na correlação\n",
        "                color = viridis(norm(corr_val))\n",
        "\n",
        "                # Criar o gráfico de dispersão\n",
        "                ax.scatter(df[var2], df[var1], s=s, alpha=alpha, color=color)\n",
        "\n",
        "                # Adicionar a correlação como texto no gráfico\n",
        "                ax.text(0.05, 0.95, f'|ρ|: {corr_val:.2f}',\n",
        "                        transform=ax.transAxes, fontsize=8,\n",
        "                        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
        "\n",
        "    # Adicionar os nomes das variáveis apenas nos eixos externos\n",
        "    for i, var in enumerate(variables):\n",
        "        axes[n_vars-1, i].set_xlabel(var, fontsize=10)\n",
        "        axes[i, 0].set_ylabel(var, fontsize=10)\n",
        "\n",
        "    # Adicionar uma barra de cores para referência\n",
        "    cbar_ax = fig.add_axes([0.92, 0.3, 0.02, 0.4])  # [left, bottom, width, height]\n",
        "    cb = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=\"viridis\"), cax=cbar_ax)\n",
        "    cb.set_label('Correlação Absoluta |ρ|')\n",
        "\n",
        "    plt.suptitle('Pairplot com Cores Baseadas na Correlação Absoluta', fontsize=16)\n",
        "    plt.subplots_adjust(left=0.05, right=0.9, top=0.95, bottom=0.05, wspace=0.2, hspace=0.2)\n",
        "\n",
        "def calcula_vif(df,target):\n",
        "    \"\"\"\n",
        "    Calcula o Fator de Inflação da Variância (VIF) para identificar multicolinearidade em variáveis.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo apenas as variáveis independentes para as quais se deseja calcular o VIF.\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Adiciona uma constante ao DataFrame para o cálculo correto do VIF.\n",
        "    - Calcula o VIF para cada variável usando a função variance_inflation_factor.\n",
        "    - Ordena os resultados em ordem decrescente para identificar as variáveis mais problemáticas.\n",
        "    - Exibe os resultados das 15 variáveis com maior VIF.\n",
        "\n",
        "    # Retorno:\n",
        "    # --------\n",
        "    # pandas.DataFrame\n",
        "    #     DataFrame contendo as variáveis e seus respectivos valores VIF.\n",
        "\n",
        "    Interpretação:\n",
        "    --------------\n",
        "    - VIF = 1: Ausência de multicolinearidade\n",
        "    - 1 < VIF < 5: Multicolinearidade moderada\n",
        "    - 5 < VIF < 10: Multicolinearidade alta\n",
        "    - VIF > 10: Multicolinearidade muito alta (problemática)\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    vif_df = calcula_vif(df,target)\n",
        "    \"\"\"\n",
        "    # Remove a coluna da variável target, se existir\n",
        "    X = df.drop(columns=[target]) if target in df.columns else df.copy()\n",
        "    X_with_const = sm.add_constant(X)  # Adicionando uma constante\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"Variable\"] = X_with_const.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n",
        "\n",
        "    vif.set_index('Variable', inplace=True)\n",
        "    # Imprimir VIF em ordem decrescente\n",
        "    print(\"\\nVIF das variáveis (ordem decrescente):\\n\")\n",
        "    print(vif.query(\"Variable != 'const'\").sort_values(by='VIF', ascending=False).head(15).T)\n",
        "\n",
        "    # return vif\n",
        "\n",
        "def plot_boxplot_pdf(df, lower_lim=None, upper_lim=None, n_cols=4):\n",
        "    \"\"\"\n",
        "    Plota boxplot horizontal e PDF (histograma + curva normal) para todas as colunas numéricas do DataFrame.\n",
        "    O layout é de múltiplas linhas e n_cols colunas de subplots: para cada coluna, boxplot em cima, PDF embaixo.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados.\n",
        "    lower_lim : float, str ou None, opcional\n",
        "        Limite inferior do eixo x. Se None ou 'auto', usa o mínimo dos dados.\n",
        "    upper_lim : float, str ou None, opcional\n",
        "        Limite superior do eixo x. Se None ou 'auto', usa o máximo dos dados.\n",
        "    n_cols : int, opcional\n",
        "        Número de colunas no layout dos subplots. O padrão é 4.\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Para cada variável numérica, cria dois gráficos alinhados verticalmente:\n",
        "      - Um boxplot horizontal no topo para visualizar a distribuição e outliers\n",
        "      - Um histograma com curva normal teórica abaixo para visualizar a distribuição de frequência\n",
        "    - Adiciona linhas de referência nos gráficos (média, mediana, ±3σ)\n",
        "    - Permite ajustar os limites dos eixos manualmente ou automaticamente\n",
        "    - Remove automaticamente valores NaN antes de plotar\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    plot_boxplot_pdf(df)\n",
        "    plot_boxplot_pdf(df, upper_lim=100, lower_lim=0)  # Define limites fixos para todas as variáveis\n",
        "    plot_boxplot_pdf(df, n_cols=3)  # Altera o número de colunas no layout\n",
        "    \"\"\"\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    n_vars = len(numeric_cols)\n",
        "    n_rows = int(np.ceil(n_vars / n_cols))\n",
        "\n",
        "    # Dobrar a altura dos plots da PDF (segunda linha de cada variável)\n",
        "    height_ratios = []\n",
        "    for _ in range(n_rows):\n",
        "        height_ratios.extend([1, 4])  # boxplot:1, pdf:4\n",
        "\n",
        "    fig, axes = plt.subplots(\n",
        "        n_rows * 2,\n",
        "        n_cols,\n",
        "        figsize=(6 * n_cols, 5 * n_rows),\n",
        "        gridspec_kw={'height_ratios': height_ratios}\n",
        "    )\n",
        "\n",
        "    axes = np.array(axes).reshape(n_rows * 2, n_cols)\n",
        "\n",
        "    for idx, column in enumerate(numeric_cols):\n",
        "        row = (idx // n_cols) * 2\n",
        "        col = idx % n_cols\n",
        "        data = df[column].dropna()\n",
        "\n",
        "        # Use os valores reais dos dados para garantir que todos os outliers estejam visíveis\n",
        "        data_min = data.min()\n",
        "        data_max = data.max()\n",
        "\n",
        "        # Se upper_lim/lower_lim forem fornecidos, use-os, senão use os valores reais dos dados\n",
        "        if upper_lim is None or (isinstance(upper_lim, str) and upper_lim.lower() == 'auto'):\n",
        "            x_upper = data_max\n",
        "        else:\n",
        "            try:\n",
        "                x_upper = float(upper_lim)\n",
        "            except (ValueError, TypeError):\n",
        "                x_upper = data_max\n",
        "\n",
        "        if lower_lim is None or (isinstance(lower_lim, str) and lower_lim.lower() == 'auto'):\n",
        "            x_lower = data_min\n",
        "        else:\n",
        "            try:\n",
        "                x_lower = float(lower_lim)\n",
        "            except (ValueError, TypeError):\n",
        "                x_lower = data_min\n",
        "\n",
        "        # Para garantir que todos os pontos (inclusive outliers) sejam mostrados, defina os limites do eixo x\n",
        "        # um pouco além dos valores mínimos e máximos reais dos dados\n",
        "        margin = 0.02 * (data_max - data_min) if data_max > data_min else 1\n",
        "        xlim_lower = data_min - margin\n",
        "        xlim_upper = data_max + margin\n",
        "\n",
        "        # Boxplot\n",
        "        ax_box = axes[row, col]\n",
        "        ax_box.boxplot(data, vert=False, patch_artist=True, widths=0.5, showfliers=True)\n",
        "        ax_box.set_xlim(xlim_lower, xlim_upper)\n",
        "        ax_box.set_yticks([])\n",
        "        ax_box.set_xticklabels([])\n",
        "        ax_box.set_title(f'Boxplot de {column}')\n",
        "\n",
        "        # PDF (histograma + curva normal)\n",
        "        ax_pdf = axes[row + 1, col]\n",
        "        ax_pdf.hist(data, bins=30, color='lightblue', edgecolor='black', alpha=0.7, density=True, range=(xlim_lower, xlim_upper))\n",
        "\n",
        "        if len(data) > 1:\n",
        "            media = data.mean()\n",
        "            std = data.std()\n",
        "            x_grid = np.linspace(xlim_lower, xlim_upper, 200)\n",
        "            y_norm = norm.pdf(x_grid, media, std)\n",
        "            ax_pdf.plot(x_grid, y_norm, color='darkblue', lw=2, label='Normal')\n",
        "\n",
        "            mediana = data.median()\n",
        "            # Linhas estatísticas\n",
        "            ax_box.axvline(media, color='blue', linestyle='-', label=f'Média: {media:.1f}')\n",
        "            ax_box.axvline(mediana, color='green', linestyle='--', label=f'Mediana: {mediana:.1f}')\n",
        "            ax_box.axvline(media + 3*std, color='orange', linestyle=':', label=f'+3σ: {(media + 3*std):.1f}')\n",
        "            ax_box.axvline(media - 3*std, color='orange', linestyle=':', label=f'-3σ: {(media - 3*std):.1f}')\n",
        "\n",
        "            ax_pdf.axvline(media, color='blue', linestyle='-', label=f'Média: {media:.1f}')\n",
        "            ax_pdf.axvline(mediana, color='green', linestyle='--', label=f'Mediana: {mediana:.1f}')\n",
        "            ax_pdf.axvline(media + 3*std, color='orange', linestyle=':', label=f'+3σ: {(media + 3*std):.1f}')\n",
        "            ax_pdf.axvline(media - 3*std, color='orange', linestyle=':', label=f'-3σ: {(media - 3*std):.1f}')\n",
        "\n",
        "        ax_pdf.set_xlim(xlim_lower, xlim_upper)\n",
        "        ax_pdf.set_xlabel(column)\n",
        "        ax_pdf.set_ylabel('Densidade')\n",
        "        ax_pdf.set_title(f'PDF de {column}')\n",
        "        ax_pdf.legend(fontsize=8, loc='upper left')\n",
        "\n",
        "    # Remove subplots vazios\n",
        "    total_plots = n_rows * n_cols\n",
        "    for idx in range(n_vars, total_plots):\n",
        "        for r in [0, 1]:\n",
        "            fig.delaxes(axes[(idx // n_cols) * 2 + r, idx % n_cols])\n",
        "\n",
        "    plt.tight_layout(h_pad=2.5)\n",
        "    plt.show()\n",
        "\n",
        "def plot_boxplot_pdf_indiv(df, column, lower_lim=None, upper_lim=None):\n",
        "    \"\"\"\n",
        "    Plota boxplot horizontal e PDF (histograma + curva normal) para uma coluna numérica do DataFrame.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados.\n",
        "    column : str\n",
        "        Nome da coluna a ser plotada.\n",
        "    lower_lim : float, str ou None, opcional\n",
        "        Limite inferior do eixo x. Se None ou 'auto', usa o mínimo dos dados.\n",
        "    upper_lim : float, str ou None, opcional\n",
        "        Limite superior do eixo x. Se None ou 'auto', usa o máximo dos dados.\n",
        "    \"\"\"\n",
        "\n",
        "    data = df[column].dropna()\n",
        "    data_min = data.min()\n",
        "    data_max = data.max()\n",
        "\n",
        "    # Processamento dos limites\n",
        "    if upper_lim is None or (isinstance(upper_lim, str) and upper_lim.lower() == 'auto'):\n",
        "        x_upper = data_max\n",
        "    else:\n",
        "        try:\n",
        "            x_upper = float(upper_lim)\n",
        "        except (ValueError, TypeError):\n",
        "            x_upper = data_max\n",
        "\n",
        "    if lower_lim is None or (isinstance(lower_lim, str) and lower_lim.lower() == 'auto'):\n",
        "        x_lower = data_min\n",
        "    else:\n",
        "        try:\n",
        "            x_lower = float(lower_lim)\n",
        "        except (ValueError, TypeError):\n",
        "            x_lower = data_min\n",
        "\n",
        "    # Margem para visualização\n",
        "    margin = 0.02 * (data_max - data_min) if data_max > data_min else 1\n",
        "\n",
        "    # Usar os limites definidos pelo usuário quando fornecidos\n",
        "    xlim_lower = x_lower if lower_lim is not None and lower_lim != 'auto' else data_min - margin\n",
        "    xlim_upper = x_upper if upper_lim is not None and upper_lim != 'auto' else data_max + margin\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(8, 6), gridspec_kw={'height_ratios': [1, 4]})\n",
        "\n",
        "    # Boxplot\n",
        "    ax_box = axes[0]\n",
        "    ax_box.boxplot(data, vert=False, patch_artist=True, widths=0.5, showfliers=True)\n",
        "    ax_box.set_xlim(xlim_lower, xlim_upper)\n",
        "    ax_box.set_yticks([])\n",
        "    ax_box.set_xticklabels([])\n",
        "    ax_box.set_title(f'Boxplot de {column}')\n",
        "\n",
        "    # PDF (histograma + curva normal)\n",
        "    ax_pdf = axes[1]\n",
        "\n",
        "    # Ajustar o range do histograma para os limites definidos\n",
        "    hist_range = (xlim_lower, xlim_upper)\n",
        "    ax_pdf.hist(data, bins=30, color='lightblue', edgecolor='black', alpha=0.7, density=True, range=hist_range)\n",
        "\n",
        "    if len(data) > 1:\n",
        "        media = data.mean()\n",
        "        std = data.std()\n",
        "        mediana = data.median()\n",
        "\n",
        "        # Usar os limites definidos para o grid da curva normal\n",
        "        x_grid = np.linspace(xlim_lower, xlim_upper, 200)\n",
        "        y_norm = norm.pdf(x_grid, media, std)\n",
        "        ax_pdf.plot(x_grid, y_norm, color='darkblue', lw=2, label='Normal')\n",
        "\n",
        "        # Linhas estatísticas\n",
        "        ax_box.axvline(media, color='blue', linestyle='-', label=f'Média: {media:.1f}')\n",
        "        ax_box.axvline(mediana, color='green', linestyle='--', label=f'Mediana: {mediana:.1f}')\n",
        "        ax_box.axvline(media + 3*std, color='orange', linestyle=':', label=f'+3σ: {(media + 3*std):.1f}')\n",
        "        ax_box.axvline(media - 3*std, color='orange', linestyle=':', label=f'-3σ: {(media - 3*std):.1f}')\n",
        "\n",
        "        ax_pdf.axvline(media, color='blue', linestyle='-', label=f'Média: {media:.1f}')\n",
        "        ax_pdf.axvline(mediana, color='green', linestyle='--', label=f'Mediana: {mediana:.1f}')\n",
        "        ax_pdf.axvline(media + 3*std, color='orange', linestyle=':', label=f'+3σ: {(media + 3*std):.1f}')\n",
        "        ax_pdf.axvline(media - 3*std, color='orange', linestyle=':', label=f'-3σ: {(media - 3*std):.1f}')\n",
        "\n",
        "    ax_pdf.set_xlim(xlim_lower, xlim_upper)\n",
        "    ax_pdf.set_xlabel(column)\n",
        "    ax_pdf.set_ylabel('Densidade')\n",
        "    ax_pdf.set_title(f'PDF de {column}')\n",
        "    ax_pdf.legend(fontsize=8, loc='upper left')\n",
        "\n",
        "    plt.tight_layout(h_pad=2.5)\n",
        "    plt.show()\n",
        "\n",
        "def subplot_serie_hist(df, n_cols=3):\n",
        "    \"\"\"\n",
        "    Plota múltiplas séries temporais em subplots com linhas de referência estatísticas.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados das séries temporais.\n",
        "    n_cols : int, opcional\n",
        "        Número de colunas no layout dos subplots. O padrão é 3.\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Cria uma grade de subplots para visualizar múltiplas séries temporais simultaneamente.\n",
        "    - Para cada variável, adiciona linhas de referência estatísticas (mínimo, máximo, limites do IQR).\n",
        "    - Organiza os gráficos em uma grade de n_cols colunas, calculando automaticamente o número de linhas necessárias.\n",
        "    - Adiciona títulos correspondentes aos nomes das colunas do DataFrame.\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    subplot_serie_hist(df)  # Plota todas as colunas do DataFrame\n",
        "    subplot_serie_hist(df[['pv_nafta', 'f_carg_nafta']], n_cols=2)  # Plota apenas as colunas selecionadas\n",
        "    \"\"\"\n",
        "\n",
        "    n_vars = len(df.columns)\n",
        "    n_rows = math.ceil(n_vars / n_cols)\n",
        "\n",
        "    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=df.columns)\n",
        "\n",
        "    for idx, col in enumerate(df.columns):\n",
        "        row = idx // n_cols + 1\n",
        "        col_idx = idx % n_cols + 1\n",
        "\n",
        "        # Usa a lógica do serie_hist: plota a linha e adiciona linhas de referência (opcional)\n",
        "        trace = px.line(df, y=col).data[0]\n",
        "        fig.add_trace(trace, row=row, col=col_idx)\n",
        "\n",
        "        # Adiciona linhas de referência (mínimo, máximo, IQR) igual ao serie_hist\n",
        "        min_value = df[col].min()\n",
        "        max_value = df[col].max()\n",
        "        q1 = df[col].quantile(0.25)\n",
        "        q3 = df[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        loval = q1 - (1.5 * iqr)\n",
        "        hival = q3 + (1.5 * iqr)\n",
        "\n",
        "        fig.add_shape(type='line', x0=df.index.min(), y0=min_value, x1=df.index.max(), y1=min_value,\n",
        "                    line=dict(color='gray', width=1, dash='dash'), row=row, col=col_idx)\n",
        "        fig.add_shape(type='line', x0=df.index.min(), y0=max_value, x1=df.index.max(), y1=max_value,\n",
        "                    line=dict(color='gray', width=1, dash='dash'), row=row, col=col_idx)\n",
        "        fig.add_shape(type='line', x0=df.index.min(), y0=loval, x1=df.index.max(), y1=loval,\n",
        "                    line=dict(color='orange', width=1, dash='dash'), row=row, col=col_idx)\n",
        "        fig.add_shape(type='line', x0=df.index.min(), y0=hival, x1=df.index.max(), y1=hival,\n",
        "                    line=dict(color='orange', width=1, dash='dash'), row=row, col=col_idx)\n",
        "\n",
        "    # Adicionar uma legenda única para todas as linhas de referência\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[None], y=[None], mode='lines', line=dict(color='gray', width=1, dash='dash'),\n",
        "                  name='Min/Max', showlegend=True),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[None], y=[None], mode='lines', line=dict(color='orange', width=1, dash='dash'),\n",
        "                  name='IQR Limits (±1.5*IQR)', showlegend=True),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        height=250*n_rows,\n",
        "        width=450*n_cols,\n",
        "        showlegend=True,\n",
        "        title_text=\"Séries Temporais das Variáveis\",\n",
        "        legend=dict(\n",
        "            orientation=\"h\",\n",
        "            yanchor=\"bottom\",\n",
        "            y=1.02,\n",
        "            xanchor=\"right\",\n",
        "            x=1\n",
        "        )\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "def _serialize_space(obj):\n",
        "    # from skopt.space import Real, Integer, Categorical\n",
        "    if isinstance(obj, Real):\n",
        "        return {\"_type\": \"Real\", \"low\": obj.low, \"high\": obj.high, \"prior\": getattr(obj, \"prior\", None)}\n",
        "    if isinstance(obj, Integer):\n",
        "        return {\"_type\": \"Integer\", \"low\": obj.low, \"high\": obj.high, \"prior\": getattr(obj, \"prior\", None)}\n",
        "    if isinstance(obj, Categorical):\n",
        "        # categories pode ser array/numpy; transformar em lista simples\n",
        "        return {\"_type\": \"Categorical\", \"categories\": list(obj.categories)}\n",
        "    return obj\n",
        "\n",
        "def _normalize_scalar(v):\n",
        "    \"\"\"Converte strings 'True'/'False' para bool e tenta interpretar tuplas/listas/números via ast.literal_eval.\"\"\"\n",
        "    # import ast\n",
        "    if isinstance(v, str):\n",
        "        s = v.strip()\n",
        "        ls = s.lower()\n",
        "        if ls == \"true\":\n",
        "            return True\n",
        "        if ls == \"false\":\n",
        "            return False\n",
        "        try:\n",
        "            # converte \"(50,)\" -> (50,), \"[50,50]\" -> [50,50], \"1e-3\" -> float, \"10\" -> int\n",
        "            return ast.literal_eval(s)\n",
        "        except Exception:\n",
        "            return v\n",
        "    return v\n",
        "\n",
        "def serialize_algo_configs(cfg):\n",
        "    # from copy import deepcopy\n",
        "    # from skopt.space import Real, Integer, Categorical\n",
        "    out = deepcopy(cfg)\n",
        "    for model_k, model_v in out.items():\n",
        "        # Normalizar default_params (ex.: \"True\"/\"(50,)\" etc.)\n",
        "        dp = model_v.get(\"default_params\", {})\n",
        "        if isinstance(dp, dict):\n",
        "            model_v[\"default_params\"] = {k: _normalize_scalar(v) for k, v in dp.items()}\n",
        "\n",
        "        ss = model_v.get(\"search_space\", {})\n",
        "        if isinstance(ss, dict):\n",
        "            for param_k, param_v in list(ss.items()):\n",
        "                # se for um objeto skopt -> serializa\n",
        "                if isinstance(param_v, (Real, Integer, Categorical)):\n",
        "                    ss[param_k] = _serialize_space(param_v)\n",
        "                    continue\n",
        "                # se já for um dict serializável, normalizar categorias se for Categorical\n",
        "                if isinstance(param_v, dict) and param_v.get(\"_type\") == \"Categorical\":\n",
        "                    cats = param_v.get(\"categories\", [])\n",
        "                    newcats = []\n",
        "                    for c in cats:\n",
        "                        newcats.append(_normalize_scalar(c))\n",
        "                    param_v[\"categories\"] = newcats\n",
        "                    ss[param_k] = param_v\n",
        "                    continue\n",
        "                # Caso seja lista/tuple definido inline (ex.: [(50,), (100,)]), tentar normalizar conteúdo\n",
        "                if isinstance(param_v, (list, tuple)):\n",
        "                    # tenta detectar se elementos são strings representando tuplas/numeros\n",
        "                    new_list = []\n",
        "                    for elem in param_v:\n",
        "                        new_list.append(_normalize_scalar(elem))\n",
        "                    ss[param_k] = new_list\n",
        "                    continue\n",
        "                # fallback: mantém como está\n",
        "                ss[param_k] = param_v\n",
        "            model_v[\"search_space\"] = ss\n",
        "    return out\n",
        "\n",
        "def _deserialize_space(obj):\n",
        "    # from skopt.space import Real, Integer, Categorical\n",
        "    # # já é um objeto skopt\n",
        "    if isinstance(obj, (Real, Integer, Categorical)):\n",
        "        return obj\n",
        "    # se não for dict, retorna como está (fallback)\n",
        "    if not isinstance(obj, dict):\n",
        "        return obj\n",
        "\n",
        "    def _make_hashable(x):\n",
        "        if isinstance(x, list):\n",
        "            return tuple(_make_hashable(v) for v in x)\n",
        "        if isinstance(x, dict):\n",
        "            return {k: _make_hashable(v) for k, v in x.items()}\n",
        "        return x\n",
        "\n",
        "    t = obj.get(\"_type\")\n",
        "    if t == \"Real\":\n",
        "        return Real(obj[\"low\"], obj[\"high\"], prior=obj.get(\"prior\"))\n",
        "    if t == \"Integer\":\n",
        "        return Integer(obj[\"low\"], obj[\"high\"], prior=obj.get(\"prior\"))\n",
        "\n",
        "    if t == \"Categorical\":\n",
        "        cats = obj.get(\"categories\", [])\n",
        "        if not cats:\n",
        "            raise ValueError(\"Categorical requires a non-empty 'categories' list\")\n",
        "        from ast import literal_eval\n",
        "        norm_cats = []\n",
        "        for c in cats:\n",
        "            if isinstance(c, str):\n",
        "                try:\n",
        "                    parsed = literal_eval(c)\n",
        "                    v = parsed\n",
        "                except (ValueError, SyntaxError):\n",
        "                    # preserve original string if it's not a literal encoding\n",
        "                    v = c\n",
        "            else:\n",
        "                v = c\n",
        "            # tornar cada categoria hashable (ex.: listas -> tuplas)\n",
        "            v = _make_hashable(v)\n",
        "            norm_cats.append(v)\n",
        "        return Categorical(norm_cats)\n",
        "    return obj\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"r2\": r2_score(y_true, y_pred),\n",
        "        \"rmse\": root_mean_squared_error(y_true, y_pred),\n",
        "        \"mse\": mean_squared_error(y_true, y_pred),\n",
        "        \"mae\": mean_absolute_error(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "def get_regression_metric(abbreviated_name):\n",
        "    \"\"\"\n",
        "    Retorna o nome completo da métrica de regressão baseado na abreviação\n",
        "\n",
        "    Args:\n",
        "        abbreviated_name (str): Nome abreviado da métrica\n",
        "\n",
        "    Returns:\n",
        "        str: Nome completo da métrica para usar no BayesSearchCV\n",
        "    \"\"\"\n",
        "    # Dicionário de métricas de regressão para BayesSearchCV\n",
        "    regression_metrics_dict = {\n",
        "        'mse': 'neg_mean_squared_error',\n",
        "        'rmse': 'neg_root_mean_squared_error',\n",
        "        'mae': 'neg_mean_absolute_error',\n",
        "        'medae': 'neg_median_absolute_error',\n",
        "        'r2': 'r2',\n",
        "        'var': 'explained_variance',\n",
        "        'max_error': 'neg_max_error',\n",
        "        'msle': 'neg_mean_squared_log_error',\n",
        "        'rmsle': 'neg_root_mean_squared_log_error',\n",
        "        'mape': 'neg_mean_absolute_percentage_error',\n",
        "        'poisson': 'neg_mean_poisson_deviance',\n",
        "        'gamma': 'neg_mean_gamma_deviance',\n",
        "        'd2': 'd2_absolute_error_score'\n",
        "    }\n",
        "    return regression_metrics_dict.get(abbreviated_name.lower(), abbreviated_name)\n",
        "\n",
        "def boxplot_algo(cv_df, test_df, metric_name, order):\n",
        "    \"\"\"\n",
        "    Plota boxplot dos scores de validação cruzada (cv_df) e pontos de teste (test_df) para cada algoritmo,\n",
        "    ordenando os algoritmos conforme a métrica (minimização ou maximização).\n",
        "    A linha tracejada vermelha é traçada no menor valor (minimização) ou maior valor (maximização) do score de teste.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    cv_df : pd.DataFrame\n",
        "        DataFrame com colunas ['Algoritmo', 'Score'] para os folds de validação cruzada.\n",
        "    test_df : pd.DataFrame\n",
        "        DataFrame com colunas ['Algoritmo', 'Score'] para os pontos de teste.\n",
        "    metric_name : str\n",
        "        Nome da métrica (ex: 'rmse', 'mae', 'r2', etc).\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    # MINIMIZE_METRICS = {'rmse', 'mae', 'mse', 'msle', 'rmsle', 'mape', 'poisson', 'gamma', 'max_error', 'medae'}\n",
        "    # MAXIMIZE_METRICS = {'r2', 'var', 'd2', 'explained_variance'}\n",
        "\n",
        "    metric = metric_name.lower() if isinstance(metric_name, str) else str(metric_name).lower()\n",
        "\n",
        "    sns.boxplot(\n",
        "        x='Algoritmo',\n",
        "        y='Score',\n",
        "        data=cv_df,\n",
        "        order=order,\n",
        "        ax=ax,\n",
        "        boxprops=dict(facecolor='darkblue', color='darkblue', alpha=0.3),\n",
        "        medianprops=dict(color='darkblue'),\n",
        "        whiskerprops=dict(color='darkblue'),\n",
        "        capprops=dict(color='darkblue'),\n",
        "        flierprops=dict(markerfacecolor='lightblue', markeredgecolor='darkblue')\n",
        "        )\n",
        "\n",
        "    sns.stripplot(\n",
        "        x='Algoritmo',\n",
        "        y='Score',\n",
        "        data=test_df,\n",
        "        order=order,\n",
        "        color='red',\n",
        "        ax=ax,\n",
        "        jitter=True,\n",
        "        size=6,\n",
        "        zorder=10\n",
        "        )\n",
        "\n",
        "    # Decide se a linha horizontal é o menor ou maior valor, dependendo da métrica\n",
        "    # Métricas onde menor é melhor (negativas no sklearn): 'rmse', 'mse', 'mae', 'medae', 'max_error', 'msle', 'rmsle', 'mape', 'poisson', 'gamma'\n",
        "    # Métricas onde maior é melhor: 'r2', 'var', 'd2'\n",
        "    metric_lower_is_better = MINIMIZE_METRICS\n",
        "    metric = metric_name.lower() if isinstance(metric_name, str) else str(metric_name).lower()\n",
        "\n",
        "    if not test_df.empty:\n",
        "        if metric in metric_lower_is_better:\n",
        "            test_score_line = test_df['Score'].min()\n",
        "        else:\n",
        "            test_score_line = test_df['Score'].max()\n",
        "        ax.axhline(test_score_line, color='red', linestyle='--', linewidth=1.5, zorder=5)\n",
        "    else:\n",
        "        test_score_line = None\n",
        "\n",
        "    cv_handle = Patch(facecolor='darkblue', edgecolor='darkblue', alpha=0.3, label='CV Folds')\n",
        "    teste_handle = Line2D([0], [0], color='red', marker='o', linestyle='', markersize=5, label='Score do Teste')\n",
        "    handles = [cv_handle, teste_handle]\n",
        "    if test_score_line is not None:\n",
        "        test_line_handle = Line2D([0], [0], color='red', linestyle='--', label='Linha Score Teste')\n",
        "        handles.append(test_line_handle)\n",
        "\n",
        "    ax.legend(handles=handles, loc='best')\n",
        "\n",
        "    # Ajustes de visualização\n",
        "    ax.set_title(f'Boxplot de Métricas ({METRIC_TO_OPT}) de Validação (CV) e Ponto de Teste Final')\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def boxplot_algo_grouped_by_scaler(df_box, metric_name, ncols=3, figsize_per_plot=(6,4)):\n",
        "    \"\"\"\n",
        "    Plota subplots por scaler com a mesma escala Y.\n",
        "    Usa df_box já preparado no notebook (colunas: Algoritmo, Scaler, Score, Type).\n",
        "    Ordenação por scaler respeita METRIC_TO_OPT: 'r2' (maximizar) -> ordem decrescente,\n",
        "    demais métricas consideradas de minimização -> ordem crescente.\n",
        "    Além disso plota uma linha tracejada vermelha no melhor score de teste (global) passando por todos os subplots,\n",
        "    e colore o(s) ponto(s) de teste correspondente(s) a esse melhor score em azul-marinho.\n",
        "    Adiciona uma linha tracejada cinza indicando o valor do score do algoritmo baseline.\n",
        "    \"\"\"\n",
        "    metric = metric_name.lower() if isinstance(metric_name, str) else str(metric_name).lower()\n",
        "\n",
        "    cv_df = df_box[df_box['Type'] == 'CV']\n",
        "    cv_df_merged = cv_df.copy()\n",
        "    cv_df_merged['Algoritmo'] = cv_df_merged['Algoritmo'] + \" (\" + cv_df_merged['Scaler'] + \")\"\n",
        "    test_df = df_box[df_box['Type'] == 'Test']\n",
        "    test_df_merged = test_df.copy()\n",
        "    test_df_merged['Algoritmo'] = test_df_merged['Algoritmo'] + \" (\" + test_df_merged['Scaler'] + \")\"\n",
        "\n",
        "    if METRIC_TO_OPT.lower() in MINIMIZE_METRICS:\n",
        "        ascending = True\n",
        "    elif METRIC_TO_OPT.lower() in MAXIMIZE_METRICS:\n",
        "        ascending = False\n",
        "    else:\n",
        "        # fallback: assume maximizar\n",
        "        ascending = False\n",
        "\n",
        "    # Atenção: para métricas de erro (minimização), o sklearn retorna valores negativos no cross-validation (cv_df)\n",
        "    # enquanto os valores de teste (test_df) são positivos. Portanto, precisamos inverter o sinal dos scores do cv_df\n",
        "    # para que ambos fiquem na mesma escala (positivos para erro, maiores = pior).\n",
        "\n",
        "    if METRIC_TO_OPT.lower() in MINIMIZE_METRICS:\n",
        "        cv_df_merged['Score'] = -cv_df_merged['Score']\n",
        "\n",
        "    # ordenado pelo valor da métrica de teste\n",
        "    order_teste = test_df_merged.groupby('Algoritmo')['Score'].mean().sort_values(ascending=ascending).index.tolist()\n",
        "    order_cv = cv_df_merged.groupby('Algoritmo')['Score'].median().sort_values(ascending=ascending).index.tolist()\n",
        "\n",
        "    df = df_box.copy()\n",
        "    if metric in MINIMIZE_METRICS:\n",
        "        df.loc[df['Type'] == 'CV', 'Score'] = -df.loc[df['Type'] == 'CV', 'Score']\n",
        "\n",
        "    if metric in MAXIMIZE_METRICS:\n",
        "        ascending = False\n",
        "    else:\n",
        "        ascending = True\n",
        "\n",
        "    test_scores = df.loc[df['Type'] == 'Test', 'Score']\n",
        "    global_best_test_score = None\n",
        "    if not test_scores.empty:\n",
        "        if metric in MINIMIZE_METRICS:\n",
        "            global_best_test_score = test_scores.min()\n",
        "        else:\n",
        "            global_best_test_score = test_scores.max()\n",
        "\n",
        "    # Score do baseline: primeiro algoritmo do primeiro scaler (assumindo ordenação)\n",
        "    baseline_row = df[(df['Type'] == 'Test')].iloc[0] if not df[(df['Type'] == 'Test')].empty else None\n",
        "    baseline_score = baseline_row['Score'] if baseline_row is not None else None\n",
        "\n",
        "    scalers_list = sorted(df['Scaler'].unique(), key=lambda x: str(x))\n",
        "    ymin = df['Score'].quantile(0.01)\n",
        "    ymax = df['Score'].quantile(0.99)\n",
        "    margin = 0.05 * (ymax - ymin) if ymax > ymin else 0.1\n",
        "    y_min_plot = ymin - margin\n",
        "    y_max_plot = ymax + margin\n",
        "\n",
        "    n = len(scalers_list)\n",
        "    nrows = math.ceil(n / ncols)\n",
        "    fig_w = ncols * figsize_per_plot[0]\n",
        "    fig_h = nrows * figsize_per_plot[1]\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(fig_w, fig_h), squeeze=False)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    tol = 1e-8\n",
        "\n",
        "    # Inverter o gradiente de cores do boxplot (usar 'Blues_r')\n",
        "    box_palette = 'Blues_r'\n",
        "\n",
        "    for idx, scaler in enumerate(scalers_list):\n",
        "        ax = axes[idx]\n",
        "        df_cv_sub = df[(df['Scaler'] == scaler) & (df['Type'] == 'CV')]\n",
        "        df_test_sub = df[(df['Scaler'] == scaler) & (df['Type'] == 'Test')]\n",
        "\n",
        "        if df_cv_sub.empty and df_test_sub.empty:\n",
        "            ax.text(0.5, 0.5, 'Sem dados', ha='center', va='center')\n",
        "            ax.set_title(f'Scaler: {scaler}')\n",
        "            ax.set_ylim(y_min_plot, y_max_plot)\n",
        "            ax.set_xticks([])\n",
        "            continue\n",
        "\n",
        "        algs_cv = set(df_cv_sub['Algoritmo'].unique())\n",
        "        algs_test = set(df_test_sub['Algoritmo'].unique())\n",
        "        algs_present = list(algs_cv.union(algs_test))\n",
        "\n",
        "        order_for_scaler = []\n",
        "        if not df_test_sub.empty:\n",
        "            order_for_scaler = df_test_sub.groupby('Algoritmo')['Score'].mean().sort_values(ascending=ascending).index.tolist()\n",
        "        elif not df_cv_sub.empty:\n",
        "            order_for_scaler = df_cv_sub.groupby('Algoritmo')['Score'].median().sort_values(ascending=ascending).index.tolist()\n",
        "        order_for_scaler = [a for a in order_for_scaler if a in algs_present]\n",
        "        if not order_for_scaler:\n",
        "            order_for_scaler = sorted(algs_present, key=lambda x: str(x))\n",
        "\n",
        "        sns.boxplot(\n",
        "            x='Algoritmo',\n",
        "            y='Score',\n",
        "            data=df_cv_sub,\n",
        "            order=order_for_scaler,\n",
        "            ax=ax,\n",
        "            palette=box_palette,\n",
        "            boxprops=dict(alpha=0.6)\n",
        "        )\n",
        "\n",
        "        if not df_test_sub.empty:\n",
        "            sns.stripplot(\n",
        "                x='Algoritmo',\n",
        "                y='Score',\n",
        "                data=df_test_sub,\n",
        "                order=order_for_scaler,\n",
        "                ax=ax,\n",
        "                color='red',\n",
        "                size=7,\n",
        "                jitter=False,\n",
        "                marker='D'\n",
        "            )\n",
        "            if global_best_test_score is not None:\n",
        "                matches = df_test_sub[np.isclose(df_test_sub['Score'].astype(float), float(global_best_test_score), atol=tol)]\n",
        "                if not matches.empty:\n",
        "                    for _, row in matches.iterrows():\n",
        "                        alg = row['Algoritmo']\n",
        "                        if alg not in order_for_scaler:\n",
        "                            continue\n",
        "                        x_pos = order_for_scaler.index(alg)\n",
        "                        y_val = row['Score']\n",
        "                        ax.scatter(x_pos, y_val, color='navy', edgecolor='white', s=49, marker='D', zorder=30)\n",
        "\n",
        "        positions = np.arange(len(order_for_scaler))\n",
        "        ax.set_xticks(positions)\n",
        "        ax.set_xticklabels(order_for_scaler, rotation=45, ha='right')\n",
        "\n",
        "        ax.set_title(f'Scaler: {scaler}')\n",
        "        ax.set_ylim(y_min_plot, y_max_plot)\n",
        "        ax.set_xlabel('')\n",
        "        if idx % ncols == 0:\n",
        "            ax.set_ylabel(metric_name)\n",
        "        else:\n",
        "            ax.set_ylabel('')\n",
        "        ax.tick_params(axis='x')\n",
        "\n",
        "        # Linha tracejada vermelha do melhor score de teste global\n",
        "        if global_best_test_score is not None:\n",
        "            ax.axhline(global_best_test_score, color='red', linestyle='--', linewidth=1.5, zorder=20, label='Melhor Score Teste (global)')\n",
        "        # Linha tracejada cinza do baseline\n",
        "        if baseline_score is not None:\n",
        "            ax.axhline(baseline_score, color='gray', linestyle=':', linewidth=1.5, zorder=10, label='Baseline')\n",
        "\n",
        "    for j in range(n, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    handles = [\n",
        "        Patch(facecolor='darkblue', edgecolor='k', alpha=0.6, label='CV folds'),\n",
        "        Line2D([0], [0], color='red', marker='D', linestyle='', markersize=7, label='Test score'),\n",
        "    ]\n",
        "    if global_best_test_score is not None:\n",
        "        handles.append(Line2D([0], [0], color='red', linestyle='--', label='Melhor Score Teste (global)'))\n",
        "    if baseline_score is not None:\n",
        "        handles.append(Line2D([0], [0], color='gray', linestyle=':', label='Baseline'))\n",
        "\n",
        "    # Reposicionar a legenda para fora do gráfico, acima do título\n",
        "    fig.legend(handles=handles, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 1.08))\n",
        "    plt.suptitle(f'Comparativo por Scaler (Métrica: {metric_name})', fontsize=14, y=1.03)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "    plt.show()\n",
        "\n",
        "    # Tabela resumo dos resultados ordenada pela métrica de teste (melhor desempenho primeiro)\n",
        "    summary_table = test_df_merged.copy()\n",
        "    summary_table['Algoritmo'] = summary_table['Algoritmo'].str.replace(r\"\\s*\\(.*\\)\", \"\", regex=True)\n",
        "    summary_table = summary_table.sort_values('Score', ascending=ascending).reset_index(drop=True)\n",
        "    summary_table['Ordem'] = summary_table.index + 1\n",
        "\n",
        "    print(f\"{'Ordem':<6} {'Algoritmo':<25} {'Scaler':<10} {'Score':<10} {'Métrica':<10}\")\n",
        "    print(\"-\" * 65)\n",
        "    for idx, row in summary_table.iterrows():\n",
        "        # Coloca em negrito o melhor resultado (primeira linha)\n",
        "        if idx == 0:\n",
        "            print(f\"\\033[1m{row['Ordem']:<6} {row['Algoritmo']:<25} {row['Scaler']:<10} {row['Score']:<10.4f} {METRIC_TO_OPT:<10}\\033[0m\")\n",
        "        else:\n",
        "            print(f\"{row['Ordem']:<6} {row['Algoritmo']:<25} {row['Scaler']:<10} {row['Score']:<10.4f} {METRIC_TO_OPT:<10}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjAK4aVkx3x2"
      },
      "source": [
        "## 2.4 Configuração dos algoritmo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrrmAwRmx3x2"
      },
      "source": [
        "### 2.4.1 Algoritmos de Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrU1lngox3x2"
      },
      "outputs": [],
      "source": [
        "# Configuração dos metadados dos algoritmos (JSON)\n",
        "ALGO_CONFIGS = {\n",
        "    \"linear\": {\n",
        "        \"alias\": \"Linear\",\n",
        "        \"model_class\": \"LinearRegression\",\n",
        "        \"module\": \"sklearn.linear_model\",\n",
        "        \"default_params\": {},\n",
        "        \"search_space\": {\n",
        "            'positive': Categorical([\"False\", \"True\"])\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"ridge\": {\n",
        "        \"alias\": \"Ridge\",\n",
        "        \"model_class\": \"Ridge\",\n",
        "        \"module\": \"sklearn.linear_model\",\n",
        "        \"default_params\": {\"random_state\": 42},\n",
        "        \"search_space\": {\n",
        "            \"alpha\": Real(1e-3, 10.0, prior=\"log-uniform\")\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"lasso\": {\n",
        "        \"alias\": \"Lasso\",\n",
        "        \"model_class\": \"Lasso\",\n",
        "        \"module\": \"sklearn.linear_model\",\n",
        "        \"default_params\": {\"random_state\": 42, \"max_iter\": 2000},\n",
        "        \"search_space\": {\n",
        "            \"alpha\": Real(1e-3, 10.0, prior=\"log-uniform\")\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"elastic_net\": {\n",
        "        \"alias\": \"ElasticNet\",\n",
        "        \"model_class\": \"ElasticNet\",\n",
        "        \"module\": \"sklearn.linear_model\",\n",
        "        \"default_params\": {\"random_state\": 42, \"max_iter\": 2000},\n",
        "        \"search_space\": {\n",
        "            \"alpha\": Real(1e-3, 10.0, prior=\"log-uniform\"),\n",
        "            \"l1_ratio\": Real(0.0, 1.0, prior=\"uniform\")\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"svr\": {\n",
        "        \"alias\": \"SVR\",\n",
        "        \"model_class\": \"SVR\",\n",
        "        \"module\": \"sklearn.svm\",\n",
        "        \"default_params\": {},\n",
        "        \"search_space\": {\n",
        "            \"C\": Real(0.1, 100.0, prior=\"log-uniform\"),\n",
        "            \"gamma\": Real(1e-4, 1e-1, prior=\"log-uniform\"),\n",
        "            \"kernel\": Categorical(['rbf', 'linear', 'poly'])\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"knn\": {\n",
        "        \"alias\": \"KNN\",\n",
        "        \"model_class\": \"KNeighborsRegressor\",\n",
        "        \"module\": \"sklearn.neighbors\",\n",
        "        \"default_params\": {\"n_jobs\": -1},\n",
        "        \"search_space\": {\n",
        "            \"n_neighbors\": Integer(3, 20),\n",
        "            \"weights\": Categorical(['uniform', 'distance'])\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"decision_tree\": {\n",
        "        \"alias\": \"DecisionTree\",\n",
        "        \"model_class\": \"DecisionTreeRegressor\",\n",
        "        \"module\": \"sklearn.tree\",\n",
        "        \"default_params\": {\"random_state\": 42},\n",
        "        \"search_space\": {\n",
        "            \"max_depth\": Integer(3, 20),\n",
        "            \"min_samples_split\": Integer(2, 20),\n",
        "            \"min_samples_leaf\": Integer(1, 10),\n",
        "            \"max_features\": Categorical(['sqrt', 'log2'])\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"random_forest\": {\n",
        "        \"alias\": \"RandomForest\",\n",
        "        \"model_class\": \"RandomForestRegressor\",\n",
        "        \"module\": \"sklearn.ensemble\",\n",
        "        \"default_params\": {\"random_state\": 42, \"n_jobs\": -1},\n",
        "        \"search_space\": {\n",
        "            \"n_estimators\": Integer(50, 300),\n",
        "            \"max_depth\": Integer(5, 20),\n",
        "            \"min_samples_split\": Integer(2, 10),\n",
        "            \"min_samples_leaf\": Integer(1, 5),\n",
        "            \"max_features\": Categorical(['sqrt', 'log2'])\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"ada_boost\": {\n",
        "        \"alias\": \"AdaBoost\",\n",
        "        \"model_class\": \"AdaBoostRegressor\",\n",
        "        \"module\": \"sklearn.ensemble\",\n",
        "        \"default_params\": {\"random_state\": 42},\n",
        "        \"search_space\": {\n",
        "            \"n_estimators\": Integer(50, 200),\n",
        "            \"learning_rate\": Real(0.01, 2.0, prior=\"log-uniform\"),\n",
        "            \"loss\": Categorical(['linear', 'square', 'exponential'])\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"gradient_boosting\": {\n",
        "        \"alias\": \"GradientBoost\",\n",
        "        \"model_class\": \"GradientBoostingRegressor\",\n",
        "        \"module\": \"sklearn.ensemble\",\n",
        "        \"default_params\": {\"random_state\": 42},\n",
        "        \"search_space\": {\n",
        "            \"n_estimators\": Integer(50, 200),\n",
        "            \"max_depth\": Integer(3, 10),\n",
        "            \"learning_rate\": Real(0.01, 0.3, prior=\"log-uniform\"),\n",
        "            \"subsample\": Real(0.6, 1.0)\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"bagging\": {\n",
        "        \"alias\": \"Bagging\",\n",
        "        \"model_class\": \"BaggingRegressor\",\n",
        "        \"module\": \"sklearn.ensemble\",\n",
        "        \"default_params\": {\"random_state\": 42, \"n_jobs\": -1},\n",
        "        \"search_space\": {\n",
        "            \"n_estimators\": Integer(10, 100),\n",
        "            \"max_samples\": Real(0.5, 1.0),\n",
        "            \"max_features\": Real(0.5, 1.0)\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"extra_trees\": {\n",
        "        \"alias\": \"ExtraTrees\",\n",
        "        \"model_class\": \"ExtraTreesRegressor\",\n",
        "        \"module\": \"sklearn.ensemble\",\n",
        "        \"default_params\": {\"random_state\": 42, \"n_jobs\": -1},\n",
        "        \"search_space\": {\n",
        "            \"max_depth\": Integer(5, 20),\n",
        "            \"min_samples_split\": Integer(2, 10),\n",
        "            \"min_samples_leaf\": Integer(1, 5)\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"lightgbm\": {\n",
        "        \"alias\": \"LightGBM\",\n",
        "        \"model_class\": \"LGBMRegressor\",\n",
        "        \"module\": \"lightgbm\",\n",
        "        \"default_params\": {\n",
        "            \"random_state\": 42,\n",
        "            \"verbose\": -1\n",
        "        },\n",
        "        \"search_space\": {\n",
        "            \"n_estimators\": {\n",
        "                \"_type\": \"Integer\",\n",
        "                \"low\": 50,\n",
        "                \"high\": 300,\n",
        "                \"prior\": \"uniform\"\n",
        "            },\n",
        "            \"max_depth\": {\n",
        "                \"_type\": \"Integer\",\n",
        "                \"low\": 3,\n",
        "                \"high\": 10,\n",
        "                \"prior\": \"uniform\"\n",
        "            },\n",
        "            \"learning_rate\": {\n",
        "                \"_type\": \"Real\",\n",
        "                \"low\": 0.01,\n",
        "                \"high\": 0.3,\n",
        "                \"prior\": \"log-uniform\"\n",
        "            },\n",
        "            \"subsample\": {\n",
        "                \"_type\": \"Real\",\n",
        "                \"low\": 0.6,\n",
        "                \"high\": 1.0,\n",
        "                \"prior\": \"uniform\"\n",
        "            },\n",
        "            \"colsample_bytree\": {\n",
        "                \"_type\": \"Real\",\n",
        "                \"low\": 0.6,\n",
        "                \"high\": 1.0,\n",
        "                \"prior\": \"uniform\"\n",
        "            },\n",
        "            \"num_leaves\": {\n",
        "                \"_type\": \"Integer\",\n",
        "                \"low\": 20,\n",
        "                \"high\": 150,\n",
        "                \"prior\": \"uniform\"\n",
        "            },\n",
        "            \"min_child_samples\": {\n",
        "                \"_type\": \"Integer\",\n",
        "                \"low\": 5,\n",
        "                \"high\": 100,\n",
        "                \"prior\": \"uniform\"\n",
        "            },\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"xgboost\": {\n",
        "        \"alias\": \"XGBoost\",\n",
        "        \"model_class\": \"XGBRegressor\",\n",
        "        \"module\": \"xgboost\",\n",
        "        \"default_params\": {\"random_state\": 42, \"verbosity\": 0},\n",
        "        \"search_space\": {\n",
        "            \"n_estimators\": Integer(50, 300),\n",
        "            \"max_depth\": Integer(3, 10),\n",
        "            \"learning_rate\": Real(0.01, 0.3, prior=\"log-uniform\"),\n",
        "            \"subsample\": Real(0.6, 1.0),\n",
        "            \"colsample_bytree\": Real(0.6, 1.0)\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"catboost\": {\n",
        "        \"alias\": \"CatBoost\",\n",
        "        \"model_class\": \"CatBoostRegressor\",\n",
        "        \"module\": \"catboost\",\n",
        "        \"default_params\": {\n",
        "            \"random_seed\": 42,\n",
        "            \"verbose\": 0,\n",
        "            \"allow_writing_files\": \"False\"\n",
        "        },\n",
        "        \"search_space\": {\n",
        "            \"iterations\": {\n",
        "                \"_type\": \"Integer\",\n",
        "                \"low\": 50,\n",
        "                \"high\": 300,\n",
        "                \"prior\": \"uniform\"\n",
        "            },\n",
        "            \"depth\": {\n",
        "                \"_type\": \"Integer\",\n",
        "                \"low\": 3,\n",
        "                \"high\": 10,\n",
        "                \"prior\": \"uniform\"\n",
        "            },\n",
        "            \"learning_rate\": {\n",
        "                \"_type\": \"Real\",\n",
        "                \"low\": 0.01,\n",
        "                \"high\": 0.3,\n",
        "                \"prior\": \"log-uniform\"\n",
        "            },\n",
        "            \"subsample\": {\n",
        "                \"_type\": \"Real\",\n",
        "                \"low\": 0.6,\n",
        "                \"high\": 1.0,\n",
        "                \"prior\": \"uniform\"\n",
        "            },\n",
        "            \"colsample_bylevel\": {\n",
        "                \"_type\": \"Real\",\n",
        "                \"low\": 0.6,\n",
        "                \"high\": 1.0,\n",
        "                \"prior\": \"uniform\"\n",
        "            },\n",
        "            \"l2_leaf_reg\": {\n",
        "                \"_type\": \"Real\",\n",
        "                \"low\": 1.0,\n",
        "                \"high\": 10.0,\n",
        "                \"prior\": \"uniform\"\n",
        "            }\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    },\n",
        "    \"mlp\": {\n",
        "        \"alias\": \"MLP\",\n",
        "        \"model_class\": \"MLPRegressor\",\n",
        "        \"module\": \"sklearn.neural_network\",\n",
        "        \"default_params\": {\n",
        "            \"random_state\": 42,\n",
        "            \"max_iter\": 1000,\n",
        "            \"early_stopping\": \"True\",\n",
        "            \"n_iter_no_change\": 10,\n",
        "            \"tol\": 1e-4,\n",
        "            \"solver\": \"adam\",\n",
        "            \"learning_rate_init\": \"1e-3\",\n",
        "            \"batch_size\": \"auto\"\n",
        "\n",
        "        },\n",
        "        \"search_space\": {\n",
        "            \"hidden_layer_sizes\": {\n",
        "                \"_type\": \"Categorical\",\n",
        "                \"categories\": [16, 32, 64, 128, 256, 512]\n",
        "            },\n",
        "            \"activation\": {\n",
        "                \"_type\": \"Categorical\",\n",
        "                \"categories\": [\n",
        "                    \"relu\",\n",
        "                    \"tanh\",\n",
        "                    \"logistic\"\n",
        "                ]\n",
        "            },\n",
        "            \"alpha\": {\n",
        "                \"_type\": \"Real\",\n",
        "                \"low\": 1e-5,\n",
        "                \"high\": 1.0,\n",
        "                \"prior\": \"log-uniform\"\n",
        "            },\n",
        "            \"learning_rate\": {\n",
        "                \"_type\": \"Categorical\",\n",
        "                \"categories\": [\n",
        "                    \"constant\",\n",
        "                    \"adaptive\",\n",
        "                    \"invscaling\"\n",
        "                ]\n",
        "            },\n",
        "        },\n",
        "        \"default_metric\": \"r2\"\n",
        "    }\n",
        "}\n",
        "\n",
        "ALGO_CONFIGS_SERIALIZABLE = serialize_algo_configs(ALGO_CONFIGS)\n",
        "\n",
        "with open('algo_configs.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(ALGO_CONFIGS_SERIALIZABLE, f, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IziqQX07x3x2"
      },
      "source": [
        "### 2.4.2 Algoritmos de Padronização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5D01owBx3x3"
      },
      "outputs": [],
      "source": [
        "# Métodos de padronização\n",
        "SCALERS_CONFIGS = [\n",
        "    (\"Minmax\", MinMaxScaler()),\n",
        "    (\"Standard\", StandardScaler()),\n",
        "    (\"YeoJohn\", PowerTransformer(method='yeo-johnson')), # Box-Cox requer dados positivos\n",
        "    # # # Vamos deixar comentado para eventual uso em outros trabalhos.\n",
        "    # (\"Maxabs\", MaxAbsScaler()),\n",
        "    # (\"Robust\", RobustScaler()),\n",
        "    # (\"Normal\", Normalizer())\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N91bTLbiBxrm"
      },
      "source": [
        "# 3. Dados: entendimento, carga e qualidade\n",
        "\n",
        "O dataset **Pressão de Vapor da nafta** é um conjunto de dados previamente tratado na SPRINT de Análise de Dados e Boas Práticas.\n",
        "\n",
        "Esse dataset já passou por vários tratamentos, dentre eles os de valores nulos, faltantes e de outliers.\n",
        "\n",
        "Será necessária uma breve análise exploratória dos dados de forma a torná-lo uma fonte de dados ainda mais curada para o uso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC6QJHhWBnzH"
      },
      "source": [
        "## 3.1. Atributos do dataset\n",
        "\n",
        "O dataset Pressão de Vapor de nafta pode ser assim resumido:\n",
        "\n",
        "***pv_nafta***: pressão de vapor da nafta (unidade de pressão)<br>\n",
        "***t_carg_nafta***: temperatura da carga da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_fund_nafta***: temperatura do fundo da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_aque_nafta***: temperatura do aquecedor de fundo da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_esup_nafta***: temperatura estágio superior interno da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_eint_nafta***: temperatura estágio intermediário interno da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_einf_nafta***: temperatura estágio inferior interno da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_topo_nafta***: temperatura de topo da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_lhtp_nafta***: temperatura da linha de topo da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***p_topo_nafta***: pressão de topo da fracionadora de nafta (unidade de pressão)<br>\n",
        "***f_carg_nafta***: vazão de carga da fracionadora de nafta (volume por unidade de tempo)<br>\n",
        "***f_refl_nafta***: vazão de refluxo de topo da fracionadora de nafta (volume por unidade de tempo)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyEV4wQUosGG"
      },
      "source": [
        "## 3.2. Carregamento do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCPbhofsE59F"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/fdamata/pucrj-machinelearning-mvp-ml/main/dataset_pv_nafta_ml.xlsx\"\n",
        "# url = 'dataset_pv_nafta_ml.xlsx'\n",
        "\n",
        "\n",
        "df = pd.read_excel(url, engine='openpyxl') # Carregamento do dataset\n",
        "display(df.head()) # Imprime as primeiras linhas do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcHpQ9s_B4n1"
      },
      "source": [
        "## 3.3. Análise de dados\n",
        "\n",
        "Nesta etapa buscamos entender a distribuição, as relações e as características das variáveis, o que é crucial para as etapas subsequentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kuan1F5EA8J"
      },
      "source": [
        "### 3.3.1. Total e tipo dos atributos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqU2LmS7osGH"
      },
      "source": [
        "O dataset possui 906 observações. Os doze (12) atributos são de tipo numérico (float)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3asH5qmbHKVe"
      },
      "outputs": [],
      "source": [
        "print(f\"Total de instâncias: {len(df)}\")\n",
        "\n",
        "tags = df.columns.to_list()\n",
        "\n",
        "# Definição de que a variável dependente está na primeira coluna\n",
        "target = tags[0]\n",
        "inputs = [tag for tag in tags if tag != target]\n",
        "\n",
        "print(f'Num. de atributos: {len(tags)}')\n",
        "# Separação entre variável dependente (target) e independentes (inputs)\n",
        "print(f'target = {target}')\n",
        "print(f'inputs = {inputs}')\n",
        "# print(f'Num. inputs = {len(inputs)}')\n",
        "\n",
        "print(\"\\nTipos de dados por coluna:\\n\")\n",
        "\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDmFNxPdosGH"
      },
      "source": [
        "### 3.3.2. Análise de comportamento temporal\n",
        "\n",
        "O objetivo é observar o comportamento das variáveis ao longo do tempo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Mhv3gRWosGH"
      },
      "outputs": [],
      "source": [
        "subplot_serie_hist(df, n_cols=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gxh5bzdosGH"
      },
      "source": [
        "É possível observar oscilações atípicas, mas nesse contexto podem voltar a repetir. Entendemos ser necessário tentar capturar esse tipo de contribuição na previsão da propriedade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-bS4g_ECPoP"
      },
      "source": [
        "### 3.3.3. Estatísticas Descritivas\n",
        "\n",
        "Estatísticas descritivas fornecem um resumo das características numéricas, incluindo média, desvio padrão, mínimo, máximo e quartis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTCOuOolC_rG"
      },
      "outputs": [],
      "source": [
        "# estasticas descritivas básicas do dataset\n",
        "print('Estatísticas descritivas:\\n')\n",
        "# Exibe as estatísticas descritivas transpostas com floats formatados para 2 casas decimais\n",
        "with pd.option_context('display.float_format', '{:.2f}'.format):\n",
        "    print(df.describe().T)\n",
        "plot_boxplot_pdf(df, n_cols=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmMjjsADosGM"
      },
      "source": [
        "É possível observar que nas unidades de engenharia utilizadas não há um único padrão para todas as distribuições.<br>\n",
        "Há algumas distribuições que aparentam a normalidade, mas que requerem avaliação específica para confirmação das hipóteses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldDSqg7WosGM"
      },
      "outputs": [],
      "source": [
        "# Loop para aplicar a função teste_n em todas as colunas do DataFrame\n",
        "print(\"Teste de normalidade para todas as variáveis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "normality_results = {}\n",
        "for col in df.columns:\n",
        "    print(f\"Variável: '{col}'\")\n",
        "    _, p_valor = teste_n(df, col)\n",
        "    normality_results[col] = \"Normal\" if p_valor > 0.05 else \"Não normal\"\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# # Criar um DataFrame com os resultados para melhor visualização\n",
        "# normality_df = pd.DataFrame.from_dict(normality_results, orient='index', columns=['Distribuição'])\n",
        "# print(\"\\nResumo dos testes de normalidade:\")\n",
        "# print(normality_df)\n",
        "\n",
        "# Calcular a proporção de variáveis com distribuição normal\n",
        "normal_count = sum(1 for status in normality_results.values() if status == \"Normal\")\n",
        "print(f\"\\nProporção de variáveis com distribuição normal: {normal_count}/{len(df.columns)} ({normal_count/len(df.columns)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWUPBcXrosGM"
      },
      "source": [
        "Nesse dataset 50% dos atributos apresentam distribuição normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_ERHJAPEZAt"
      },
      "source": [
        "### 3.3.4. Matriz de Correlação\n",
        "\n",
        "A matriz de correlação mede a força e a direção de uma relação linear entre os atributos do dataset.<br>\n",
        "Valores próximos a 1 indicam uma forte correlação positiva, -1 uma forte correlação negativa, e 0 ausência de correlação linear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1tzwXM2osGM"
      },
      "source": [
        "Para simplificar a visualização de correlação linear, o mapa de calor será realizado com o valor absoluto da correlação [0,1] ao invés de [-1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iSnofo-HF2u"
      },
      "outputs": [],
      "source": [
        "# Matriz de correlação\n",
        "print(\"\\nPara simplificar a visualização de correlação linear, o mapa de calor será realizado com o valor absoluto da correlação [0,1] ao invés de [-1, 1]\\n\")\n",
        "# print(df.corr())\n",
        "calcula_corr(df)\n",
        "pairplot_corr_hm(df, figsize=(24, 24), hist_bins=30, s=10, alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBm9kvFMosGM"
      },
      "source": [
        "### 3.3.5. Análise de VIF\n",
        "\n",
        "Análise de VIF (Variance Inflation Factor ou Fator de Inflação da Variância) é uma medida para quantificar o grau de multicolinearidade entre as variáveis independentes do dataset. A multicolinearidade ocorre quando duas ou mais variáveis independentes estão altamente correlacionadas, o que pode dificultar a estimativa dos coeficientes da regressão e afetar a interpretação dos resultados, principalmente para modelos lineares.\n",
        "\n",
        "Interpretação do VIF:\n",
        "- VIF = 1: não há correlação entre a variável $i$ e as outras variáveis. A variância não está inflacionada.\n",
        "- 1 < VIF < 5: a correlação é moderada, mas geralmente não é uma preocupação.\n",
        "- VIF ≥ 5: indica uma alta correlação e pode ser motivo de preocupação; a variável pode estar contribuindo para a multicolinearidade.\n",
        "- VIF ≥ 10: geralmente considerado um sinal forte de multicolinearidade, e pode ser necessário considerar a remoção da variável ou a aplicação de técnicas de regularização."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7vsxPaZosGM"
      },
      "outputs": [],
      "source": [
        "calcula_vif(df,target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wag-Aq8NosGN"
      },
      "source": [
        "Processo iterativo de remoção de variáveis independentes até a obtenção de maior VIF < 10 (desejavelmente < 5).<br>\n",
        "Nesse momento é importante analisar o VIF não apenas como um critério estatístico, mas também com o conhecimento de domínio.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fLrBcG1osGN"
      },
      "source": [
        "Pelo conhecimento de domínio as variáveis independentes **'t_topo_nafta'**, **'t_esup_nafta'**, **'t_eint_nafta'** e **'t_lhtp_nafta'** apresentam interdependência por representar parte do equilíbrio termodinâmico nessa seção da torre. Como também observado nas correlações, podemos aplicar redução de dimensionalidade do dataset pela supressão de alguma delas.\n",
        "\n",
        "Nesse conjunto de variáveis observamos que as 4 variáveis estão entre as 6 maiores contribuições de VIF. O objetivo é realizar a menor supressão de regressores que carreguem informações semelhantes. Das 4 variáveis **'t_eint_nafta'** é a que carrega menor VIF, indicando ser a candidata a permanecer, pois ao suprimir uma das variáveis que encontram-se com VIF maior, provavelmente se enquadrará em VIF < 5.\n",
        "\n",
        "Iniciando a exclusão da variável de maior VIF (**'t_topo_nafta**) já tivemos uma melhora significativa, no entanto ainda com VIF para as variáveis desse grupo superior a 10. Então foi excluída mais uma variável (**'t_esup_nafta'**) alcançando então VIF < 5 para as variáveis desse grupo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MQBFBcJosGN"
      },
      "outputs": [],
      "source": [
        "# df1 = df.drop(columns=['t_lhtp_nafta','t_topo_nafta', 't_esup_nafta','t_eint_nafta' ])\n",
        "df1 = df.drop(columns=['t_topo_nafta','t_esup_nafta'])\n",
        "calcula_vif(df1,target)\n",
        "# calcula_corr(df1)\n",
        "# pairplot_corr_hm(df1, figsize=(24, 24), hist_bins=30, s=10, alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnRDIsr7osGN"
      },
      "source": [
        "De forma análoga à seção anterior, o conjunto de **'t_fund_nafta'**, **'t_aque_nafta'**, **'t_carga_nafta'** e **'t_einf_nafta'** representam parte do equilíbrio termodinâmico na região inferior da torre, permitindo-nos realizar a redução de dimensionalidade como realizado anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZcnKF04osGN"
      },
      "source": [
        "As candidatas naturais pelo critério estatístico seriam **'t_fund_nafta'** ou **'t_aque_nafta'**, no entanto a **'t_fund_nafta'** melhor caracteriza o que poderia ser um indicativo de preservação ou supressão de leves na nafta, pois já está saindo da torre para produção do produto final, entanto **'t_aque_nafta'** é um reciclo que não necessariamente indica o acabamento do produto, mas sim uma oportunidade para alteração do equilíbrio termodinâmica na torre, mas que está sendo contempladas pelas demais variáveis dessa mesma seção que serão preservdas.\n",
        "\n",
        "Com isso, excluindo **'t_aque_nafta'** enquadramos o critérios da VIF e preservando melhor as informações do processo físico-químico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0I2e6g7CosGN"
      },
      "outputs": [],
      "source": [
        "# df2 = df1.drop(columns=['t_fund_nafta','t_aque_nafta','t_carg_nafta','t_einf_nafta'])\n",
        "df2 = df1.drop(columns=['t_aque_nafta'])\n",
        "calcula_vif(df2,target)\n",
        "# calcula_corr(df2)\n",
        "# pairplot_corr_hm(df2, figsize=(24, 24), hist_bins=30, s=10, alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o07n7nDosGO"
      },
      "outputs": [],
      "source": [
        "# Resumo das distribuições dos atributos remanescentes\n",
        "\n",
        "plot_boxplot_pdf(df2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mledpAbhosGP"
      },
      "source": [
        "### 3.3.6. Criação de novas características\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkgGsbL0osGP"
      },
      "source": [
        "Razão de refluxo: uma variável derivada, definida como a relação entre a vazão de refluxo de topo **'f_refl_nafta'** e a vazão de carga da fracionadora **'f_carg_nafta'** (**'f_refl_nafta'**/**'f_carg_nafta'**). Esta variável representa um parâmetros operacionais que pode ser críticos em fracionadoras, pois determina a eficiência da separação dos componentes.\n",
        "\n",
        "Eficiência da separação: valores mais altos de razão de refluxo proporcionam maior contato entre as fases líquida e vapor em cada estágio do fracionamento, resultando em melhor separação dos componentes com diferentes volatilidades.\n",
        "\n",
        "Controle da composição do produto: ao aumentar a razão de refluxo, mais componentes leves são retidos e retornam à fracionadora em vez de saírem no produto de topo, esperando-se uma elevação da pressão de vapor da nafta que sai pelo fundo da fracionadora.\n",
        "\n",
        "Estabilidade operacional: uma razão de refluxo adequada ajuda a manter condições operacionais estáveis, reduzindo flutuações na qualidade do produto.\n",
        "\n",
        "Perfil térmico da coluna: influencia diretamente o gradiente de temperatura ao longo da fracionadora, afetando a distribuição dos componentes em cada estágio.\n",
        "\n",
        "Implicações: na prática, existe um trade-off importante no ajuste da razão de refluxo:\n",
        "\n",
        "- Razão de refluxo baixa: menor consumo energético e maior capacidade de processamento, porém com qualidade de separação potencialmente comprometida.\n",
        "\n",
        "- Razão de refluxo alta: melhor separação e controle mais preciso da pressão de vapor do produto, porém com maior consumo energético e menor capacidade de processamento.\n",
        "\n",
        "Para a predição da pressão de vapor da nafta, a razão de refluxo representa uma variável derivada de alto valor preditivo, pois sintetiza em um único parâmetro a interação entre duas variáveis operacionais críticas que afetam diretamente o equilíbrio termodinâmico e a composição do produto final.\n",
        "\n",
        "A inclusão desta variável no modelo preditivo provavelmente aumentará seu poder explicativo, capturando um mecanismo de controle operacional que impacta a propriedade alvo que estamos tentando prever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmzoDAuZosGP"
      },
      "outputs": [],
      "source": [
        "df2['r_refl_nafta'] = df2['f_refl_nafta'] / df2['f_carg_nafta']\n",
        "plot_boxplot_pdf_indiv(df2, 'f_refl_nafta')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWcky1H4osGP"
      },
      "source": [
        "O novo atributo criado apresenta distribuição assimétrica à direita, com uma cauda da distribuição se estendendo nesse sentido, indicando a presença de valores mais altos menos frequentes. Esse comportamento é derivado da variável **'f_refl_nafta'** que deu origem a essa nova criação, conforme já apresentado anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkYHU7CQosGP"
      },
      "source": [
        "Vamos avaliar como ficaram as correlações e a VIF após essa nova vaiável."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa1lgM08osGP"
      },
      "outputs": [],
      "source": [
        "calcula_vif(df2,target)\n",
        "calcula_corr(df2)\n",
        "# pairplot_corr_hm(df2, figsize=(24, 24), hist_bins=30, s=10, alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR21vAuqosGP"
      },
      "source": [
        "É possível observar uma elevada correlação, conforme esperado, entre **'f_refl_nafta'** e a nova **'r_refl_nafta'**. Em função da relevância dessa nova variável, conforme explicado anteriormente e até mesmo da discreta maior correlação com a variável alvo, vamos suprimir a **'f_refl_nafta'** para reduzir a VIF (reduzir a mulcolinearidade)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jnn4yn7qosGP"
      },
      "outputs": [],
      "source": [
        "df3 = df2.drop(columns=['f_refl_nafta'])\n",
        "calcula_vif(df3,target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCMhFpoxosGP"
      },
      "source": [
        "Como a VIF ficou novamente abaixo de 5 vamos parar a remoção de variáveis nessa etapa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx_sNOk8osGQ"
      },
      "source": [
        "# 4. Separação e divisão do dataset entre features (X) e target (y)\n",
        "\n",
        "- Iniciamos aqui as definições paramétricas para o problema de aprendizado de máquina. Definimos uma semente (SEED) para garantir a reprodutibilidade. Os demais parâmetros estão relacionados aos critérios de separação dos dados entre treino e teste (SPLIT), a quantidade de 'folds\" na validação cruzada (CV_FOLDS) e por último, e não menos importante, a definição da métrica de avaliação.\n",
        "\n",
        "A escolha pelo R² é muito mais pela intruição que essa métrica trás referente à explicação de quanto da variância está sendo explicada pelo modelo, mas poderia também termos escolhidos a RMSE por tra´s também uma boa intuição, já que representa o erro absoluto médio na escala de engenharia do target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3SGv2yox3yD"
      },
      "outputs": [],
      "source": [
        "X = df3.drop(target, axis=1)\n",
        "y = df3[target].copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=SPLIT, random_state=SEED, stratify=y if PROBLEM_TYPE==\"classificacao\" else None\n",
        ")\n",
        "print(\"Treino:\", X_train.shape, \"| Teste:\", X_test.shape)\n",
        "print(\"Tipo:\", PROBLEM_TYPE)\n",
        "print(\"Target:\", target)\n",
        "print(\"Nº features:\", X.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjZ5ZJbmx3yD"
      },
      "source": [
        "Separação prévia entre dados de treino e de teste para evitar vazamento de dados. Os dados de teste são considerados intocados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEU-Ujhnx3yD"
      },
      "source": [
        "# 5 Seleção de algorítmos e definição da Baseline\n",
        "\n",
        "## 5.1 Algorítmos\n",
        "\n",
        "Selecionados previamente no início do notebook\n",
        "\n",
        "### 5.1.1 Algorítmos de regressão\n",
        "\n",
        "Algoritmos Lineares: Assumem relação linear entre features e target, são interpretáveis, mas limitados em capturar relações complexas.\n",
        "  - Linear, Ridge, Lasso, ElasticNet, SVR (kernel linear)\n",
        "\n",
        "Algoritmos Baseados em Distância: Fazem previsões com base na similaridade/distância entre pontos de dados, não fazem suposições sobre a distribuição.\n",
        "  - KNN, SVR (kernels não lineares)\n",
        "\n",
        "Algoritmos Baseados em Árvores:\n",
        "   - Árvore Única: Divide o espaço de features recursivamente, altamente interpretável mas propensa a overfitting.\n",
        "     - DecisionTree\n",
        "   - Ensembles - Bagging: Reduz a variância combinando múltiplos modelos treinados em diferentes subconjuntos dos dados.\n",
        "     - RandomForest, Bagging, ExtraTrees\n",
        "   - Ensembles - Boosting: Reduz o viés construindo modelos sequencialmente, cada um focando nos erros dos anteriores.\n",
        "     - AdaBoost, GradientBoost, XGBoost, LightGBM, CatBoost\n",
        "\n",
        "### 5.1.2 Métodos de padronização (Scalers)\n",
        "\n",
        "Considerando a estratégia de escolha dos algorítmos de regressão e sabendo que algoritmos baseados em árvores geralmente não são sensíveis à escala das features (talvez influenciados por grande assimetria), devemos nos concentrar na escolha dos scalers para os algoritmos lineares e baseados em distância. E ainda, como é sabido que outliers foram tratados na etapa de pre=processamento, RobustScaler pode ser preterido.\n",
        "\n",
        "Desta forma, vamos selecionar StandardScaler, MinMaxScaler e para avaliar eventuais impactos de assimetria na distribuição de algumas features, vamos considerar ainda uma PowerTransformer com o método de 'Yeo-Johnson', dado que esse métrodo é mais robusto que o 'Box-Cox' por contemplar tratamento de valores negativos.\n",
        "\n",
        "### 5.1.3 Métricas calculadas\n",
        "\n",
        "Em relação às métricas a escolha foi baseada na representatividade da explicação da variabilidade (R²) pelo modelo, na dimensão do erro na escala de engenharia da medida do target (RMSE), o erro quadrático médio (MSE), aonde valores mais extremos de erro tem maior penalização e também o erro absoluto (MAE).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFsiewTnx3yD"
      },
      "outputs": [],
      "source": [
        "# Carregando dos algoritmos de regressão\n",
        "filename = 'algo_configs.json'\n",
        "if os.path.getsize(filename) == 0:\n",
        "    raise ValueError(f\"O arquivo '{filename}' está vazio. Gere ou preencha o arquivo antes de carregar.\")\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    algo_configs = json.load(f)\n",
        "\n",
        "# Métodos de padronização\n",
        "scalers = SCALERS_CONFIGS\n",
        "\n",
        "# Definição das métricas de avaliação\n",
        "scoring = {\n",
        "    'r2': 'r2',\n",
        "    'rmse': 'neg_root_mean_squared_error',\n",
        "    'mse': 'neg_mean_squared_error',\n",
        "    'mae': 'neg_mean_absolute_error'\n",
        "}\n",
        "\n",
        "# Sumário das configurações carregadas\n",
        "print(\"Algorítmos considerados:\\n\")\n",
        "for i, (scaler_name,scaler) in enumerate(scalers):\n",
        "    print(f\"Scaler[{i}]: {scaler_name}\")\n",
        "\n",
        "print('')\n",
        "\n",
        "for i, (key, cfg) in enumerate(algo_configs.items()):\n",
        "    alias = cfg.get('alias')\n",
        "    model_class = cfg.get('model_class')\n",
        "    module_name = cfg.get('module')\n",
        "    default_params = cfg.get('default_params')\n",
        "    search_space = cfg.get('search_space')\n",
        "    print(f\"Algoritmos[{i}]: {alias}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILUklFEgx3yD"
      },
      "source": [
        "## 5.2 Baseline\n",
        "\n",
        "Como Baseline a escolha foi pelo modelo de regressão linear (OLS) em função da sua simplicidade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgZub5u-x3yD"
      },
      "outputs": [],
      "source": [
        "# Modelo baseline\n",
        "baseline = algo_configs[list(algo_configs.keys())[0]]\n",
        "baseline_algorithm = baseline['model_class']\n",
        "baseline_model = baseline['alias']\n",
        "print(f\"\\nBaseline - Modelo: {baseline_model}\")\n",
        "print(f\"Baseline - Algoritmo: {baseline_algorithm}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQEklRSkx3yE",
        "outputId": "c5929e48-a0a3-4cd5-b5d6-888a6e31d275"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executando HPO para: Linear - Minmax\n",
            " - Tempo estimado para Linear - Minmax: 24.71 segundos\n",
            "Executando HPO para: Ridge - Minmax\n",
            " - Tempo estimado para Ridge - Minmax: 38.16 segundos\n",
            "Executando HPO para: Lasso - Minmax\n",
            " - Tempo estimado para Lasso - Minmax: 41.36 segundos\n",
            "Executando HPO para: ElasticNet - Minmax\n",
            " - Tempo estimado para ElasticNet - Minmax: 88.36 segundos\n",
            "Executando HPO para: SVR - Minmax\n",
            " - Tempo estimado para SVR - Minmax: 133.59 segundos\n",
            "Executando HPO para: KNN - Minmax\n",
            " - Tempo estimado para KNN - Minmax: 82.99 segundos\n",
            "Executando HPO para: DecisionTree - Minmax\n",
            " - Tempo estimado para DecisionTree - Minmax: 112.71 segundos\n",
            "Executando HPO para: RandomForest - Minmax\n",
            " - Tempo estimado para RandomForest - Minmax: 420.71 segundos\n",
            "Executando HPO para: AdaBoost - Minmax\n",
            " - Tempo estimado para AdaBoost - Minmax: 314.65 segundos\n",
            "Executando HPO para: GradientBoost - Minmax\n",
            " - Tempo estimado para GradientBoost - Minmax: 379.31 segundos\n",
            "Executando HPO para: Bagging - Minmax\n",
            " - Tempo estimado para Bagging - Minmax: 295.79 segundos\n",
            "Executando HPO para: ExtraTrees - Minmax\n",
            " - Tempo estimado para ExtraTrees - Minmax: 200.66 segundos\n",
            "Executando HPO para: LightGBM - Minmax\n",
            " - Tempo estimado para LightGBM - Minmax: 195.21 segundos\n",
            "Executando HPO para: XGBoost - Minmax\n",
            " - Tempo estimado para XGBoost - Minmax: 430.43 segundos\n",
            "Executando HPO para: CatBoost - Minmax\n",
            " - Tempo estimado para CatBoost - Minmax: 911.96 segundos\n",
            "Executando HPO para: MLP - Minmax\n",
            " - Tempo estimado para MLP - Minmax: 3271.48 segundos\n",
            "Executando HPO para: Linear - Standard\n",
            " - Tempo estimado para Linear - Standard: 19.05 segundos\n",
            "Executando HPO para: Ridge - Standard\n",
            " - Tempo estimado para Ridge - Standard: 32.68 segundos\n",
            "Executando HPO para: Lasso - Standard\n",
            " - Tempo estimado para Lasso - Standard: 28.10 segundos\n",
            "Executando HPO para: ElasticNet - Standard\n",
            " - Tempo estimado para ElasticNet - Standard: 83.57 segundos\n",
            "Executando HPO para: SVR - Standard\n",
            " - Tempo estimado para SVR - Standard: 111.58 segundos\n",
            "Executando HPO para: KNN - Standard\n",
            " - Tempo estimado para KNN - Standard: 92.26 segundos\n",
            "Executando HPO para: DecisionTree - Standard\n",
            " - Tempo estimado para DecisionTree - Standard: 110.16 segundos\n",
            "Executando HPO para: RandomForest - Standard\n",
            " - Tempo estimado para RandomForest - Standard: 407.49 segundos\n",
            "Executando HPO para: AdaBoost - Standard\n",
            " - Tempo estimado para AdaBoost - Standard: 315.12 segundos\n",
            "Executando HPO para: GradientBoost - Standard\n",
            " - Tempo estimado para GradientBoost - Standard: 353.91 segundos\n",
            "Executando HPO para: Bagging - Standard\n"
          ]
        }
      ],
      "source": [
        "# --- Célula de Otimização de Hiperparâmetros com BayesSearchCV ---\n",
        "N_ITER = 50  # Número de iterações da busca. Aumentar para uma busca mais exaustiva.\n",
        "\n",
        "results = {}\n",
        "cv_results = {}\n",
        "test_results = {}\n",
        "opt_res = {}\n",
        "opt_res2 = {}\n",
        "\n",
        "# Criar o objeto de cross-validation explicitamente\n",
        "cv_splitter = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "# Criar um callback DeltaYStopper com delta pequeno para demonstração\n",
        "delta_metric = 0.0001 if METRIC_TO_OPT in ['r2'] else 0.01\n",
        "\n",
        "for scaler_name, scaler in scalers:\n",
        "    for i, (algo_name, config) in enumerate(algo_configs.items()):\n",
        "        print(f\"Executando HPO para: {config.get('alias')} - {scaler_name}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        model_class = config[\"model_class\"]\n",
        "        module_name = config[\"module\"]\n",
        "        model_cls = getattr(importlib.import_module(module_name), model_class)\n",
        "        model = model_cls(**config[\"default_params\"])\n",
        "\n",
        "        pipe = Pipeline([\n",
        "            (\"scaler\", scaler),\n",
        "            (\"model\", model)\n",
        "        ])\n",
        "\n",
        "        # Reconstruir search_space com o prefixo 'model__' para uso com Pipeline\n",
        "        search_space = {f\"model__{k}\": _deserialize_space(v) for k, v in config.get(\"search_space\", {}).items()}\n",
        "\n",
        "        opt = BayesSearchCV(\n",
        "            estimator=pipe,\n",
        "            search_spaces=search_space,\n",
        "            n_iter=N_ITER,\n",
        "            cv=cv_splitter,\n",
        "            scoring=get_regression_metric(METRIC_TO_OPT),\n",
        "            random_state=SEED,\n",
        "            n_jobs=-1,\n",
        "            return_train_score=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        opt.fit(X_train, y_train, callback=[DeltaYStopper(delta=delta_metric, n_best=15)])\n",
        "\n",
        "        end_time = time.time()\n",
        "        elapsed = end_time - start_time\n",
        "        print(f\" - Tempo estimado para {config.get('alias')} - {scaler_name}: {elapsed:.2f} segundos\")\n",
        "\n",
        "        best_model = opt.best_estimator_\n",
        "        best_idx = opt.best_index_\n",
        "\n",
        "        # Histórico completo da otimização bayesiana para cada (algoritmo, scaler)\n",
        "        opt_res[algo_name, scaler_name] = {\n",
        "            \"optimizer_result\": opt.optimizer_results_[0],\n",
        "            \"elapsed_time\": elapsed\n",
        "        }\n",
        "\n",
        "        y_pred_train = best_model.predict(X_train)\n",
        "        y_pred_test = best_model.predict(X_test)\n",
        "\n",
        "        # Resultados finais de cada combinação de algoritmo e scaler:\n",
        "        # Melhores hiperparâmetros, métricas (treino, teste, CV) e\n",
        "        # os scores dos folds apenas para o melhor modelo de cada busca.\n",
        "        results[algo_name,scaler_name] = {\n",
        "            \"params\": opt.best_params_,\n",
        "            \"scores_dev\": compute_metrics(y_train, y_pred_train),\n",
        "            \"scores_test\": compute_metrics(y_test, y_pred_test)\n",
        "        }\n",
        "        # Armazena as métricas de cada fold do treino do cross-validation apenas para o melhor modelo\n",
        "        results[algo_name, scaler_name][\"cv_scores\"] = []\n",
        "        fold_metrics = {}\n",
        "        for fold in range(CV_FOLDS):\n",
        "            fold_metrics[f\"fold_{fold}_train\"] = opt.cv_results_[f'split{fold}_train_score'][best_idx]\n",
        "            fold_metrics[f\"fold_{fold}_val\"] = opt.cv_results_[f'split{fold}_test_score'][best_idx]\n",
        "        results[algo_name, scaler_name][\"cv_scores\"].append(fold_metrics)\n",
        "\n",
        "# Salvar variáveis para execução sem necessidade de novo treinamento\n",
        "# Lista de variáveis que você deseja salvar para reuso posterior\n",
        "vars_to_save = [\n",
        "    'scalers', 'algo_configs', 'results', 'opt_res', 'N_ITER'\n",
        "]\n",
        "\n",
        "# Dicionário para armazenar as variáveis\n",
        "vars_dict = {}\n",
        "\n",
        "for var in vars_to_save:\n",
        "    if var in globals():\n",
        "        vars_dict[var] = globals()[var]\n",
        "\n",
        "# Salvar em arquivo\n",
        "joblib.dump(vars_dict, 'training_checkpoint.joblib')\n",
        "print(\"Variáveis salvas em 'training_checkpoint.joblib'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07WrpSqbx3yD"
      },
      "source": [
        "# 6 Treinamento dos modelos com otimização de hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwWpIrAax3yE"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Carregar variáveis salvas do arquivo 'training_checkpoint.joblib'\n",
        "url_cp = 'https://raw.githubusercontent.com/fdamata/pucrj-machinelearning-mvp-ml/ded71860989b54bcf713791b2de1f388e643dd2f/training_checkpoint.joblib'\n",
        "checkpoint = joblib.load(url_cp)\n",
        "\n",
        "# Exemplo de acesso às variáveis carregadas:\n",
        "scalers = checkpoint['scalers']\n",
        "algo_configs = checkpoint['algo_configs']\n",
        "results = checkpoint['results']\n",
        "opt_res = checkpoint['opt_res']\n",
        "N_ITER = checkpoint['N_ITER']\n",
        "\n",
        "print(\"Variáveis carregadas de 'training_checkpoint.joblib':\", list(checkpoint.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwZoenYvx3yE"
      },
      "source": [
        "## 6.1 Desempenho computacional do treinamento dos modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ekJudfcx3yE"
      },
      "outputs": [],
      "source": [
        "print(f\"{'Algoritmo':<25} {'Scaler':<10} {'Iterações':<10} {'Early Stop?':<12} {'Tempo (s)':<10}\")\n",
        "print(\"-\" * 75)\n",
        "for (algo, scaler), res in opt_res.items():\n",
        "    # Recupera o alias do algoritmo a partir do dicionário de configuração\n",
        "    alias = algo_configs[algo]['alias'] if algo in algo_configs else algo\n",
        "    n_iter = len(res['optimizer_result'].x_iters)\n",
        "    elapsed = res['elapsed_time']\n",
        "    early_stop = n_iter < N_ITER\n",
        "    print(f\"{alias:<25} {scaler:<10} {n_iter:<10} {'Sim' if early_stop else 'Não':<12} {elapsed:<10.2f}\")\n",
        "\n",
        "total_elapsed = sum(res['elapsed_time'] for res in opt_res.values())\n",
        "print(\"-\" * 75)\n",
        "print(f\"{'Tempo total (s):':<61}{total_elapsed:<10.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQegtojxx3yE"
      },
      "source": [
        "Importante observar que houve atuação da estratégia de parada prematura do treinamento de algoritmos em função de atingimento do critério de evolução da métrica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXi0hZtMx3yE"
      },
      "outputs": [],
      "source": [
        "# Juntar todos os resultados de otimização em uma lista\n",
        "optimizer_results = [res['optimizer_result'] for res in opt_res.values()]\n",
        "\n",
        "# Calcular os limites globais do eixo y\n",
        "all_func_vals = []\n",
        "for res in optimizer_results:\n",
        "\tall_func_vals.extend(res.func_vals)\n",
        "ymin = min(all_func_vals)\n",
        "ymax = max(all_func_vals)\n",
        "margin = 0.05 * (ymax - ymin) if ymax > ymin else 0.1\n",
        "ymin_plot = ymin - margin\n",
        "ymax_plot = ymax + margin\n",
        "\n",
        "# Plotar todas as curvas de convergência em um único gráfico\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_convergence(*optimizer_results, ax=ax)\n",
        "ax.set_ylim(ymin_plot, ymax_plot)\n",
        "ax.set_title('Convergência da Otimização Bayesiana (todos os modelos)', fontsize=14)\n",
        "ax.set_xlabel('Iteração')\n",
        "ax.set_ylabel('Score')\n",
        "ax.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHjqMR73x3yE"
      },
      "source": [
        "## 6.2 Desempenho das métricas de treino e teste dos modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhppQ4U_x3yE"
      },
      "outputs": [],
      "source": [
        "# Preparar dados para boxplot dos folds de treino, ponto do teste e linhas baseline e melhor teste global\n",
        "# Encontrar o melhor modelo (menor erro para métricas de minimização, maior valor para maximização)\n",
        "\n",
        "data = []\n",
        "best_key = None\n",
        "best_score = None\n",
        "\n",
        "for (algo, scaler), result in results.items():\n",
        "    # Adiciona os scores de validação dos folds\n",
        "    for fold_score in result['cv_scores'][0]:\n",
        "        if fold_score.startswith('fold_') and fold_score.endswith('_val'):\n",
        "            data.append({'Algoritmo': algo_configs[algo]['alias'], 'Scaler': scaler, 'Score': result['cv_scores'][0][fold_score], 'Type': 'CV'})\n",
        "    # Adiciona o score de teste final\n",
        "    test_score = result['scores_test'][METRIC_TO_OPT]\n",
        "    data.append({'Algoritmo': algo_configs[algo]['alias'], 'Scaler': scaler, 'Score': test_score, 'Type': 'Test'})\n",
        "\n",
        "    score = result['scores_test'][METRIC_TO_OPT]\n",
        "    if METRIC_TO_OPT in MINIMIZE_METRICS:\n",
        "        if (best_score is None) or (score < best_score):\n",
        "            best_score = score\n",
        "            best_key = (algo, scaler)\n",
        "    else:\n",
        "        if (best_score is None) or (score > best_score):\n",
        "            best_score = score\n",
        "            best_key = (algo, scaler)\n",
        "\n",
        "df_box = pd.DataFrame(data)\n",
        "\n",
        "# Boxplot agrupado por scaler\n",
        "boxplot_algo_grouped_by_scaler(df_box, METRIC_TO_OPT, ncols= len(scalers), figsize_per_plot=(6,4))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIM_m6qcx3yE"
      },
      "outputs": [],
      "source": [
        "# Plot 2: Influencia hiperparametro\n",
        "\n",
        "for algo, res in opt_res.items():\n",
        "    print(f\"Algoritmo: {algo}\")\n",
        "    plot_objective(res['optimizer_result'], n_points=40, size=3)\n",
        "    plt.suptitle('Impacto dos Hiperparâmetros')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7B2ONE_x3yE"
      },
      "outputs": [],
      "source": [
        "# Recuperar o melhor modelo considerando a métrica escolhida (METRIC_TO_OPT)\n",
        "# Usar os resultados já armazenados no dicionário 'results' e a lista de scalers\n",
        "\n",
        "# 2. Encontrar o melhor modelo (menor erro para métricas de minimização, maior valor para maximização)\n",
        "best_key = None\n",
        "best_score = None\n",
        "\n",
        "for (algo, scaler), res in results.items():\n",
        "    score = res['scores_test'][METRIC_TO_OPT]\n",
        "    if METRIC_TO_OPT in MINIMIZE_METRICS:\n",
        "        if (best_score is None) or (score < best_score):\n",
        "            best_score = score\n",
        "            best_key = (algo, scaler)\n",
        "    else:\n",
        "        if (best_score is None) or (score > best_score):\n",
        "            best_score = score\n",
        "            best_key = (algo, scaler)\n",
        "\n",
        "# 3. Detalhar o melhor modelo, scaler e hiperparâmetros\n",
        "if best_key is not None:\n",
        "    best_algo, best_scaler = best_key\n",
        "    best_params = results[best_key]['params']\n",
        "    print(f\"Melhor modelo considerando a métrica '{METRIC_TO_OPT}':\")\n",
        "    print(f\"  Algoritmo: {best_algo}\")\n",
        "    print(f\"  Scaler: {best_scaler}\")\n",
        "    print(f\"  Score de teste ({METRIC_TO_OPT}): {best_score:.4f}\")\n",
        "    print(f\"  Melhores hiperparâmetros encontrados:\")\n",
        "    for param, value in best_params.items():\n",
        "        print(f\"    {param}: {value}\")\n",
        "else:\n",
        "    print(\"Não foi possível encontrar o melhor modelo.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37J6LUUwx3yE"
      },
      "outputs": [],
      "source": [
        "# --- Avaliação do Melhor Modelo no Conjunto de Teste (Dados Não Vistos) ---\n",
        "\n",
        "# Recuperar o melhor modelo e scaler do dicionário 'results' (já definido em célula anterior)\n",
        "# O best_key, best_algo, best_scaler, best_params já foram definidos na célula anterior\n",
        "\n",
        "\n",
        "\n",
        "# 1. Recuperar o melhor modelo e scaler\n",
        "best_algo, best_scaler = best_key\n",
        "best_params = results[best_key]['params']\n",
        "\n",
        "# Encontrar o scaler correspondente\n",
        "scaler_obj = None\n",
        "for name, scaler in scalers:\n",
        "    if name == best_scaler:\n",
        "        scaler_obj = scaler\n",
        "        break\n",
        "\n",
        "# Encontrar a classe do modelo e os parâmetros default\n",
        "model_class = None\n",
        "module_name = None\n",
        "default_params = None\n",
        "for algo_name, config in algo_configs.items():\n",
        "    if algo_name == best_algo:\n",
        "        model_class = config['model_class']\n",
        "        module_name = config['module']\n",
        "        default_params = config['default_params']\n",
        "        break\n",
        "\n",
        "if scaler_obj is None or model_class is None or module_name is None:\n",
        "    print(\"Erro: Não foi possível encontrar o modelo ou scaler correspondente.\")\n",
        "else:\n",
        "    # Importar a classe do modelo dinamicamente\n",
        "    import importlib\n",
        "    model_cls = getattr(importlib.import_module(module_name), model_class)\n",
        "    model_obj = model_cls(**default_params)\n",
        "\n",
        "    # 2. Criar o pipeline com o melhor scaler e modelo\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', scaler_obj),\n",
        "        ('regressor', model_obj)\n",
        "    ])\n",
        "\n",
        "    # 3. Definir os melhores hiperparâmetros encontrados\n",
        "    final_params = {}\n",
        "    for key, value in best_params.items():\n",
        "        if not key.startswith('regressor__'):\n",
        "            final_params[f'regressor__{key.replace(\"model__\", \"\")}'] = value\n",
        "        else:\n",
        "            final_params[key] = value\n",
        "    pipeline.set_params(**final_params)\n",
        "\n",
        "    # 4. Treinar o pipeline com os dados de treino\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # 5. Fazer previsões no conjunto de teste\n",
        "    y_pred_test = pipeline.predict(X_test)\n",
        "\n",
        "    # 6. Calcular as métricas de avaliação\n",
        "    r2_test = r2_score(y_test, y_pred_test)\n",
        "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "    rmse_test = np.sqrt(mse_test)\n",
        "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "    # 7. Exibir os resultados\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Resultados do Melhor Modelo no Conjunto de Teste (X_test)\")\n",
        "    model_name_in_pipeline = pipeline.named_steps['regressor'].__class__.__name__\n",
        "    print(f\"Melhor scaler: {best_scaler}\")\n",
        "    print(f\"Modelo: {model_name_in_pipeline}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    metrics_data = [\n",
        "        [\"R² (R-squared)\", f\"{r2_test:.4f}\"],\n",
        "        [\"MSE (Mean Squared Error)\", f\"{mse_test:.4f}\"],\n",
        "        [\"RMSE (Root Mean Squared Error)\", f\"{rmse_test:.4f}\"],\n",
        "        [\"MAE (Mean Absolute Error)\", f\"{mae_test:.4f}\"]\n",
        "    ]\n",
        "    headers = [\"Métrica\", \"Valor no Conjunto de Teste\"]\n",
        "\n",
        "    print(tabulate(metrics_data, headers=headers, tablefmt=\"grid\"))\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMNMJXehx3yE"
      },
      "outputs": [],
      "source": [
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.base import clone\n",
        "# from tabulate import tabulate\n",
        "\n",
        "\n",
        "# 1. Recuperar o melhor modelo e scaler\n",
        "best_algo, best_scaler = best_key\n",
        "best_params = results[best_key]['params']\n",
        "\n",
        "# Encontrar o scaler correspondente\n",
        "scaler_obj = None\n",
        "for name, scaler in scalers:\n",
        "    if name == best_scaler:\n",
        "        scaler_obj = scaler\n",
        "        break\n",
        "\n",
        "# Encontrar a classe do modelo e os parâmetros default\n",
        "model_class = None\n",
        "module_name = None\n",
        "default_params = None\n",
        "for algo_name, config in algo_configs.items():\n",
        "    if algo_name == best_algo:\n",
        "        model_class = config['model_class']\n",
        "        module_name = config['module']\n",
        "        default_params = config['default_params']\n",
        "        break\n",
        "\n",
        "if scaler_obj is None or model_class is None or module_name is None:\n",
        "    print(\"Erro: Não foi possível encontrar o modelo ou scaler correspondente.\")\n",
        "else:\n",
        "    # Importar a classe do modelo dinamicamente\n",
        "    model_cls = getattr(importlib.import_module(module_name), model_class)\n",
        "    model_obj = model_cls(**default_params)\n",
        "\n",
        "    # Criar o pipeline com o melhor scaler e modelo\n",
        "    final_pipeline = Pipeline([\n",
        "        ('scaler', scaler_obj),\n",
        "        ('regressor', model_obj)\n",
        "    ])\n",
        "\n",
        "    # Definir os melhores hiperparâmetros encontrados\n",
        "    final_params = {}\n",
        "    for key, value in best_params.items():\n",
        "        if not key.startswith('regressor__'):\n",
        "            final_params[f'regressor__{key.replace(\"model__\", \"\")}'] = value\n",
        "        else:\n",
        "            final_params[key] = value\n",
        "    final_pipeline.set_params(**final_params)\n",
        "\n",
        "    # Retreinar o pipeline com todos os dados (X e y)\n",
        "    print(\"=\"*80)\n",
        "    print(\"Retreinando o melhor modelo com o dataset completo (X, y)\")\n",
        "    print(\"=\"*80)\n",
        "    final_pipeline.fit(X, y)\n",
        "\n",
        "    # Fazer previsões nos próprios dados de treinamento para avaliar o ajuste\n",
        "    y_pred_full = final_pipeline.predict(X)\n",
        "\n",
        "    # Calcular e exibir as métricas de desempenho no conjunto de dados completo\n",
        "    r2_full = r2_score(y, y_pred_full)\n",
        "    mse_full = mean_squared_error(y, y_pred_full)\n",
        "    rmse_full = np.sqrt(mse_full)\n",
        "    mae_full = mean_absolute_error(y, y_pred_full)\n",
        "\n",
        "    metrics_data = [\n",
        "        [\"R² (R-squared)\", f\"{r2_full:.4f}\"],\n",
        "        [\"MSE (Mean Squared Error)\", f\"{mse_full:.4f}\"],\n",
        "        [\"RMSE (Root Mean Squared Error)\", f\"{rmse_full:.4f}\"],\n",
        "        [\"MAE (Mean Absolute Error)\", f\"{mae_full:.4f}\"]\n",
        "    ]\n",
        "    headers = [\"Métrica\", \"Valor no Dataset Completo\"]\n",
        "\n",
        "    print(tabulate(metrics_data, headers=headers, tablefmt=\"grid\"))\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Salvar o modelo final em disco\n",
        "    model_filename = f'modelo_final_{target}.joblib'\n",
        "    joblib.dump(final_pipeline, model_filename)\n",
        "    print(f\"Modelo final salvo em '{model_filename}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-YfQwZwx3yE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- Gráficos de Análise do Modelo Final ---\n",
        "\n",
        "# 1. Calcular o erro\n",
        "prediction_error = y - y_pred_full\n",
        "\n",
        "# 2. Criar os subplots\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=1,\n",
        "    shared_xaxes=True,\n",
        "    vertical_spacing=0.1,\n",
        "    subplot_titles=(\n",
        "        \"Tendência do Erro de Predição (Real - Previsto)\",\n",
        "        \"Comparativo: Valores Reais vs. Valores Previstos\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# --- Gráfico 1: Tendência do Erro ---\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=y.index,\n",
        "        y=prediction_error,\n",
        "        mode='lines',\n",
        "        name='Erro',\n",
        "        line=dict(color='indianred')\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "# Adiciona uma linha em y=0 para referência de erro nulo\n",
        "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", row=1, col=1)\n",
        "\n",
        "# --- Gráfico 2: Real vs. Previsto ---\n",
        "# Adiciona a linha dos valores reais\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=y.index,\n",
        "        y=y,\n",
        "        mode='lines',\n",
        "        name='Valor Real',\n",
        "        line=dict(color='royalblue')\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "# Adiciona a linha dos valores previstos\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=y.index,\n",
        "        y=y_pred_full,\n",
        "        mode='lines',\n",
        "        name='Valor Previsto',\n",
        "        line=dict(color='darkorange', dash='dot')\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# 3. Atualizar o layout geral\n",
        "fig.update_layout(\n",
        "    height=700,\n",
        "    title_text=\"Análise Gráfica do Desempenho do Modelo Final no Dataset Completo\",\n",
        "    showlegend=True,\n",
        "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
        ")\n",
        "\n",
        "# Atualizar os títulos dos eixos\n",
        "fig.update_yaxes(title_text=\"Erro (Real - Previsto)\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Valor da Variável Target\", row=2, col=1)\n",
        "fig.update_xaxes(title_text=\"Índice da Amostra\", row=2, col=1)\n",
        "\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWAtlOxSx3yF"
      },
      "outputs": [],
      "source": [
        "# --- Salvando e Carregando o Modelo Final ---\n",
        "\n",
        "# 1. Definir o nome do arquivo para salvar o modelo\n",
        "model_filename = f'modelo_final_{target}.joblib'\n",
        "\n",
        "# 2. Salvar o pipeline treinado em um arquivo\n",
        "print(\"=\"*80)\n",
        "print(f\"Salvando o modelo final em '{model_filename}'...\")\n",
        "joblib.dump(final_pipeline, model_filename)\n",
        "print(\"Modelo salvo com sucesso!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "# --- Demonstração de Carregamento e Uso ---\n",
        "\n",
        "# 3. Carregar o modelo a partir do arquivo\n",
        "print(\"\\nCarregando o modelo para demonstração...\")\n",
        "loaded_model = joblib.load(model_filename)\n",
        "print(\"Modelo carregado com sucesso.\")\n",
        "\n",
        "# 4. Usar o modelo carregado para fazer uma predição em um novo dado\n",
        "#    (usaremos a primeira linha do dataset X como exemplo)\n",
        "sample_data = X.head(1)\n",
        "prediction = loaded_model.predict(sample_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Exemplo de predição com o modelo carregado:\")\n",
        "print(f\"Dados de entrada (primeira linha do dataset):\\n{sample_data.to_string()}\")\n",
        "print(f\"\\nValor Predito: {prediction[0]:.4f}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY_GDjf1G-PM"
      },
      "source": [
        "# Conclusão\n",
        "\n",
        "O presente estudo demonstrou a importância de um processo rigoroso de análise exploratória, tratamento e pré-processamento dos dados para problemas de regressão em ambientes industriais. A partir do dataset de Pressão de Vapor da Nafta, foi possível identificar, tratar e justificar a remoção de outliers, realizar imputação de valores faltantes, criar variáveis derivadas relevantes (como a razão de refluxo), além de aplicar técnicas de transformação para redução de assimetrias e multicolinearidade.\n",
        "\n",
        "A análise estatística e visual das variáveis, aliada ao conhecimento de domínio, permitiu selecionar os atributos mais representativos para a modelagem preditiva, reduzindo redundâncias e otimizando o conjunto de dados para futuros algoritmos de machine learning. As etapas de normalização e padronização garantiram que as diferentes escalas das variáveis não influenciassem indevidamente o desempenho dos modelos.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmYX3PLx58Jg"
      },
      "source": [
        "#### MVP Machine Learning & Analytics\n",
        "\n",
        "**Nome:** Fabiano da Mata Almeida<br>\n",
        "**Matrícula:** 4052025000952<br>\n",
        "**Dataset:** Pressão de Vapor da nafta\n",
        "\n",
        "**Nota sobre confidencialidade e descaracterização dos dados:**  \n",
        "> Para garantir a confidencialidade e o respeito à privacidade, todos os dados utilizados neste estudo foram devidamente descaracterizados, não permitindo a identificação na sua unidade de medida original ou informações sensíveis.<br>\n",
        "O uso desse dataset segue as boas práticas de ética em ciência de dados, assegurando que nenhuma informação pessoal ou confidencial seja exposta durante as análises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW_JS-EEBFaR"
      },
      "source": [
        "# Descrição do problema\n",
        "\n",
        "Este conjunto de dados, **Pressão de Vapor da nafta**, contém informações detalhadas sobre 950 observações dessa corrente derivada do processo de fracionamento de petróleo, incluindo variáveis físico-químicas de processo e da propriedade da corrente.<br>\n",
        "\n",
        "A variável alvo, **Pressão de Vapor da nafta**, é uma propriedade fundamental que representa a pressão exercida pelo vapor quando em equilíbrio com sua fase líquida a uma determinada temperatura. Esta propriedade é crítica para caracterizar a volatilidade da corrente, impactando diretamente nos requisitos de armazenamento, segurança, transporte e adequação às especificações regulatórias do produto produzido.<br>\n",
        "\n",
        "O dataset fornece uma base de dados para uma análise exploratória mais simplificada, dado que a grande parte dessa etapa já fora realizada na SPRINT anterior de Análise Exploratório e Boas Práticas, e na sequência o estudo para identificação o melhor modelo de Aprendizado de Máquina para representar o fenômeno e a previsibilidade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm6mOo5PBYwr"
      },
      "source": [
        "## Hipóteses do problema\n",
        "\n",
        "A **Pressão de Vapor da nafta** é uma propriedade crítica influenciada por múltiplos fatores ao longo da cadeia de processamento de petróleo. Este estudo parte da premissa que essa propriedade é determinada por condições operacionais diretas da fracionadora de nafta.\n",
        "\n",
        "O processo de fracionamento de derivados de petróleo opera sob princípios termodinâmicos de equilíbrio líquido-vapor, onde a distribuição dos componentes depende fundamentalmente das condições de pressão e temperatura, que alteram a volatilidade relativa dos hidrocarbonetos presentes. \n",
        "\n",
        "#### Hipóteses sobre variáveis da fracionadora de nafta\n",
        "**H1**: Existe correlação entre a temperatura de topo da fracionadora de nafta (***t_topo_nafta*** e ***t_lhtp_nafta***) e a pressão de vapor do produto (***pv_nafta***)?<br>\n",
        "&emsp;- Temperaturas mais elevadas promovem maior remoção de componentes leves.\n",
        "\n",
        "**H2**: A pressão de topo da fracionadora de nafta (***p_topo_nafta***) apresenta correlação com a pressão de vapor do produto final  (***pv_nafta***)?<br>\n",
        "&emsp;- Pressões mais baixas favorecem a remoção dos componentes mais leves.\n",
        "\n",
        "**H3**: A vazão de refluxo (***f_refl_nafta***) e/ou a vazão de carga (***f_carg_nafta***) da fracionadora de nafta impactam a pressão de vapor (***pv_nafta***)?<br>\n",
        "&emsp;- Maiores taxas de refluxo ou fluxo ascentes de vapores promovem melhor separação dos componentes.\n",
        "\n",
        "**H4**: Existe alguma temperatura relacionada com a fracionadora de nafta (***t_carg_nafta***, ***t_fund_nafta***, ***t_aque_nafta***, ***t_esup_nafta***, ***t_eint_nafta*** e ***t_einf_nafta***) que mostre maior correlação com a pressão de vapor (***pv_nafta***)?<br>\n",
        "&emsp;- É possível que haja alguma temperatura que seja mais significantemente influenciadora na pressão de vapor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1zNNZt6BfdF"
      },
      "source": [
        "## Tipo de problema\n",
        "\n",
        "Este é um problema típico de **regressão supervisionada** (modelagem preditiva) em ambiente industrial/processo químico, onde as variáveis de processo podem ter relações complexas com a propriedade que se deseja prever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N91bTLbiBxrm"
      },
      "source": [
        "## Seleção de dados\n",
        "\n",
        "O dataset **Pressão de Vapor da nafta** é um conjunto de dados previamente tratado na SPRINT de Análise de Dados e Boas Práticas. Será necessária uma breve análise exploratória dos dados de forma a torná-lo uma fonte de dados ainda mais curada para o uso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC6QJHhWBnzH"
      },
      "source": [
        "## Atributos do dataset\n",
        "\n",
        "O dataset Pressão de Vapor de nafta contém 950 amostras com dezoito (18) atributos:\n",
        "\n",
        "***pv_nafta***: pressão de vapor da nafta (unidade de pressão)<br>\n",
        "***t_carg_nafta***: temperatura da carga da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_fund_nafta***: temperatura do fundo da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_aque_nafta***: temperatura do aquecedor de fundo da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_esup_nafta***: temperatura estágio superior interno da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_eint_nafta***: temperatura estágio intermediário interno da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_einf_nafta***: temperatura estágio inferior interno da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_topo_nafta***: temperatura de topo da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***t_lhtp_nafta***: temperatura da linha de topo da fracionadora de nafta (unidade de temperatura)<br>\n",
        "***p_topo_nafta***: pressão de topo da fracionadora de nafta (unidade de pressão)<br>\n",
        "***f_carg_nafta***: vazão de carga da fracionadora de nafta (volume por unidade de tempo)<br>\n",
        "***f_refl_nafta***: vazão de refluxo de topo da fracionadora de nafta (volume por unidade de tempo)<br>\n",
        "\n",
        "Temperaturas em unidade genérica<sup>(1)</sup>\n",
        "\n",
        "<div style=\"margin-left: 30px\">\n",
        "<sup>(1)</sup>\n",
        "Formulação generalizada para conversão entre quaisquer duas escalas de temperatura, desde que se conheça/estabeleça os pontos de congelamento e ebulição da água em ambas as escalas.<br>\n",
        "Para possibilitar a anonimização da temperatura, guardando as mesmas relações termodinâmicas, utilizou-se dessa abordagem para tal.\n",
        "\n",
        "Definição das Variáveis:<br><br>\n",
        "$T_{R_C}$ = Temperatura de Congelamento na escala de Referência<br>\n",
        "$T_{R_E}$ = Temperatura de Ebulição na escala de Referência<br>\n",
        "$T_{G_C}$ = Temperatura de Congelamento na escala Genérica<br>\n",
        "$T_{G_E}$ = Temperatura de Ebulição na escala Genérica<br>\n",
        "\n",
        "Onde:<br><br>\n",
        "$T_G$ é a temperatura na escala Genérica<br>\n",
        "$T_R$ é a temperatura na escala de Referência</div>\n",
        "\n",
        "$$\\frac{(T_G - T_{G_C})}{(T_{G_E} - T_{G_C})} =  \\frac{(T_R - T_{R_C})}{(T_{R_E} - T_{R_C})}$$\n",
        "<br>\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://raw.githubusercontent.com/fdamata/pucrj-analisededados-mvp-eda/refs/heads/main/images/conv_temp2.png\" alt=\"Diagrama mostrando a relação de conversão entre escalas de temperatura, com setas conectando pontos de congelamento e ebulição nas escalas genérica e de referência, ilustrando a proporcionalidade entre as diferenças de temperatura. O ambiente é neutro e acadêmico, sem elementos emocionais ou texto adicional.\" width=\"400\"/>\n",
        "</p>\n",
        "<!-- $$T_G = T_{CG} + \\frac{(T_{EG} - T_{CG})(T_R - T_{CR})}{(T_{ER} - T_{CR})}$$ -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DObGnkf0bJBh"
      },
      "source": [
        "# Importação de bibliotecas, definição de funções e carga de dados\n",
        "\n",
        "Esta seção consolida todas as importações de bibliotecas necessárias, definições das funções utilizadas e algumas configurações iniciais globais, além da carga do dataset para a análise, visualização e pré-processamento dos dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCohWn2jbkDB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import Normalize\n",
        "from matplotlib.cm import viridis\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm, kstest, skew\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
        "\n",
        "# Configuração para não exibir os warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Configurações de exibição\n",
        "pd.options.display.float_format = '{:.2f}'.format  # Define o formato de exibição dos números float no pandas para duas casas decimais\n",
        "pd.set_option('display.expand_frame_repr', False)  # Não quebra a representação do dataframe\n",
        "np.set_printoptions(precision=8, suppress=True, floatmode='maxprec') # Define o formato de exibição dos números float no numpy para oito casas decimais e sem notação científica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGdp0MmhosGF"
      },
      "source": [
        "Definição das função que serão utilizadas ao longo do trabalho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO15k761osGF"
      },
      "outputs": [],
      "source": [
        "# Declaração de funções\n",
        "\n",
        "def histo(df, column_name, bins=None):\n",
        "    \"\"\"\n",
        "    Plota o histograma de uma coluna numérica de um DataFrame e sobrepõe uma curva normal teórica para comparação visual.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados.\n",
        "    column_name : str\n",
        "        Nome da coluna a ser analisada.\n",
        "    bins : int, str ou None, opcional\n",
        "        Número de bins do histograma. Se None ou vazio, utiliza 20 como padrão.\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Verifica se a coluna existe no DataFrame.\n",
        "    - Plota o histograma dos valores da coluna, normalizado (density=True).\n",
        "    - Sobrepõe a curva normal teórica baseada na média e desvio padrão da coluna.\n",
        "    - Adiciona rótulos, título e legenda ao gráfico.\n",
        "    - Exibe o gráfico resultante.\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    histo(df, 'pv_nafta', bins=30)\n",
        "    \"\"\"\n",
        "    # Verifique se a coluna existe no DataFrame\n",
        "    if column_name not in df.columns:\n",
        "        print(f\"A coluna '{column_name}' não existe no DataFrame.\")\n",
        "        return\n",
        "\n",
        "    # Verifica se bins é None ou vazio\n",
        "    if bins is None or bins == '':\n",
        "        bins = 20  # Valor padrão\n",
        "\n",
        "    # Crie um histograma dos valores\n",
        "    plt.hist(df[column_name], bins=bins, density=True, color='grey', alpha=0.6, edgecolor='black')\n",
        "\n",
        "    # Adicione a curva normal teórica\n",
        "    x_vals = np.linspace(df[column_name].min(), df[column_name].max(), 100)\n",
        "    y_vals = norm.pdf(x_vals, loc=np.mean(df[column_name]), scale=np.std(df[column_name]))\n",
        "    plt.plot(x_vals, y_vals, color='red', label='Curva Normal Teórica')\n",
        "\n",
        "    plt.xlabel(column_name)\n",
        "    plt.ylabel('Frequência')\n",
        "    plt.title(f'Histograma com Curva Normal Teórica de \"{column_name}\"')\n",
        "    plt.legend()\n",
        "\n",
        "    # Exiba o gráfico\n",
        "    plt.show()\n",
        "\n",
        "def serie_hist(df, column_names):\n",
        "    \"\"\"\n",
        "    Plota a(s) série(s) temporal(is) de uma ou mais colunas de um DataFrame, com linhas de referência estatísticas.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados.\n",
        "    column_names : str ou list\n",
        "        Nome da coluna (ou lista de colunas) a ser(em) analisada(s).\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Plota a série temporal da(s) coluna(s) especificada(s) usando Plotly.\n",
        "    - Se apenas uma coluna for fornecida, adiciona linhas de referência para o valor mínimo, máximo, limites do IQR (whiskers).\n",
        "    - As linhas de referência ajudam a identificar outliers e o comportamento estatístico da série ao longo do tempo.\n",
        "    - Exibe o gráfico interativo.\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    serie_hist(df, 't_forn_atm')\n",
        "    serie_hist(df, ['pv_nafta', 'f_carg_nafta'])\n",
        "    \"\"\"\n",
        "    if isinstance(column_names, list):\n",
        "        title = ', '.join(column_names)\n",
        "    else:\n",
        "        title = column_names\n",
        "\n",
        "    fig = px.line(\n",
        "        data_frame = df[column_names].dropna(),\n",
        "        title = title\n",
        "    )\n",
        "\n",
        "    if not isinstance(column_names, list):\n",
        "\n",
        "        min_value = df[column_names].min()\n",
        "        max_value = df[column_names].max()\n",
        "        q1 = df[column_names].quantile(0.25)\n",
        "        q3 = df[column_names].quantile(0.75)\n",
        "        iqr = q3-q1\n",
        "        loval = q1 - (1.5 * iqr)\n",
        "        hival = q3 + (1.5 * iqr)\n",
        "\n",
        "        fig.add_shape(\n",
        "            type = 'line',\n",
        "            x0 = df.index.min(),\n",
        "            y0 = min_value,\n",
        "            x1 = df.index.max(),\n",
        "            y1 = min_value,\n",
        "            line = dict(\n",
        "                color = 'gray',\n",
        "                width = 1,\n",
        "                dash = 'dash'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        fig.add_shape(\n",
        "            type = 'line',\n",
        "            x0 = df.index.min(),\n",
        "            y0 = max_value,\n",
        "            x1 = df.index.max(),\n",
        "            y1 = max_value,\n",
        "            line = dict(\n",
        "                color = 'gray',\n",
        "                width = 1,\n",
        "                dash = 'dash'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        fig.add_shape(\n",
        "            type = 'line',\n",
        "            x0 = df.index.min(),\n",
        "            y0 = loval,\n",
        "            x1 = df.index.max(),\n",
        "            y1 = loval,\n",
        "            line = dict(\n",
        "                color = 'orange',\n",
        "                width = 1,\n",
        "                dash = 'dash'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        fig.add_shape(\n",
        "            type = 'line',\n",
        "            x0 = df.index.min(),\n",
        "            y0 = hival,\n",
        "            x1 = df.index.max(),\n",
        "            y1 = hival,\n",
        "            line = dict(\n",
        "                color = 'orange',\n",
        "                width = 1,\n",
        "                dash = 'dash'\n",
        "            )\n",
        "        )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "def box_p(df, column_name, lower_lim=None, upper_lim=None):\n",
        "    \"\"\"\n",
        "    Plota um boxplot horizontal para a coluna especificada de um DataFrame.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados.\n",
        "    column_name : str\n",
        "        Nome da coluna a ser visualizada no boxplot.\n",
        "    lower_lim : float, str ou None, opcional\n",
        "        Limite inferior do eixo x. Se None ou 'auto', usa o mínimo dos dados.\n",
        "    upper_lim : float, str ou None, opcional\n",
        "        Limite superior do eixo x. Se None ou 'auto', usa o máximo dos dados.\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Cria um boxplot horizontal usando a biblioteca Plotly Express.\n",
        "    - Remove automaticamente valores NaN da coluna antes de plotar.\n",
        "    - Adiciona um título igual ao nome da coluna.\n",
        "    - Utiliza a opção 'notched' para exibir o intervalo de confiança da mediana.\n",
        "    - Permite ajustar os limites do eixo x manualmente ou automaticamente.\n",
        "    - Exibe o gráfico interativo.\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    box_p(df, 'pv_nafta')\n",
        "    box_p(df, 'pv_nafta', lower_lim=50, upper_lim=150)\n",
        "    \"\"\"\n",
        "    # Criar o boxplot\n",
        "    fig = px.box(\n",
        "        data_frame=df[column_name].dropna(),\n",
        "        orientation='h',\n",
        "        title=column_name,\n",
        "        notched=True,\n",
        "    )\n",
        "\n",
        "    # Processar os limites do eixo x\n",
        "    if upper_lim is not None and upper_lim != 'auto':\n",
        "        try:\n",
        "            x_upper = float(upper_lim)\n",
        "            fig.update_xaxes(range=[None, x_upper])\n",
        "        except (ValueError, TypeError):\n",
        "            pass  # Se não for possível converter para float, mantém o limite automático\n",
        "\n",
        "    if lower_lim is not None and lower_lim != 'auto':\n",
        "        try:\n",
        "            x_lower = float(lower_lim)\n",
        "            current_range = fig.layout.xaxis.range\n",
        "            fig.update_xaxes(range=[x_lower, current_range[1] if current_range else None])\n",
        "        except (ValueError, TypeError):\n",
        "            pass  # Se não for possível converter para float, mantém o limite automático\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "def plot_boxplot_pdf(df, lower_lim=None, upper_lim=None, n_cols=4):\n",
        "    \"\"\"\n",
        "    Plota boxplot horizontal e PDF (histograma + curva normal) para todas as colunas numéricas do DataFrame.\n",
        "    O layout é de múltiplas linhas e n_cols colunas de subplots: para cada coluna, boxplot em cima, PDF embaixo.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados.\n",
        "    lower_lim : float, str ou None, opcional\n",
        "        Limite inferior do eixo x. Se None ou 'auto', usa o mínimo dos dados.\n",
        "    upper_lim : float, str ou None, opcional\n",
        "        Limite superior do eixo x. Se None ou 'auto', usa o máximo dos dados.\n",
        "    n_cols : int, opcional\n",
        "        Número de colunas no layout dos subplots. O padrão é 4.\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Para cada variável numérica, cria dois gráficos alinhados verticalmente:\n",
        "      - Um boxplot horizontal no topo para visualizar a distribuição e outliers\n",
        "      - Um histograma com curva normal teórica abaixo para visualizar a distribuição de frequência\n",
        "    - Adiciona linhas de referência nos gráficos (média, mediana, ±3σ)\n",
        "    - Permite ajustar os limites dos eixos manualmente ou automaticamente\n",
        "    - Remove automaticamente valores NaN antes de plotar\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    plot_boxplot_pdf(df)\n",
        "    plot_boxplot_pdf(df, upper_lim=100, lower_lim=0)  # Define limites fixos para todas as variáveis\n",
        "    plot_boxplot_pdf(df, n_cols=3)  # Altera o número de colunas no layout\n",
        "    \"\"\"\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    n_vars = len(numeric_cols)\n",
        "    n_rows = int(np.ceil(n_vars / n_cols))\n",
        "\n",
        "    # Dobrar a altura dos plots da PDF (segunda linha de cada variável)\n",
        "    height_ratios = []\n",
        "    for _ in range(n_rows):\n",
        "        height_ratios.extend([1, 4])  # boxplot:1, pdf:4\n",
        "\n",
        "    fig, axes = plt.subplots(\n",
        "        n_rows * 2,\n",
        "        n_cols,\n",
        "        figsize=(6 * n_cols, 5 * n_rows),\n",
        "        gridspec_kw={'height_ratios': height_ratios}\n",
        "    )\n",
        "\n",
        "    axes = np.array(axes).reshape(n_rows * 2, n_cols)\n",
        "\n",
        "    for idx, column in enumerate(numeric_cols):\n",
        "        row = (idx // n_cols) * 2\n",
        "        col = idx % n_cols\n",
        "        data = df[column].dropna()\n",
        "\n",
        "        # Use os valores reais dos dados para garantir que todos os outliers estejam visíveis\n",
        "        data_min = data.min()\n",
        "        data_max = data.max()\n",
        "\n",
        "        # Se upper_lim/lower_lim forem fornecidos, use-os, senão use os valores reais dos dados\n",
        "        if upper_lim is None or (isinstance(upper_lim, str) and upper_lim.lower() == 'auto'):\n",
        "            x_upper = data_max\n",
        "        else:\n",
        "            try:\n",
        "                x_upper = float(upper_lim)\n",
        "            except (ValueError, TypeError):\n",
        "                x_upper = data_max\n",
        "\n",
        "        if lower_lim is None or (isinstance(lower_lim, str) and lower_lim.lower() == 'auto'):\n",
        "            x_lower = data_min\n",
        "        else:\n",
        "            try:\n",
        "                x_lower = float(lower_lim)\n",
        "            except (ValueError, TypeError):\n",
        "                x_lower = data_min\n",
        "\n",
        "        # Para garantir que todos os pontos (inclusive outliers) sejam mostrados, defina os limites do eixo x\n",
        "        # um pouco além dos valores mínimos e máximos reais dos dados\n",
        "        margin = 0.02 * (data_max - data_min) if data_max > data_min else 1\n",
        "        xlim_lower = data_min - margin\n",
        "        xlim_upper = data_max + margin\n",
        "\n",
        "        # Boxplot\n",
        "        ax_box = axes[row, col]\n",
        "        ax_box.boxplot(data, vert=False, patch_artist=True, widths=0.5, showfliers=True)\n",
        "        ax_box.set_xlim(xlim_lower, xlim_upper)\n",
        "        ax_box.set_yticks([])\n",
        "        ax_box.set_xticklabels([])\n",
        "        ax_box.set_title(f'Boxplot de {column}')\n",
        "\n",
        "        # PDF (histograma + curva normal)\n",
        "        ax_pdf = axes[row + 1, col]\n",
        "        ax_pdf.hist(data, bins=30, color='lightblue', edgecolor='black', alpha=0.7, density=True, range=(xlim_lower, xlim_upper))\n",
        "\n",
        "        if len(data) > 1:\n",
        "            media = data.mean()\n",
        "            std = data.std()\n",
        "            x_grid = np.linspace(xlim_lower, xlim_upper, 200)\n",
        "            y_norm = norm.pdf(x_grid, media, std)\n",
        "            ax_pdf.plot(x_grid, y_norm, color='darkblue', lw=2, label='Normal')\n",
        "\n",
        "            mediana = data.median()\n",
        "            # Linhas estatísticas\n",
        "            ax_box.axvline(media, color='blue', linestyle='-', label=f'Média: {media:.1f}')\n",
        "            ax_box.axvline(mediana, color='green', linestyle='--', label=f'Mediana: {mediana:.1f}')\n",
        "            ax_box.axvline(media + 3*std, color='orange', linestyle=':', label=f'+3σ: {(media + 3*std):.1f}')\n",
        "            ax_box.axvline(media - 3*std, color='orange', linestyle=':', label=f'-3σ: {(media - 3*std):.1f}')\n",
        "\n",
        "            ax_pdf.axvline(media, color='blue', linestyle='-', label=f'Média: {media:.1f}')\n",
        "            ax_pdf.axvline(mediana, color='green', linestyle='--', label=f'Mediana: {mediana:.1f}')\n",
        "            ax_pdf.axvline(media + 3*std, color='orange', linestyle=':', label=f'+3σ: {(media + 3*std):.1f}')\n",
        "            ax_pdf.axvline(media - 3*std, color='orange', linestyle=':', label=f'-3σ: {(media - 3*std):.1f}')\n",
        "\n",
        "        ax_pdf.set_xlim(xlim_lower, xlim_upper)\n",
        "        ax_pdf.set_xlabel(column)\n",
        "        ax_pdf.set_ylabel('Densidade')\n",
        "        ax_pdf.set_title(f'PDF de {column}')\n",
        "        ax_pdf.legend(fontsize=8, loc='upper left')\n",
        "\n",
        "    # Remove subplots vazios\n",
        "    total_plots = n_rows * n_cols\n",
        "    for idx in range(n_vars, total_plots):\n",
        "        for r in [0, 1]:\n",
        "            fig.delaxes(axes[(idx // n_cols) * 2 + r, idx % n_cols])\n",
        "\n",
        "    plt.tight_layout(h_pad=2.5)\n",
        "    plt.show()\n",
        "\n",
        "def plot_boxplot_pdf_indiv(df, column, lower_lim=None, upper_lim=None):\n",
        "    \"\"\"\n",
        "    Plota boxplot horizontal e PDF (histograma + curva normal) para uma coluna numérica do DataFrame.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados.\n",
        "    column : str\n",
        "        Nome da coluna a ser plotada.\n",
        "    lower_lim : float, str ou None, opcional\n",
        "        Limite inferior do eixo x. Se None ou 'auto', usa o mínimo dos dados.\n",
        "    upper_lim : float, str ou None, opcional\n",
        "        Limite superior do eixo x. Se None ou 'auto', usa o máximo dos dados.\n",
        "    \"\"\"\n",
        "\n",
        "    data = df[column].dropna()\n",
        "    data_min = data.min()\n",
        "    data_max = data.max()\n",
        "\n",
        "    # Processamento dos limites\n",
        "    if upper_lim is None or (isinstance(upper_lim, str) and upper_lim.lower() == 'auto'):\n",
        "        x_upper = data_max\n",
        "    else:\n",
        "        try:\n",
        "            x_upper = float(upper_lim)\n",
        "        except (ValueError, TypeError):\n",
        "            x_upper = data_max\n",
        "\n",
        "    if lower_lim is None or (isinstance(lower_lim, str) and lower_lim.lower() == 'auto'):\n",
        "        x_lower = data_min\n",
        "    else:\n",
        "        try:\n",
        "            x_lower = float(lower_lim)\n",
        "        except (ValueError, TypeError):\n",
        "            x_lower = data_min\n",
        "\n",
        "    # Margem para visualização\n",
        "    margin = 0.02 * (data_max - data_min) if data_max > data_min else 1\n",
        "\n",
        "    # Usar os limites definidos pelo usuário quando fornecidos\n",
        "    xlim_lower = x_lower if lower_lim is not None and lower_lim != 'auto' else data_min - margin\n",
        "    xlim_upper = x_upper if upper_lim is not None and upper_lim != 'auto' else data_max + margin\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(8, 6), gridspec_kw={'height_ratios': [1, 4]})\n",
        "\n",
        "    # Boxplot\n",
        "    ax_box = axes[0]\n",
        "    ax_box.boxplot(data, vert=False, patch_artist=True, widths=0.5, showfliers=True)\n",
        "    ax_box.set_xlim(xlim_lower, xlim_upper)\n",
        "    ax_box.set_yticks([])\n",
        "    ax_box.set_xticklabels([])\n",
        "    ax_box.set_title(f'Boxplot de {column}')\n",
        "\n",
        "    # PDF (histograma + curva normal)\n",
        "    ax_pdf = axes[1]\n",
        "\n",
        "    # Ajustar o range do histograma para os limites definidos\n",
        "    hist_range = (xlim_lower, xlim_upper)\n",
        "    ax_pdf.hist(data, bins=30, color='lightblue', edgecolor='black', alpha=0.7, density=True, range=hist_range)\n",
        "\n",
        "    if len(data) > 1:\n",
        "        media = data.mean()\n",
        "        std = data.std()\n",
        "        mediana = data.median()\n",
        "\n",
        "        # Usar os limites definidos para o grid da curva normal\n",
        "        x_grid = np.linspace(xlim_lower, xlim_upper, 200)\n",
        "        y_norm = norm.pdf(x_grid, media, std)\n",
        "        ax_pdf.plot(x_grid, y_norm, color='darkblue', lw=2, label='Normal')\n",
        "\n",
        "        # Linhas estatísticas\n",
        "        ax_box.axvline(media, color='blue', linestyle='-', label=f'Média: {media:.1f}')\n",
        "        ax_box.axvline(mediana, color='green', linestyle='--', label=f'Mediana: {mediana:.1f}')\n",
        "        ax_box.axvline(media + 3*std, color='orange', linestyle=':', label=f'+3σ: {(media + 3*std):.1f}')\n",
        "        ax_box.axvline(media - 3*std, color='orange', linestyle=':', label=f'-3σ: {(media - 3*std):.1f}')\n",
        "\n",
        "        ax_pdf.axvline(media, color='blue', linestyle='-', label=f'Média: {media:.1f}')\n",
        "        ax_pdf.axvline(mediana, color='green', linestyle='--', label=f'Mediana: {mediana:.1f}')\n",
        "        ax_pdf.axvline(media + 3*std, color='orange', linestyle=':', label=f'+3σ: {(media + 3*std):.1f}')\n",
        "        ax_pdf.axvline(media - 3*std, color='orange', linestyle=':', label=f'-3σ: {(media - 3*std):.1f}')\n",
        "\n",
        "    ax_pdf.set_xlim(xlim_lower, xlim_upper)\n",
        "    ax_pdf.set_xlabel(column)\n",
        "    ax_pdf.set_ylabel('Densidade')\n",
        "    ax_pdf.set_title(f'PDF de {column}')\n",
        "    ax_pdf.legend(fontsize=8, loc='upper left')\n",
        "\n",
        "    plt.tight_layout(h_pad=2.5)\n",
        "    plt.show()\n",
        "\n",
        "def teste_n(df, column_name, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Executa o teste de Kolmogorov-Smirnov para verificar se uma coluna do DataFrame segue uma distribuição normal.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados.\n",
        "    column_name : str\n",
        "        Nome da coluna a ser testada quanto à normalidade.\n",
        "    alpha : float, opcional\n",
        "        Nível de significância para o teste. O padrão é 0.05 (5%).\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Normaliza os dados da coluna (subtrai a média e divide pelo desvio padrão).\n",
        "    - Aplica o teste de Kolmogorov-Smirnov comparando com uma distribuição normal padrão.\n",
        "    - Interpreta os resultados com base no p-valor e o nível de significância especificado.\n",
        "    - Exibe uma mensagem informando se a distribuição pode ser considerada normal ou não.\n",
        "\n",
        "    Retorno:\n",
        "    --------\n",
        "    tuple\n",
        "        Uma tupla contendo (estatística do teste, p-valor).\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    stat, p_valor = teste_n(df, 'pv_nafta')\n",
        "    stat, p_valor = teste_n(df, 'pv_nafta', alpha=0.01)  # Usando significância de 1%\n",
        "    \"\"\"\n",
        "    # Executar o teste de Kolmogorov-Smirnov - nesse caso em relação a uma distribuição normal\n",
        "    stat, p_valor = kstest((df[column_name] - np.mean(df[column_name])) / np.std(df[column_name], ddof=1), 'norm')\n",
        "\n",
        "    # Interpretar os resultados\n",
        "    if p_valor > alpha:\n",
        "        print(\"A amostra parece vir de uma distribuição normal (não podemos rejeitar a hipótese nula) p-valor:\", f\"{p_valor:.5f}\")\n",
        "    else:\n",
        "        print(\"A amostra não parece vir de uma distribuição normal (rejeitamos a hipótese nula) p-valor:\", f\"{p_valor:.5f}\")\n",
        "    return float(stat), float(p_valor)\n",
        "\n",
        "def calcula_vif(df,target):\n",
        "    \"\"\"\n",
        "    Calcula o Fator de Inflação da Variância (VIF) para identificar multicolinearidade em variáveis.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo apenas as variáveis independentes para as quais se deseja calcular o VIF.\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Adiciona uma constante ao DataFrame para o cálculo correto do VIF.\n",
        "    - Calcula o VIF para cada variável usando a função variance_inflation_factor.\n",
        "    - Ordena os resultados em ordem decrescente para identificar as variáveis mais problemáticas.\n",
        "    - Exibe os resultados das 15 variáveis com maior VIF.\n",
        "\n",
        "    # Retorno:\n",
        "    # --------\n",
        "    # pandas.DataFrame\n",
        "    #     DataFrame contendo as variáveis e seus respectivos valores VIF.\n",
        "\n",
        "    Interpretação:\n",
        "    --------------\n",
        "    - VIF = 1: Ausência de multicolinearidade\n",
        "    - 1 < VIF < 5: Multicolinearidade moderada\n",
        "    - 5 < VIF < 10: Multicolinearidade alta\n",
        "    - VIF > 10: Multicolinearidade muito alta (problemática)\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    vif_df = calcula_vif(df,target)  \n",
        "    \"\"\"\n",
        "    # Remove a coluna da variável target, se existir\n",
        "    X = df.drop(columns=[target]) if target in df.columns else df.copy()\n",
        "    X_with_const = sm.add_constant(X)  # Adicionando uma constante\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"Variable\"] = X_with_const.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n",
        "\n",
        "    vif.set_index('Variable', inplace=True)\n",
        "    # Imprimir VIF em ordem decrescente\n",
        "    print(\"\\nVIF das variáveis (ordem decrescente):\\n\")\n",
        "    print(vif.query(\"Variable != 'const'\").sort_values(by='VIF', ascending=False).head(15).T)\n",
        "\n",
        "    # return vif\n",
        "\n",
        "def remove_whisker(df, column_name):\n",
        "    \"\"\"\n",
        "    Remove outliers de uma coluna de um DataFrame usando o método IQR (Intervalo Interquartil).\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados.\n",
        "    column_name : str\n",
        "        Nome da coluna de onde os outliers serão removidos.\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Calcula o primeiro quartil (Q1), terceiro quartil (Q3) e IQR da coluna.\n",
        "    - Define os limites inferior e superior como Q1-1.5*IQR e Q3+1.5*IQR.\n",
        "    - Substitui os valores fora desses limites por NaN.\n",
        "    - Remove as linhas com valores NaN na coluna especificada.\n",
        "    - Exibe a quantidade de valores removidos.\n",
        "\n",
        "    Retorno:\n",
        "    --------\n",
        "    tuple\n",
        "        Uma tupla contendo (DataFrame sem outliers, número de valores removidos).\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    df_clean, n_removed = remove_whisker(df, 'pv_nafta')\n",
        "    \"\"\"\n",
        "    # calculando o whisker e removendo os Ys além da fronteira\n",
        "    df_aux = df.copy()\n",
        "    q1 = df_aux[column_name].quantile(.25)\n",
        "    q3 = df_aux[column_name].quantile(.75)\n",
        "    iqr = q3-q1\n",
        "    loval = q1 - (1.5 * iqr)\n",
        "    hival = q3 + (1.5 * iqr)\n",
        "    loW = df_aux[df_aux[column_name] >= loval][column_name].min()\n",
        "    hiW = df_aux[df_aux[column_name] <= hival][column_name].max()\n",
        "    # retirando os outliers\n",
        "    df_aux[column_name] = df_aux[column_name].mask((df_aux[column_name] < loW) | (df_aux[column_name] > hiW))\n",
        "    print('Valores removidos por wishers:', df_aux[column_name].isna().sum())\n",
        "    n_removed = df_aux[column_name].isna().sum()\n",
        "    df_aux.dropna(inplace=True)\n",
        "\n",
        "    return df_aux, n_removed\n",
        "\n",
        "def calcula_corr(df):\n",
        "    \"\"\"\n",
        "    Calcula e visualiza a matriz de correlação absoluta entre as variáveis de um DataFrame.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados para cálculo da correlação.\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Remove valores NaN do DataFrame antes de calcular as correlações.\n",
        "    - Calcula a matriz de correlação absoluta entre todas as variáveis.\n",
        "    - Cria uma visualização interativa usando Plotly Express.\n",
        "    - Aplica uma escala de cores Viridis para representar a intensidade das correlações.\n",
        "    - Mostra os valores numéricos das correlações com duas casas decimais.\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    calcula_corr(df)\n",
        "    calcula_corr(df[selected_columns])  # Para um subconjunto de colunas\n",
        "    \"\"\"\n",
        "    df_corr = df.dropna().corr().abs()\n",
        "\n",
        "    fig = px.imshow(\n",
        "        img    = df_corr,\n",
        "        color_continuous_scale='Viridis',\n",
        "        width  = 900, # caso não esteja visualizando todas as variáveis, altere esse valor\n",
        "        height = 900, # caso não esteja visualizando todas as variáveis, altere esse valor\n",
        "        text_auto = \".2f\"  # Mostra os valores com 2 casas decimais\n",
        "    )\n",
        "\n",
        "    fig.update_traces(textfont_size=12)  # Altere o valor conforme desejado\n",
        "    fig.show()\n",
        "\n",
        "def subplot_serie_hist(df, n_cols=3):\n",
        "    \"\"\"\n",
        "    Plota múltiplas séries temporais em subplots com linhas de referência estatísticas.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados das séries temporais.\n",
        "    n_cols : int, opcional\n",
        "        Número de colunas no layout dos subplots. O padrão é 3.\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Cria uma grade de subplots para visualizar múltiplas séries temporais simultaneamente.\n",
        "    - Para cada variável, adiciona linhas de referência estatísticas (mínimo, máximo, limites do IQR).\n",
        "    - Organiza os gráficos em uma grade de n_cols colunas, calculando automaticamente o número de linhas necessárias.\n",
        "    - Adiciona títulos correspondentes aos nomes das colunas do DataFrame.\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    subplot_serie_hist(df)  # Plota todas as colunas do DataFrame\n",
        "    subplot_serie_hist(df[['pv_nafta', 'f_carg_nafta']], n_cols=2)  # Plota apenas as colunas selecionadas\n",
        "    \"\"\"\n",
        "\n",
        "    n_vars = len(df.columns)\n",
        "    n_rows = math.ceil(n_vars / n_cols)\n",
        "\n",
        "    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=df.columns)\n",
        "\n",
        "    for idx, col in enumerate(df.columns):\n",
        "        row = idx // n_cols + 1\n",
        "        col_idx = idx % n_cols + 1\n",
        "\n",
        "        # Usa a lógica do serie_hist: plota a linha e adiciona linhas de referência (opcional)\n",
        "        trace = px.line(df, y=col).data[0]\n",
        "        fig.add_trace(trace, row=row, col=col_idx)\n",
        "\n",
        "        # Adiciona linhas de referência (mínimo, máximo, IQR) igual ao serie_hist\n",
        "        min_value = df[col].min()\n",
        "        max_value = df[col].max()\n",
        "        q1 = df[col].quantile(0.25)\n",
        "        q3 = df[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        loval = q1 - (1.5 * iqr)\n",
        "        hival = q3 + (1.5 * iqr)\n",
        "\n",
        "        fig.add_shape(type='line', x0=df.index.min(), y0=min_value, x1=df.index.max(), y1=min_value,\n",
        "                    line=dict(color='gray', width=1, dash='dash'), row=row, col=col_idx)\n",
        "        fig.add_shape(type='line', x0=df.index.min(), y0=max_value, x1=df.index.max(), y1=max_value,\n",
        "                    line=dict(color='gray', width=1, dash='dash'), row=row, col=col_idx)\n",
        "        fig.add_shape(type='line', x0=df.index.min(), y0=loval, x1=df.index.max(), y1=loval,\n",
        "                    line=dict(color='orange', width=1, dash='dash'), row=row, col=col_idx)\n",
        "        fig.add_shape(type='line', x0=df.index.min(), y0=hival, x1=df.index.max(), y1=hival,\n",
        "                    line=dict(color='orange', width=1, dash='dash'), row=row, col=col_idx)\n",
        "\n",
        "    # Adicionar uma legenda única para todas as linhas de referência\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[None], y=[None], mode='lines', line=dict(color='gray', width=1, dash='dash'),\n",
        "                  name='Min/Max', showlegend=True),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[None], y=[None], mode='lines', line=dict(color='orange', width=1, dash='dash'),\n",
        "                  name='IQR Limits (±1.5*IQR)', showlegend=True),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        height=250*n_rows,\n",
        "        width=450*n_cols,\n",
        "        showlegend=True,\n",
        "        title_text=\"Séries Temporais das Variáveis\",\n",
        "        legend=dict(\n",
        "            orientation=\"h\",\n",
        "            yanchor=\"bottom\",\n",
        "            y=1.02,\n",
        "            xanchor=\"right\",\n",
        "            x=1\n",
        "        )\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "def pairplot_corr_hm(df, figsize=(12, 12), hist_bins=30, s=10, alpha=0.6):\n",
        "    \"\"\"\n",
        "    Cria um pairplot onde a cor dos pontos é baseada na correlação absoluta entre variáveis\n",
        "    usando uma paleta de cores Viridis.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas DataFrame\n",
        "        O DataFrame contendo os dados a serem plotados\n",
        "    figsize : tuple, opcional\n",
        "        Tamanho da figura (largura, altura) em polegadas\n",
        "    hist_bins : int, opcional\n",
        "        Número de bins para os histogramas na diagonal\n",
        "    s : int, opcional\n",
        "        Tamanho dos pontos nos gráficos de dispersão\n",
        "    alpha : float, opcional\n",
        "        Nível de transparência dos pontos (0-1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Obter a matriz de correlação absoluta\n",
        "    corr_matrix = df.corr().abs()\n",
        "\n",
        "    # Configurar a normalização de cores para a escala viridis\n",
        "    norm = Normalize(vmin=0, vmax=1)\n",
        "\n",
        "    # Obter as variáveis e o número de variáveis\n",
        "    variables = df.columns\n",
        "    n_vars = len(variables)\n",
        "\n",
        "    # Criar a figura e os subplots\n",
        "    fig, axes = plt.subplots(n_vars, n_vars, figsize=figsize)\n",
        "\n",
        "    # Ajustar o espaçamento entre os subplots\n",
        "    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
        "\n",
        "    # Criar os gráficos para cada par de variáveis\n",
        "    for i, var1 in enumerate(variables):\n",
        "        for j, var2 in enumerate(variables):\n",
        "            ax = axes[i, j]\n",
        "\n",
        "            # Remover os ticks dos eixos internos\n",
        "            if i < n_vars - 1:\n",
        "                ax.set_xticks([])\n",
        "            if j > 0:\n",
        "                ax.set_yticks([])\n",
        "\n",
        "            # Se estamos na diagonal, plotar histograma\n",
        "            if i == j:\n",
        "                ax.hist(df[var1], bins=hist_bins, alpha=0.7, color='darkblue')\n",
        "                ax.set_title(var1, fontsize=10)\n",
        "            else:\n",
        "                # Obter a correlação absoluta entre as variáveis\n",
        "                corr_val = corr_matrix.loc[var1, var2]\n",
        "\n",
        "                # Determinar a cor com base na correlação\n",
        "                color = viridis(norm(corr_val))\n",
        "\n",
        "                # Criar o gráfico de dispersão\n",
        "                ax.scatter(df[var2], df[var1], s=s, alpha=alpha, color=color)\n",
        "\n",
        "                # Adicionar a correlação como texto no gráfico\n",
        "                ax.text(0.05, 0.95, f'|ρ|: {corr_val:.2f}',\n",
        "                        transform=ax.transAxes, fontsize=8,\n",
        "                        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
        "\n",
        "    # Adicionar os nomes das variáveis apenas nos eixos externos\n",
        "    for i, var in enumerate(variables):\n",
        "        axes[n_vars-1, i].set_xlabel(var, fontsize=10)\n",
        "        axes[i, 0].set_ylabel(var, fontsize=10)\n",
        "\n",
        "    # Adicionar uma barra de cores para referência\n",
        "    cbar_ax = fig.add_axes([0.92, 0.3, 0.02, 0.4])  # [left, bottom, width, height]\n",
        "    cb = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=\"viridis\"), cax=cbar_ax)\n",
        "    cb.set_label('Correlação Absoluta |ρ|')\n",
        "\n",
        "    plt.suptitle('Pairplot com Cores Baseadas na Correlação Absoluta', fontsize=16)\n",
        "    plt.subplots_adjust(left=0.05, right=0.9, top=0.95, bottom=0.05, wspace=0.2, hspace=0.2)\n",
        "\n",
        "def scatter_plot_corr(df, x_col, y_col, figsize=(10, 6), color='blue', add_regression=False):\n",
        "    \"\"\"\n",
        "    Cria um scatter plot entre duas variáveis do DataFrame e exibe o coeficiente de correlação no gráfico.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame contendo os dados.\n",
        "    x_col : str\n",
        "        Nome da coluna para o eixo x.\n",
        "    y_col : str\n",
        "        Nome da coluna para o eixo y.\n",
        "    figsize : tuple, opcional\n",
        "        Tamanho da figura (largura, altura). O padrão é (10, 6).\n",
        "    color : str, opcional\n",
        "        Cor dos pontos do scatter plot. O padrão é 'blue'.\n",
        "    add_regression : bool, opcional\n",
        "        Se True, adiciona uma linha de regressão ao gráfico. O padrão é False.\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Cria um scatter plot para visualizar a relação entre duas variáveis.\n",
        "    - Calcula e exibe o coeficiente de correlação de Pearson no canto superior esquerdo.\n",
        "    - Opcionalmente, adiciona uma linha de regressão linear para mostrar a tendência.\n",
        "    - Utiliza grade para facilitar a visualização dos dados.\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    scatter_plot_corr(df, 'pv_nafta', 'h_rsup_atm')\n",
        "    scatter_plot_corr(df, 'pv_nafta', 'h_rsup_atm', add_regression=True, color='darkblue')\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.scatter(df[x_col], df[y_col], alpha=0.6, color=color)\n",
        "    plt.xlabel(x_col)\n",
        "    plt.ylabel(y_col)\n",
        "    plt.title(f'Relação entre {x_col} e {y_col}')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Adicionar texto com coeficiente de correlação\n",
        "    corr = df[x_col].corr(df[y_col])\n",
        "    plt.annotate(f'Correlação: {corr:.2f}', xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
        "\n",
        "    # Opcional: adicionar linha de regressão\n",
        "    if add_regression and len(df) > 1:\n",
        "        z = np.polyfit(df[x_col], df[y_col], 1)\n",
        "        p = np.poly1d(z)\n",
        "        plt.plot(df[x_col], p(df[x_col]), \"r--\", alpha=0.7)\n",
        "        plt.annotate(f'y = {z[0]:.2f}x + {z[1]:.2f}', xy=(0.05, 0.85), xycoords='axes fraction',\n",
        "                    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyEV4wQUosGG"
      },
      "source": [
        "Carregamento do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "RCPbhofsE59F",
        "outputId": "8e570c63-05ca-4141-9018-4fcfb0cfa356"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/fdamata_petro/pucrj-analisededados-mvp-ml/raw/refs/heads/main/dataset_pv_nafta_ml.xlsx\"\n",
        "url = 'dataset_pv_nafta_ml.xlsx'\n",
        "\n",
        "\n",
        "df = pd.read_excel(url) # Carregamento do dataset\n",
        "df.head() # Imprime as primeiras linhas do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcHpQ9s_B4n1"
      },
      "source": [
        "# Análise de dados\n",
        "\n",
        "Nesta etapa buscamos entender a distribuição, as relações e as características das variáveis, o que é crucial para as etapas subsequentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kuan1F5EA8J"
      },
      "source": [
        "## Total e tipo dos atributos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3asH5qmbHKVe",
        "outputId": "dc89ca4b-20c0-4fa2-86a4-25563fdd7431"
      },
      "outputs": [],
      "source": [
        "print(f\"Total de instâncias: {len(df)}\")\n",
        "\n",
        "tags = df.columns.to_list()\n",
        "\n",
        "# Definição de que a variável dependente está na primeira coluna\n",
        "target = tags[0]\n",
        "inputs = [tag for tag in tags if tag != target]\n",
        "\n",
        "print(f'Num. de atributos: {len(tags)}')\n",
        "# Separação entre variável dependente (target) e independentes (inputs)\n",
        "print(f'target = {target}')\n",
        "print(f'inputs = {inputs}')\n",
        "# print(f'Num. inputs = {len(inputs)}')\n",
        "\n",
        "print(\"\\nTipos de dados por coluna:\\n\")\n",
        "\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqU2LmS7osGH"
      },
      "source": [
        "O dataset possui 950 observações. Os dezoito (12) atributos são de tipo numérico (float)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDmFNxPdosGH"
      },
      "source": [
        "## Análise de comportamento temporal\n",
        "\n",
        "O objetivo é observar o comportamento das variáveis ao longo do tempo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5Mhv3gRWosGH",
        "outputId": "2ed4fed1-f385-48e7-faca-17c387b6c976"
      },
      "outputs": [],
      "source": [
        "subplot_serie_hist(df, n_cols=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gxh5bzdosGH"
      },
      "source": [
        "É possível observar que em **'t_forn_atm'** e em **'f_refl_nafta'** tiveram períodos em que o comportamento da variável difere do padrão e podem ser úteis para trazer mais variância aos dados, o que pode ser útil lá na porte de modelgaem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-bS4g_ECPoP"
      },
      "source": [
        "## Estatísticas Descritivas\n",
        "\n",
        "Estatísticas descritivas fornecem um resumo das características numéricas, incluindo média, desvio padrão, mínimo, máximo e quartis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTCOuOolC_rG",
        "outputId": "15ae95de-eb40-4666-b92c-06b22758a9ff"
      },
      "outputs": [],
      "source": [
        "# estasticas descritivas básicas do dataset\n",
        "df_descr = df.describe()\n",
        "print('Estatísticas descritivas:\\n')\n",
        "print(df_descr.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWPH7rj8Ck6v"
      },
      "source": [
        "### Média, Mediana, Desvio padrão, MAD e Coef. de Variação Robusto\n",
        "\n",
        "A média e mediana são medidas de tendência central dos dados. A média representa o valor do ponto de equilíbrio dos dados. Já a mediana representa o valor central de um conjunto de dados ordenados.<br>\n",
        "\n",
        "Diferente da média, a mediana não é influenciada por valores extremos (outliers), sendo especialmente útil também para descrever distribuições assimétricas ou com presença de valores atípicos.\n",
        "\n",
        "O desvio padrão e MAD (Median Absolute Deviation ou Desvio Absoluto da Mediana) são medidas de dispersão de um conjunto de valores. <br>\n",
        "Por ser baseado na mediana, o MAD é menos sensível a outliers do que o desvio padrão, fornecendo uma visão mais fiel da variabilidade dos dados em distribuições não normais ou contaminadas por valores extremos.\n",
        "\n",
        "O Coeficiente de Variação Robusto é uma métrica relativa de dispersão que utiliza o MAD em vez do desvio padrão e a mediana no lugar da média. Ele é calculado como:\n",
        "\n",
        "$$ CV_{robusto} = \\frac{MAD}{|mediana|} \\times 100 $$\n",
        "\n",
        "Esse coeficiente permite comparar a variabilidade relativa entre diferentes variáveis, mesmo quando apresentam escalas ou unidades distintas, sendo mais adequado para dados com outliers e até mesmo para distribuições assimétricas. Valores mais altos indicam maior dispersão relativa em relação à mediana."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm3QIAb-CZbt",
        "outputId": "6b2950f7-3e17-4d6b-ef62-253699313dbd"
      },
      "outputs": [],
      "source": [
        "# média dos atributos numéricos do dataset\n",
        "df_descr = df.describe()\n",
        "df_descr = df_descr.T\n",
        "df_descr.rename(columns={'50%': 'median'}, inplace=True)\n",
        "\n",
        "# Calculate Mean Absolute Deviation (MAD)\n",
        "mad = df.apply(lambda x: np.mean(np.abs(x - x.mean())))\n",
        "df_descr['mad'] = mad\n",
        "\n",
        "df_descr['cv_robust'] = (df_descr['mad'] / df_descr['median'].abs()) *100\n",
        "\n",
        "print(df_descr[['mean', 'median', 'std', 'mad', 'cv_robust']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJMNUGWyosGI"
      },
      "source": [
        "No gráfico a seguir é possível observar os valores relativos do CV Robusto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "vTT8tgewosGI",
        "outputId": "8a91b062-5c8b-4dae-f904-ab30913fd1c3"
      },
      "outputs": [],
      "source": [
        "# Configurar estilo\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Criar dataframe ordenado para melhor visualização\n",
        "df_descr.sort_values('cv_robust', ascending=False, inplace=True)\n",
        "\n",
        "# Criar gráfico de barras para o CV Robusto\n",
        "plt.figure(figsize=(12, 7))\n",
        "ax = sns.barplot(x=df_descr.index, y='cv_robust', data=df_descr, hue=df_descr.index, legend=False)\n",
        "\n",
        "# Adicionar rótulos e título\n",
        "plt.title('Coeficiente de Variação Robusto por Variável', fontsize=14)\n",
        "plt.xlabel('Variáveis', fontsize=12)\n",
        "plt.ylabel('CV Robusto (%)', fontsize=12)\n",
        "\n",
        "# Adicionar os valores nas barras\n",
        "for i, v in enumerate(df_descr['cv_robust']):\n",
        "    if not pd.isna(v):\n",
        "        ax.text(i, v + 0.5, f'{v:.1f}%', ha='center', fontsize=9)\n",
        "\n",
        "# Adicionar linhas de referência para as classificações\n",
        "plt.axhline(y=40, color='red', linestyle='--', alpha=0.7, label='Alta dispersão > 40%')\n",
        "plt.axhline(y=30, color='orange', linestyle='--', alpha=0.7, label='Média dispersão > 30%')\n",
        "plt.axhline(y=20, color='blue', linestyle='--', alpha=0.7, label='Baixa dispersão > 20%')\n",
        "plt.axhline(y=10, color='green', linestyle='--', alpha=0.7, label='Muito baixa dispersão < 10%')\n",
        "\n",
        "# Rotacionar os rótulos do eixo x para melhor visualização\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Adicionar legenda\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "# Ajustar layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jiac-x7OosGI"
      },
      "source": [
        "Todas as variáveis, com exceção da **'f_refl_nafta'**, tem baixa ou muito baixa dispersão segundo o critério de Coeficiente de Variação Robusto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tX_H2zZEFwU"
      },
      "source": [
        "## Histograma\n",
        "\n",
        "O Histrograma permite-nos visualizar a dispersão e a frequência dos dados. <br>\n",
        "Com isso podemos descobrir padrões, tendências centrais, dispersão e a presença de valores atípicos (outliers). <br>\n",
        "No histograma ainda pode-se ter a identificação de simetria ou assimetria, se são unimodais ou multimodais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSs4jBF7HUnV"
      },
      "source": [
        "*pv_nafta*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "id": "MdBuOXxzHI5h",
        "outputId": "d6c5a966-23fd-4f8d-89e4-7622a38a32f0"
      },
      "outputs": [],
      "source": [
        "histo(df, 'f_refl_nafta')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btChgWlqosGL"
      },
      "source": [
        "Ao observarmos a primeira distribuição, relacionada à **'f_refl_nafta'** percebemos se tratar de uma distribuição assimétrica à direita (ou positivamente assimétrica), com uma cauda da distribuição se estendendo mais para a direita, indicando a presença de valores mais altos menos frequentes. Em relação ao modo, podemos classificar como unimodal, pois possui apenas um pico principal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av_qTo2YERdw"
      },
      "source": [
        "## Boxplot\n",
        "\n",
        "Outra forma de entender distribuiçãp dos dados é através do boxplot.<br>\n",
        "Nele é possível comparar a mediana, quartis, wiskers e outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iH6ov-UMHHHJ",
        "outputId": "d6a7cb20-5d58-4ead-efcf-3873d167b9f6"
      },
      "outputs": [],
      "source": [
        "teste_n(df, target)\n",
        "box_p(df, target)\n",
        "histo(df, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqwFIROrHHHJ"
      },
      "source": [
        "No boxplot da pressão de vapor da nafta **'pv_nafta'** é possível ver a simetria da sua distribuição, que também é comprovada pelo histrograma. E no teste de normalidade é possível afirmar que a distribuição parece de fato vir de uma distribuição normal ao nível de significância de 0,05."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgMO8M8josGM"
      },
      "source": [
        "Consolidado essas ferramentas, conseguimos visualmente observar as variáveis envolvidas no dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AdLn_LnxosGM",
        "outputId": "fbb3eb18-617e-40b9-fab6-38d29843e311"
      },
      "outputs": [],
      "source": [
        "plot_boxplot_pdf(df, n_cols=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmMjjsADosGM"
      },
      "source": [
        "É possível observar que nas unidades de engenharia utilizadas não há um único padrão para todas as distribuições.<br>\n",
        "Há algumas distribuições que aparentam a normalidade, mas que requerem avaliação específica para avaliação das hipóteses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldDSqg7WosGM",
        "outputId": "7c388b1b-37fa-4a72-ac81-ea6318dbf995"
      },
      "outputs": [],
      "source": [
        "# Loop para aplicar a função teste_n em todas as colunas do DataFrame\n",
        "print(\"Teste de normalidade para todas as variáveis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "normality_results = {}\n",
        "for col in df.columns:\n",
        "    print(f\"Variável: '{col}'\")\n",
        "    _, p_valor = teste_n(df, col)\n",
        "    normality_results[col] = \"Normal\" if p_valor > 0.05 else \"Não normal\"\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Criar um DataFrame com os resultados para melhor visualização\n",
        "normality_df = pd.DataFrame.from_dict(normality_results, orient='index', columns=['Distribuição'])\n",
        "print(\"\\nResumo dos testes de normalidade:\")\n",
        "print(normality_df)\n",
        "\n",
        "# Calcular a proporção de variáveis com distribuição normal\n",
        "normal_count = sum(1 for status in normality_results.values() if status == \"Normal\")\n",
        "print(f\"\\nProporção de variáveis com distribuição normal: {normal_count}/{len(df.columns)} ({normal_count/len(df.columns)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWUPBcXrosGM"
      },
      "source": [
        "Nesse dataset 50% dos atributos apresentam distribuição normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_ERHJAPEZAt"
      },
      "source": [
        "## Matriz de Correlação\n",
        "\n",
        "A matriz de correlação mede a força e a direção de uma relação linear entre os atributos do dataset.<br>\n",
        "Valores próximos a 1 indicam uma forte correlação positiva, -1 uma forte correlação negativa, e 0 ausência de correlação linear.\n",
        "\n",
        "### Classificação da relevância de coeficientes de correlação\n",
        "\n",
        "Uma referência para os valores do coeficiente de correlação facilita a sua interpretação.<br>\n",
        "Embora não exista um consenso absoluto, existem algumas classificações que podem ser adaptadas ao contexto de análise de dados de processo.<br>\n",
        "No nosso caso vamos adotar a classificação de Cohen\n",
        "> (Cohen, J. (1988). *Statistical Power Analysis for the Behavioral Sciences* (2nd ed.). Lawrence Erlbaum Associates.)\n",
        "\n",
        "**Classificação de Cohen**\n",
        "\n",
        "- **\\|r\\| < 0.10**: Correlação negligenciável\n",
        "- **0.10 ≤ \\|r\\| < 0.30**: Correlação fraca\n",
        "- **0.30 ≤ \\|r\\| < 0.50**: Correlação moderada\n",
        "- **0.50 ≤ \\|r\\| < 0.70**: Correlação forte\n",
        "- **\\|r\\| ≥ 0.70**: Correlação muito forte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAmPsbzDHF2t",
        "outputId": "3c94cb54-02e7-4712-ed6f-1138eac0ffce"
      },
      "outputs": [],
      "source": [
        "# Matriz de correlação\n",
        "print(\"\\nMatriz de Correlação:\\n\")\n",
        "print(df.corr())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1tzwXM2osGM"
      },
      "source": [
        "Para simplificar a visualização de correlação linear, o mapa de calor será realizado com o valor absoluto da correlação [0,1] ao invés de [-1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "1iSnofo-HF2u",
        "outputId": "b9bc69b3-56a5-4a69-f769-0c7ad3390cd7"
      },
      "outputs": [],
      "source": [
        "calcula_corr(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz1caOiQHF2u"
      },
      "source": [
        "No mapa de calor da matriz de correlação temos a representação visual para as correlação dos atributos. É possível observar que entre as variáveis independentes há variáveis com correlação muito forte, indicando possível redundância de informação, como por exemplo as temperaturas de Topo **'t_topo_nafta'**, do Estágio Superior **'t_esup_nafta'**, do Estágio Intermediario **'t_esup_nafta'** e da Linha de topo **'t_lhtp_nafta'** da fracionadora de nafta com correlações maiores que 0,85.\n",
        "\n",
        "Para permitir uma avaliação visual mais intuitiva, vamos construir um pairplot conjugado com uma mapa de calor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0H-dVGYxosGM",
        "outputId": "58857034-1d6e-45da-98bb-6f440d92dba0"
      },
      "outputs": [],
      "source": [
        "pairplot_corr_hm(df, figsize=(24, 24), hist_bins=30, s=10, alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl5B6vtdosGM"
      },
      "source": [
        "Uma visualização diferente, mas que trás as mesma informações do heatmap da matriz de correlação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBm9kvFMosGM"
      },
      "source": [
        "## Análise de VIF\n",
        "\n",
        "Análise de VIF (Variance Inflation Factor ou Fator de Inflação da Variância) é uma medida para quantificar o grau de multicolinearidade entre as variáveis independentes do dataset. A multicolinearidade ocorre quando duas ou mais variáveis independentes estão altamente correlacionadas, o que pode dificultar a estimativa dos coeficientes da regressão e afetar a interpretação dos resultados, principalmente para modelos lineares.\n",
        "\n",
        "O VIF é calculado para cada variável independente e é definido como:\n",
        "> Montgomery, D. C., Peck, E. A., & Vining, G. G. (2012). *Introduction to Linear Regression Analysis* (5th ed.). Wiley.\n",
        "\n",
        "$$VIF_i = \\frac{1}{1 - R^2_i}$$\n",
        "\n",
        "onde $R^2_i$ é o coeficiente de determinação da regressão da variável $i$ em relação a todas as outras variáveis independentes.\n",
        "\n",
        "Interpretação do VIF:\n",
        "- VIF = 1: não há correlação entre a variável $i$ e as outras variáveis. A variância não está inflacionada.\n",
        "- 1 < VIF < 5: a correlação é moderada, mas geralmente não é uma preocupação.\n",
        "- VIF ≥ 5: indica uma alta correlação e pode ser motivo de preocupação; a variável pode estar contribuindo para a multicolinearidade.\n",
        "- VIF ≥ 10: geralmente considerado um sinal forte de multicolinearidade, e pode ser necessário considerar a remoção da variável ou a aplicação de técnicas de regularização."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7vsxPaZosGM",
        "outputId": "04bfebed-9abb-4bc5-cbaf-d95e9abe971a"
      },
      "outputs": [],
      "source": [
        "calcula_vif(df,target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wag-Aq8NosGN"
      },
      "source": [
        "Processo iterativo de remoção de variáveis independentes até a obtenção de maior VIF entre 5 e 10.<br>\n",
        "Nesse momento é importante analisar o VIF com o conhecimento de domínio e não apenas exclusivamente pelo valor numérico da estatística.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fLrBcG1osGN"
      },
      "source": [
        "Pelo conhecimento de domínio as variáveis independentes **'t_topo_nafta'**, **'t_esup_nafta'**, **'t_eint_nafta'** e **'t_lhtp_nafta'** apresentam interdependência e carregam a mesma informação, podendo haver a redução de dimensionalidade do dataset pela supressão de alguma delas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "VMq4B3nJosGN",
        "outputId": "561f3ce4-de8c-4fff-fe74-cd0096e0114d"
      },
      "outputs": [],
      "source": [
        "# Veja o comportamento da série histórico como isso se dá.\n",
        "\n",
        "serie_hist(df, ['t_lhtp_nafta','t_topo_nafta','t_eint_nafta','t_esup_nafta'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cNG3ZuDcosGN",
        "outputId": "8d426c25-9cbc-4884-be05-650e78b921e7"
      },
      "outputs": [],
      "source": [
        "# Comportamento das distribuições.\n",
        "\n",
        "box_p(df, 't_lhtp_nafta')\n",
        "box_p(df, 't_topo_nafta')\n",
        "box_p(df, 't_eint_nafta')\n",
        "box_p(df, 't_esup_nafta')\n",
        "print(\"\\nDesvio Padrão das distribuições:\")\n",
        "print(df[['t_lhtp_nafta','t_topo_nafta', 't_eint_nafta', 't_esup_nafta']].describe().loc['std'])\n",
        "\n",
        "# Teste de normalidade para colunas específicas\n",
        "specific_columns = ['t_lhtp_nafta', 't_topo_nafta', 't_eint_nafta', 't_esup_nafta']\n",
        "print(\"\\nTeste de normalidade para colunas específicas:\")\n",
        "print(normality_df.loc[specific_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2xOwsEafosGN",
        "outputId": "1c7b1adc-17c1-43c2-de8e-0a80904f1b41"
      },
      "outputs": [],
      "source": [
        "# Correla~ções com a variável independente\n",
        "\n",
        "scatter_plot_corr(df, 't_lhtp_nafta', 'pv_nafta', add_regression=True)\n",
        "scatter_plot_corr(df, 't_topo_nafta', 'pv_nafta', add_regression=True)\n",
        "scatter_plot_corr(df, 't_eint_nafta', 'pv_nafta', add_regression=True)\n",
        "scatter_plot_corr(df, 't_esup_nafta', 'pv_nafta', add_regression=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP1yJv0NosGN"
      },
      "source": [
        "Visualmente não é possível identificar um padrão que favorecesse uma ou outra variável independente, até porque os coeficientes de correlação linear com a variável dependente são muito baixos.<br>\n",
        "É possível observar que dentre as 4 variáveis a **'t_lhtp_nafta'** é a variável independente que menos apresenta valores extremos e ainda parece ter uma distribuição normal segundo o teste estatístico ao nível de significância de 0,05.<br>\n",
        "Logo vamos preservá-la e eliminar as outras três (**'t_topo_nafta'**, **'t_eint_nafta'** e **'t_esup_nafta'**).<br>\n",
        "Notar ainda que o desvio padrão entre elas é semelhante, ou seja, contribuiriam com variância semelhante para um futuro modelo de predição e com a vantagem de não eliminar uma grande massa de dados pelos pontos extremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2MQBFBcJosGN",
        "outputId": "bcb9c0bb-c562-4ef3-e24f-e43935d54384"
      },
      "outputs": [],
      "source": [
        "df1 = df.drop(columns=['t_topo_nafta', 't_eint_nafta', 't_esup_nafta'])\n",
        "calcula_vif(df1,target)\n",
        "calcula_corr(df1)\n",
        "pairplot_corr_hm(df1, figsize=(24, 24), hist_bins=30, s=10, alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnRDIsr7osGN"
      },
      "source": [
        "Seguindo com a avaliação de VIF e de correlação, foi possível observar que essa significativa correlação entre **'t_fund_nafta'**, **'t_aque_nafta'**, **'t_carga_nafta'** e **'t_einf_nafta'** também estão carregando a inflação de variância e podemos eliminar variáveis sem prejuízo às informações necessárias, também com base no conhecimento de domínio, já que tem correlação dentro do processo de fracionamento da nafta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "RxuKdAPnosGN",
        "outputId": "9d403280-f80e-4665-c462-80c5eaff89ac"
      },
      "outputs": [],
      "source": [
        "serie_hist(df1, ['t_fund_nafta','t_aque_nafta','t_carg_nafta','t_einf_nafta'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y7GOJiuhosGN",
        "outputId": "44b4f03d-b9ac-49f3-ab44-89fa3a05ad94"
      },
      "outputs": [],
      "source": [
        "# Comportamento das distribuições.\n",
        "\n",
        "box_p(df1, 't_fund_nafta')\n",
        "box_p(df1, 't_aque_nafta')\n",
        "box_p(df1, 't_carg_nafta')\n",
        "box_p(df1, 't_einf_nafta')\n",
        "print(\"\\nDesvio Padrão das distribuições:\")\n",
        "print(df1[['t_fund_nafta','t_aque_nafta','t_carg_nafta','t_einf_nafta']].describe().loc['std'])\n",
        "\n",
        "# Teste de normalidade para colunas específicas\n",
        "specific_columns = ['t_fund_nafta','t_aque_nafta','t_carg_nafta','t_einf_nafta']\n",
        "print(\"\\nTeste de normalidade para colunas específicas:\")\n",
        "print(normality_df.loc[specific_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QEGN5ozNosGN",
        "outputId": "70fbff07-5470-457b-c6fe-8a95b594186b"
      },
      "outputs": [],
      "source": [
        "# Correla~ções com a variável independente\n",
        "\n",
        "scatter_plot_corr(df1, 't_fund_nafta', 'pv_nafta', add_regression=True)\n",
        "scatter_plot_corr(df1, 't_aque_nafta', 'pv_nafta', add_regression=True)\n",
        "scatter_plot_corr(df1, 't_carg_nafta', 'pv_nafta', add_regression=True)\n",
        "scatter_plot_corr(df1, 't_einf_nafta', 'pv_nafta', add_regression=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZcnKF04osGN"
      },
      "source": [
        "Vamos preservar a **'t_einf_nafta'**, mesmo não apresentando uma distribuição que pudesse ser considerada normal ao nível de significância de 0,05. Ainda apresenta a maior correlação com a variável dependente e numa avaliação visual do padrão de comportamento de correlação também trás esse insight. Dentre as candidatas, também apresenta menor número de valores extremos. Logo vamos eliminar **'t_aque_carga'**, **'t_carg_nafta'** e **'t_fund_nafta'** de forma a tratar a redução da VIF, levando em consideração o conhecimento do processo e ainda reduzir a dimensionalidade das variáveis independentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0I2e6g7CosGN",
        "outputId": "dadb72bc-7144-4f7f-d592-586d1507b5ea"
      },
      "outputs": [],
      "source": [
        "df2 = df1.drop(columns=['t_aque_nafta','t_carg_nafta','t_fund_nafta'])\n",
        "calcula_vif(df2,target)\n",
        "calcula_corr(df2)\n",
        "pairplot_corr_hm(df2, figsize=(24, 24), hist_bins=30, s=10, alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcX1I7gtosGN"
      },
      "source": [
        "Como a maior VIF está abaixo de 5, optou-se por não remover mais variáveis independentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "id": "9o07n7nDosGO",
        "outputId": "5c194102-f7ea-4039-ef36-7e98565702b0"
      },
      "outputs": [],
      "source": [
        "# Resumo das distribuições dos atributos remanescentes\n",
        "\n",
        "plot_boxplot_pdf(df2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1cKaMw7osGO"
      },
      "source": [
        "## Tratamento de pontos extremos (outliers)\n",
        "\n",
        "Os outliers são observações que desviam significativamente do padrão geral dos dados.<br>\n",
        "Em processos industriais, como o fracionamento de nafta, eles não são necessariamente \"dados ruins\".<br>\n",
        "Podem ser sinais que revelam informações valiosas sobre o comportamento do sistema ou indicar problemas nos dados.\n",
        "\n",
        "## Classificação de Outliers por Natureza\n",
        "\n",
        "1. Erros genuínos: são outliers que não representam o fenômeno real e distorcem a análise:\n",
        "\n",
        "<div style=\"margin-left: 30px\">\n",
        "Exemplos:<br>\n",
        "\n",
        "- Erros de instrumentação: falha em sensores que possam gerar leituras anormais.\n",
        "- Erros de registro: falha em sistema de aquisição/armazenamento de dados.\n",
        "- Erros de transcrição: falha em entrada manual de dados ou ajustes de parâmetros.\n",
        "- Erros de unidade: inconsistência nas unidades de medida (por exemplo, leitura em psi quando deveria ser em kPa).\n",
        "</div>\n",
        "\n",
        "2. Eventos raros legítimos: representam fenômenos reais do processo, embora incomuns:\n",
        "\n",
        "<div style=\"margin-left: 30px\">\n",
        "Exemplos:<br>\n",
        "\n",
        "- Eventos transitórios: períodos de partida ou parada da unidade, mudanças de carga ou condições operacionais (transientes).\n",
        "- Perturbações externas: impactos por variações na qualidade do petróleo (características diferentes).\n",
        "- Eventos operacionais planejados: testes de desempenho, mecessidade de manutenção ou otimização do processo.\n",
        "- Eventos operacionais não-planejados: distúrbios no processo, intervenções emergenciais ou falhas em equipamentos.\n",
        "</div>\n",
        "\n",
        "3. Outliers dependentes de contexto: valores que são outliers apenas em contextos específicos:\n",
        "\n",
        "<div style=\"margin-left: 30px\">\n",
        "Exemplos:<br>\n",
        "\n",
        "- Outliers sazonais: mudanças climáticas (temperatura ambiente afetando sistemas de troca térmica).\n",
        "- Outliers condicionais: valores que são normais sob certas condições operacionais, mas anormais sob outras.\n",
        "- Outliers relacionais: violações conhecidas entre variáveis de processo (como balanços materiais ou energéticos).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KlIwoFN1osGO",
        "outputId": "d4d0583f-8cac-4c9e-828b-0be93009ed9a"
      },
      "outputs": [],
      "source": [
        "subplot_serie_hist(df2, n_cols=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm9Q0xh8osGO",
        "outputId": "0d7ddd7e-da70-4eee-f96c-18dcf0df80ee"
      },
      "outputs": [],
      "source": [
        "print(\"Variáveis no dataset: \",len(df2.columns))\n",
        "\n",
        "# Teste de normalidade para colunas específicas\n",
        "specific_columns = df2.columns.tolist()\n",
        "print(\"\\nTeste de normalidade para colunas específicas:\")\n",
        "print(normality_df.loc[specific_columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FTzVWrNosGO"
      },
      "source": [
        "Vamos avaliar os possíveis outliers individualmente por variável baseado no conhecimento de domínio. O método utilizado é do IQR.\n",
        "\n",
        "**'pv_nafta'**: Normal:\n",
        "- outliers no limite superior, apesar de raros podem ocorrer novamente. A sua supressão poderia restringir capacidade de predição futura. No entanto, o outlier no limite inferior, abaixo inclusive dos 3 sigmas, parece indicar uma falha na amostragem do produto, pois ela é realizada em gelo para preservação das espécies mais voláteis e esse valor tão baixo é um forte indício na falha da amostragem, logo, por ser apenas um ponto, vamos suprimir valores abaixo de 50 unidades de medida da variável.\n",
        "\n",
        "**'t_lhtp_nafta'**: Normal:\n",
        "- das variações observadas, os pontos superiores são de baixa representatividade do processo indicando uma falha pontual no sistema de resfriamento de topo. Logo vamos suprimir valores superiores a 205 unidades.\n",
        "\n",
        "**'p_topo_nafta'**: Não normal:\n",
        "- a variável apresenta uma distribuição unimodal relativamente simétrica (média e mediana bem coincidente). Ao observar a série temporal observa-se que tende a representação de operação sazonal. Vamos preservar os dados e eventualmente tratar com uma transformação mais adiante.\n",
        "\n",
        "**'t_einf_nafta'**: Não normal:\n",
        "- das variações observadas, há um ponto na região inferior bem desgarrado da distribuição indicando uma falha de medição. Diferentemente dos valores no limite superior, que são prováveis de ocorrer novamente. Logo vamos suprimir valores inferiores a 220 unidades.\n",
        "\n",
        "**'f_carg_nafta'**: Normal:\n",
        "- a variável apresenta uma distribuição normal, com um ponto caracterizado com outlier pelo critério IQR, mas vamos preservá-lo em função de estar dentro de um contexto de reprodução provável.\n",
        "\n",
        "**'f_refl_nafta'**: Não normal:\n",
        "- a variável apresenta uma distribuição assimétrica unimodal, muito em função de um comportamento sazonal, quando em um período específico teve um operação com valores acima do padrão, mas dentro de condições de processo que podem se repetir. Mais adiante avaliaremos eventual transformação dos dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iXL_KdhMosGO",
        "outputId": "9bb555d5-4feb-489e-bfc8-db7e8e433e3d"
      },
      "outputs": [],
      "source": [
        "# Aplicar filtro para remover valores da variável pv_nafta inferiores a 50\n",
        "df3 = df2[df2['pv_nafta'] >= 50]\n",
        "print(f'Valores removidos: {len(df2) - len(df3)}')\n",
        "\n",
        "box_p(df2, 'pv_nafta', lower_lim=40, upper_lim=160)\n",
        "box_p(df3, 'pv_nafta', lower_lim=40, upper_lim=160)\n",
        "plot_boxplot_pdf_indiv(df3, 'pv_nafta', lower_lim=40, upper_lim=160)\n",
        "serie_hist(df3, 'pv_nafta')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyQcpxx6osGO"
      },
      "source": [
        "A distribuição permanece aparentemente normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tY7X5sheosGO",
        "outputId": "4b2d8256-c92f-412c-b8b8-d685dbc742ab"
      },
      "outputs": [],
      "source": [
        "# Aplicar filtro para remover valores da variável t_lhtp_nafta superiores a 205\n",
        "df4 = df3[df3['t_lhtp_nafta'] <= 205]\n",
        "print(f'Valores removidos: {len(df3) - len(df4)}')\n",
        "\n",
        "box_p(df3, 't_lhtp_nafta', lower_lim=160, upper_lim=210)\n",
        "box_p(df4, 't_lhtp_nafta', lower_lim=160, upper_lim=210)\n",
        "plot_boxplot_pdf_indiv(df4, 't_lhtp_nafta', lower_lim=160, upper_lim=210)\n",
        "serie_hist(df4, 't_lhtp_nafta')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf1Uqak0osGO"
      },
      "source": [
        "É possível observar que a distribuição tente a uma distribuição normal. A mediana está bem próxima da média."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "diLq9W2NosGO",
        "outputId": "601ab3c9-ffda-4dc1-8850-f18787b7c35e"
      },
      "outputs": [],
      "source": [
        "# Aplicar filtro para remover valores da variável t_einf_nafta inferiores a 110\n",
        "df5 = df4[df4['t_einf_nafta'] >= 220]\n",
        "print(f'Valores removidos: {len(df4) - len(df5)}')\n",
        "\n",
        "box_p(df4, 't_einf_nafta', lower_lim=210, upper_lim=270)\n",
        "box_p(df5, 't_einf_nafta', lower_lim=210, upper_lim=270)\n",
        "plot_boxplot_pdf_indiv(df5, 't_einf_nafta', lower_lim=210, upper_lim=270)\n",
        "serie_hist(df5, 't_einf_nafta')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xbsL0B1osGO"
      },
      "source": [
        "A distribuição que era considerada não normal e parece apresentar o mesmo comportamento em função de uma assimetria em relação à distribuição normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgWyM83fosGP"
      },
      "source": [
        "Era esperada a remoção de dois pontos, mas note que em remoções de outliers anteriores esses pontos foram excluidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 942
        },
        "id": "7ut1gU00osGP",
        "outputId": "7e597d2a-c29e-4fcd-f278-97cae48a9036"
      },
      "outputs": [],
      "source": [
        "plot_boxplot_pdf(df5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xBHcBGA1osGP",
        "outputId": "a66d82be-f43d-499a-b529-064e3d6ea206"
      },
      "outputs": [],
      "source": [
        "subplot_serie_hist(df5, n_cols=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYhylY2dosGP",
        "outputId": "5e39286b-4fb1-4507-c5dc-750d52b669e9"
      },
      "outputs": [],
      "source": [
        "print(f'Foram removidos {df.shape[0]-df5.shape[0]} pontos extremos.')\n",
        "print(f'Após o processamento inicial dos dados, o dataset está com {df5.shape[1]} atributos e {df5.shape[0]} observações')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVpkCrkaosGP"
      },
      "source": [
        "Após o tratamento de dados iniciais, chegamos à etapa de pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDovmxgDFcbF"
      },
      "source": [
        "# Pré-Processamento de Dados\n",
        "\n",
        "O pré-processamento de dados é uma etapa crucial para preparar os dados para modelagem, garantindo que estejam no formato correto e otimizados para o desempenho do algoritmo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wtm0b2yosGP"
      },
      "source": [
        "## Transformações"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mledpAbhosGP"
      },
      "source": [
        "### Criação de novas características\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkgGsbL0osGP"
      },
      "source": [
        "Razão de refluxo: uma variável derivada, definida como a relação entre a vazão de refluxo de topo **'f_refl_nafta'** e a vazão de carga da fracionadora **'fcarg_nafta'** (**'f_refl_nafta'**/**'f_carg_nafta'**). Esta variável representa um parâmetros operacionais que pode ser críticos em fracionadoras, pois determina a eficiência da separação dos componentes.\n",
        "\n",
        "Eficiência da separação: valores mais altos de razão de refluxo proporcionam maior contato entre as fases líquida e vapor em cada estágio do fracionamento, resultando em melhor separação dos componentes com diferentes volatilidades.\n",
        "\n",
        "Controle da composição do produto: ao aumentar a razão de refluxo, mais componentes leves são retidos e retornam à fracionadora em vez de saírem no produto de topo, esperando-se uma elevação da pressão de vapor da nafta que sai pelo fundo da fracionadora.\n",
        "\n",
        "Estabilidade operacional: uma razão de refluxo adequada ajuda a manter condições operacionais estáveis, reduzindo flutuações na qualidade do produto.\n",
        "\n",
        "Perfil térmico da coluna: influencia diretamente o gradiente de temperatura ao longo da fracionadora, afetando a distribuição dos componentes em cada estágio.\n",
        "\n",
        "Implicações: na prática, existe um trade-off importante no ajuste da razão de refluxo:\n",
        "\n",
        "- Razão de refluxo baixa: menor consumo energético e maior capacidade de processamento, porém com qualidade de separação potencialmente comprometida.\n",
        "\n",
        "- Razão de refluxo alta: melhor separação e controle mais preciso da pressão de vapor do produto, porém com maior consumo energético e menor capacidade de processamento.\n",
        "\n",
        "Para a predição da pressão de vapor da nafta, a razão de refluxo representa uma variável derivada de alto valor preditivo, pois sintetiza em um único parâmetro a interação entre duas variáveis operacionais críticas que afetam diretamente o equilíbrio termodinâmico e a composição do produto final.\n",
        "\n",
        "A inclusão desta variável no modelo preditivo provavelmente aumentará seu poder explicativo, capturando um mecanismo de controle operacional que impacta a propriedade alvo que estamos tentando prever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "GmzoDAuZosGP",
        "outputId": "be648181-6b83-4478-8200-49cb575f23df"
      },
      "outputs": [],
      "source": [
        "df5['r_refl_nafta'] = df5['f_refl_nafta'] / df5['f_carg_nafta']\n",
        "plot_boxplot_pdf_indiv(df5, 'f_refl_nafta')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWcky1H4osGP"
      },
      "source": [
        "O novo atributo criado apresenta distribuição assimétrica à direita, com uma cauda da distribuição se estendendo nesse sentido, indicando a presença de valores mais altos menos frequentes. Esse comportamento é derivado da variável **'f_refl_nafta'** que deu origem a essa nova criação, conforme já apresentado anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkYHU7CQosGP"
      },
      "source": [
        "Vamos avaliar como ficaram as correlações e a VIF após essa nova vaiável."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fa1lgM08osGP",
        "outputId": "dc64cf59-762c-4fd3-e5a1-21e5fc44c7ea"
      },
      "outputs": [],
      "source": [
        "calcula_vif(df5,target)\n",
        "calcula_corr(df5)\n",
        "pairplot_corr_hm(df5, figsize=(24, 24), hist_bins=30, s=10, alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR21vAuqosGP"
      },
      "source": [
        "É possível observar uma elevada correlação, conforme esperado, entre **'f_refl_nafta'** e a nova **'r_refl_nafta'**. Em função da relevância dessa nova variável, conforme explicado anteriormente e até mesmo da discreta maior correlação com a variável alvo, vamos suprimir a **'f_refl_nafta'** para reduzir a VIF (reduzir a mulcolinearidade)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnn4yn7qosGP",
        "outputId": "5d501603-9cb3-40ff-f7d3-5977550a7d0b"
      },
      "outputs": [],
      "source": [
        "df6 = df5.drop(columns=['f_refl_nafta'])\n",
        "calcula_vif(df6,target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCMhFpoxosGP"
      },
      "source": [
        "Como a VIF ficou novamente abaixo de 5 vamos parar a remoção de variáveis nessa etapa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0xpY_jSosGQ"
      },
      "source": [
        "Novamente a VIF se estabelece próximo de 5 e manteremos assim o dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx_sNOk8osGQ"
      },
      "source": [
        "## Separação e divisão do dataset entre features (X) e target (y)\n",
        "\n",
        "Importante etapa, principalmente pensando em etapas futuras de modelagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82kzmWnHFkb6"
      },
      "outputs": [],
      "source": [
        "# Separar features (X) e target (y)\n",
        "X = df6.drop(target, axis=1)\n",
        "y = df6[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzxbuTbIeuuU"
      },
      "outputs": [],
      "source": [
        "# Dividir os dados em conjuntos de treino e teste (sem stratify, pois é regressão)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPvN4eP8ewUB",
        "outputId": "e844d065-88f3-464b-cafb-a649d8ab2675"
      },
      "outputs": [],
      "source": [
        "print(f'Resumo: teremos {X.shape[1]} regressores (features) e {X.shape[0]} observações')\n",
        "\n",
        "print(f\"\\nDimensões de X_train: {X_train.shape[0]} observações e {X_train.shape[1]} regressores (features)\")\n",
        "print(f\"Dimensões de X_test: {X_test.shape[0]} observações e {X_test.shape[1]} regressores (features)\")\n",
        "print(f\"Dimensões de y_train: {y_train.shape[0]} observações\")\n",
        "print(f\"Dimensões de y_test: {y_test.shape[0]} observações\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCFh-1pJosGQ"
      },
      "source": [
        "Desta forma teremos 661 observações para treinamento e 284 para teste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wQ9wP_zGJkk"
      },
      "source": [
        "## Normalização\n",
        "\n",
        "A normalização de dados redimensiona os valores para um intervalo comum, tipicamente entre 0 e 1, preservando a distribuição original e todas as relações entre os valores originais.\n",
        "\n",
        "Nesse estudo temos temperaturas com centenas de unidades, pressão com dezenas de unidades, vazões milhares de unidades e razão de refluxo entre 0 e poucas dezenas.\n",
        "\n",
        "Sem normalização, algoritmos baseados em distância ou gradiente dariam peso excessivo às variáveis com valores numericamente maiores, mesmo que não sejam mais importantes para prever a pressão de vapor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHrDH13DHDDU"
      },
      "outputs": [],
      "source": [
        "# Inicializar o MinMaxScaler\n",
        "scaler_norm = MinMaxScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WULXU6wBezB5"
      },
      "outputs": [],
      "source": [
        "# Aprende min e max APENAS de X_train\n",
        "scaler_norm.fit(X_train)\n",
        "X_train_normalized = scaler_norm.transform(X_train)\n",
        "# Usa a média e o desvio padrão aprendidos de X_train\n",
        "X_test_normalized = scaler_norm.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "id": "-fHBl47Ve0NM",
        "outputId": "1f74c960-6447-4104-f3f4-b2e3123d8e2c"
      },
      "outputs": [],
      "source": [
        "# Exibir as primeiras linhas dos dados normalizados (como DataFrame para melhor visualização)\n",
        "df_normalized = pd.DataFrame(X_train_normalized, columns=X_train.columns)\n",
        "df_normalized.head()\n",
        "plot_boxplot_pdf(df_normalized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyb-mWfqHDDU"
      },
      "source": [
        "O histograma das variáveis independentes após a normalização mostra que os valores foram escalados para o intervalo de 0 a 1, mantendo a forma da distribuição original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOSAwBz0GOAM"
      },
      "source": [
        "## Padronização\n",
        "\n",
        "A padronização (ou Z-score scaling) transforma os dados para ter média 0 e desvio padrão 1. É útil para algoritmos que são sensíveis à escala das características, como SVMs ou redes neurais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08rFvjoEGxb_"
      },
      "outputs": [],
      "source": [
        "# Inicializar o StandardScaler\n",
        "scaler_std = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxKYatnGe96h"
      },
      "outputs": [],
      "source": [
        "# Aprende média e desvio padrão APENAS de X_train\n",
        "scaler_std.fit(X_train)\n",
        "X_train_standardized = scaler_std.transform(X_train)\n",
        "# Usa a média e o desvio padrão aprendidos de X_train\n",
        "X_test_standardized = scaler_std.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5wlJdHKfDdp"
      },
      "outputs": [],
      "source": [
        "# Exibir as primeiras linhas dos dados padronizados (como DataFrame para melhor visualização)\n",
        "df_standardized = pd.DataFrame(X_train_standardized, columns=X_train.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "fuEUapx7fEzE",
        "outputId": "c858c1a1-0764-4d16-b263-57d6ec528fe9"
      },
      "outputs": [],
      "source": [
        "print(\"\\nPrimeiras 5 linhas dos dados padronizados (treino):\")\n",
        "df_standardized.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 942
        },
        "id": "fyfnu7jSGxcA",
        "outputId": "70315662-bed3-44fe-ec1d-9c0d42fd6023"
      },
      "outputs": [],
      "source": [
        "plot_boxplot_pdf(df_standardized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmeMPbfhGxcA"
      },
      "source": [
        "O histograma das variáveis independentes após a padronização mostra que os valores foram transformados para ter uma média próxima de zero e um desvio padrão de um, centralizando a distribuição."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY_GDjf1G-PM"
      },
      "source": [
        "# Conclusão\n",
        "\n",
        "O presente estudo demonstrou a importância de um processo rigoroso de análise exploratória, tratamento e pré-processamento dos dados para problemas de regressão em ambientes industriais. A partir do dataset de Pressão de Vapor da Nafta, foi possível identificar, tratar e justificar a remoção de outliers, realizar imputação de valores faltantes, criar variáveis derivadas relevantes (como a razão de refluxo), além de aplicar técnicas de transformação para redução de assimetrias e multicolinearidade.\n",
        "\n",
        "A análise estatística e visual das variáveis, aliada ao conhecimento de domínio, permitiu selecionar os atributos mais representativos para a modelagem preditiva, reduzindo redundâncias e otimizando o conjunto de dados para futuros algoritmos de machine learning. As etapas de normalização e padronização garantiram que as diferentes escalas das variáveis não influenciassem indevidamente o desempenho dos modelos.\n",
        "\n",
        "## Discussão das hipóteses\n",
        "\n",
        "As hipóteses levantadas ao longo do trabalho foram cuidadosamente avaliadas:\n",
        "\n",
        "- **H1:** Não podemos afirma que a temperatura no topo da fracionadora de nafta apresente significante correlação linear com a pressão de vapor. No entanto, não podemos descartar que haja algum tipo de relação não linear, a ser confirmada na próxima etapa de modelagem, quando novas relações podem ser descobertas e indicar esse atributo como um regressor iteressante para a pressão de vapor da nafta.\n",
        "- **H2:** A pressão de topo da fracionadora mostrou-se moderado pelo critério de Cohen, apesar de estar bem no limite inferior da faixa, alinhando-se com o conhecimento de processo.\n",
        "- **H3:** A razão de refluxo, criada a partir das vazões de carga e refluxo, demonstrou ser uma variável derivada de forte poder explicativo, sintetizando o efeito operacional sobre a pressão de vapor.\n",
        "- **H4:** A temperatura interna da fracionadora de nafta também apresentou forte correlação com a pressão de vapor. A seleção criteriosa das variáveis mais representativas foi fundamental para evitar redundância (multicolinearidade).\n",
        "- **H5 e H6:** As variáveis relacionadas ao forno de aquecimento (temperatura e energia térmica) apresentaram fraca correlação linear com a variável dependente, sugerindo que haja pouco ou nenhum efeito de craqueamento térmico dentro da linearidade, o que não exclui uma possível relação mais complexa.\n",
        "- **H7:** As temperaturas do topo e do condensador da fracionadora de petróleo também mostraram fraca correlação com a pressão de vapor da nafta, novamente sugerindo que haja pouco ou nenhum efeito na qualidade da carga da fracionadora de nafta dentro da linearidade, o que não exclui uma possível relação mais complexa.\n",
        "- **H8:** Vazão de petróleo apresentou uma correlação que pode ser considerada forte com a pressão de vapor, um pouco diferente do calor no refluxo superior que apresentou fraca correlação. De qualquer forma, importante notar a importância do balanço energético e material no processo.\n",
        "\n",
        "De modo geral, as hipóteses foram validadas ou parcialmente confirmadas, evidenciando que a pressão de vapor da nafta é resultado de uma complexa interação entre variáveis operacionais e condições a montante do processo. O estudo reforça a necessidade de uma abordagem multidisciplinar, combinando análise estatística, conhecimento de processo e boas práticas de ciência de dados para obtenção de insights robustos e aplicáveis à realidade industrial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkFdqJMr2He6"
      },
      "source": [
        "# Apêndice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_ep0LGjosGR"
      },
      "source": [
        "## Regressão linear dos dados pre-processador\n",
        "\n",
        "Vamos realizar um exercício de regração linear com regularização Lasso (Least Absolute Shrinkage and Selection Operator). Esta é uma técnica de regularização L1 que possui a propriedade específica de forçar alguns coeficientes a serem exatamente zero, funcionando efetivamente como um método de seleção automática de variáveis.\n",
        "\n",
        "A regressão Lasso minimiza a seguinte função de custo:\n",
        "\n",
        "$$\\text{Minimizar: } \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\alpha \\sum_{j=1}^{p} |\\beta_j|$$\n",
        "\n",
        "onde:\n",
        "\n",
        "O primeiro termo é a soma dos erros quadráticos (como na regressão linear normal)<br>\n",
        "O segundo termo é a penalidade L1, com $\\alpha$ controlando a força da regularização<br>\n",
        "$|\\beta_j|$ representa o valor absoluto dos coeficientes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daAIGOMTosGS"
      },
      "source": [
        "> Obs.: *imports* e declaração da função a seguir não foram realizados no início do código (conforme boas práticas da PEP 8) por fazer parte de uma avaliação extra, além dos requisitos do MVP. Se estivesse de fato no framework seriam deslocados o início, para ter apenas uma sessão de importação e declaração de funções."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng8noSBjosGS"
      },
      "outputs": [],
      "source": [
        "# imports adicionais\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV42Es_NosGS"
      },
      "outputs": [],
      "source": [
        "# Função para gerar e avaliar um modelo linear c/ regularização LASSO\n",
        "\n",
        "def avalia_Lasso(y_train, y_test, X_train, X_train_mod, X_test_mod, alpha=0.1):\n",
        "    \"\"\"\n",
        "    Realiza regressão linear com regularização LASSO e avalia seu desempenho.\n",
        "\n",
        "    Parâmetros:\n",
        "    -----------\n",
        "    y_train : pandas.Series\n",
        "        Series contendo os dados da variável dependente no conjunto de treino.\n",
        "    y_test : pandas.Series\n",
        "        Series contendo os dados da variável dependente no conjunto de teste.\n",
        "    X_train : pandas.DatraFrame\n",
        "        Dataframe contendo as variáveis regressoras no conjunto de treino.\n",
        "    X_train_mod : numpy.ndarray\n",
        "        Array contendo as variáveis regressoras no conjunto de treino.\n",
        "    X_test_mod : numpy.ndarray\n",
        "        Array contendo as variáveis regressoras no conjunto de teste.\n",
        "    alpha : float ou None, opcional\n",
        "        Número que representa a força de regularização do Lasso.\n",
        "\n",
        "    Descrição:\n",
        "    ----------\n",
        "    - Realiza a regressão.\n",
        "    - Gera a os valores de predição do conjunto de treino e teste.\n",
        "    - Calcula as métricas da regressão (MSE, R2 e MAE).\n",
        "    - Apresenta os coeficientes linear obtidos no modelo.\n",
        "    - Plota scatter plot dos valores reais e preditos para os dados de treino e teste.\n",
        "    - Plota a série temporal valores reais e preditos para os dados de treino e teste.\n",
        "    - Plota gráfico dos resíduos da regressão para os dados de treino e teste.\n",
        "\n",
        "    Exemplo de uso:\n",
        "    --------------\n",
        "    avalia_Lasso(y_train, y_test, X_train, X_train_normalized, X_test_normalized)\n",
        "    avalia_Lasso(y_train, y_test, X_train, X_train_normalized, X_test_normalized, alpha=0.15)\n",
        "    \"\"\"\n",
        "\n",
        "    # Inicializa e treina um modelo de regressão Lasso para prever a pressão de vapor (pv_nafta) com os dados.\n",
        "    funcao = Lasso(alpha=alpha)\n",
        "    funcao.fit(X_train_mod, y_train)\n",
        "\n",
        "    # Realiza previsões nos conjuntos de treinamento e teste\n",
        "    y_train_pred = funcao.predict(X_train_mod)\n",
        "    y_test_pred = funcao.predict(X_test_mod)\n",
        "\n",
        "    # Calcula o desempenho do modelo usando métricas como RMSE, R² e MAE\n",
        "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "    train_rmse = np.sqrt(train_mse)\n",
        "    test_rmse = np.sqrt(test_mse)\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "    # Impressão do desempenho do modelo\n",
        "    print(f\"Desempenho da regressão LASSO (alpha={alpha}):\")\n",
        "    print(f\"Treino RMSE: {train_rmse:.2f}\")\n",
        "    print(f\"Teste RMSE: {test_rmse:.2f}\")\n",
        "    print(f\"Treino R²: {train_r2:.3f}\")\n",
        "    print(f\"Teste R²: {test_r2:.3f}\")\n",
        "    print(f\"Treino MAE: {train_mae:.2f}\")\n",
        "    print(f\"Teste MAE: {test_mae:.2f}\")\n",
        "\n",
        "    # Imprime a importância dos regressores (coeficientes)\n",
        "    feature_importance = pd.DataFrame({\n",
        "    'Regressor': X_train.columns,\n",
        "    'Coeficiente': funcao.coef_\n",
        "    })\n",
        "    print(\"\\nCoef. dos Regressoes:\")\n",
        "    print(feature_importance.sort_values(by='Coeficiente', key=abs, ascending=False))\n",
        "\n",
        "    # Gráfico 1: Valores preditos vs reais\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # Dados de treinamento\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(y_train, y_train_pred, alpha=0.5)\n",
        "    plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
        "    plt.xlabel('Valores Reais')\n",
        "    plt.ylabel('Valores Preditos')\n",
        "    plt.title('Dados de treinamento: Real vs Predito')\n",
        "    plt.text(y_train.min() + 5, y_train.max() - 5, f'R² = {train_r2:.4f}\\nRMSE = {train_rmse:.2f}')\n",
        "\n",
        "    # Dados de teste\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "    plt.xlabel('Valores Reais')\n",
        "    plt.ylabel('Valores Preditos')\n",
        "    plt.title('Dados de teste: Real vs Predito')\n",
        "    plt.text(y_test.min() + 5, y_test.max() - 5, f'R² = {test_r2:.4f}\\nRMSE = {test_rmse:.2f}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Gráfico 2: Série temporal das previsões\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # Combinalção dos índices de treino e teste para plotagem\n",
        "    train_indices = np.arange(len(y_train))\n",
        "    test_indices = np.arange(len(y_train), len(y_train) + len(y_test))\n",
        "\n",
        "    # Gráfico dos dados de treino\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(train_indices, y_train.values, 'b-', label='Real')\n",
        "    plt.plot(train_indices, y_train_pred, 'r-', label='Predito')\n",
        "    plt.title('Dados de treino: Série Temporal dos valores Reais vs Preditos')\n",
        "    plt.ylabel('Pressão de vapor (pv_nafta)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Gráfico dos dados de teste\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(test_indices, y_test.values, 'b-', label='Real')\n",
        "    plt.plot(test_indices, y_test_pred, 'r-', label='Predito')\n",
        "    plt.title('Dados de teste: Série Temporal dos valores Reais vs Preditos')\n",
        "    plt.xlabel('Índice da amostra')\n",
        "    plt.ylabel('Pressão de vapor (pv_nafta)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Gráfico 3: Análise de resíduos\n",
        "    residuals_train = y_train - y_train_pred\n",
        "    residuals_test = y_test - y_test_pred\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # Resíduos do conjunto de treinamento\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(y_train_pred, residuals_train, alpha=0.5)\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.xlabel('Valores Preditos')\n",
        "    plt.ylabel('Resíduos')\n",
        "    plt.title('Dados de treino: Valores Residuais vs Preditos')\n",
        "\n",
        "    # Resíduos do conjunto de teste\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(y_test_pred, residuals_test, alpha=0.5)\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.xlabel('Valores Preditos')\n",
        "    plt.ylabel('Resíduos')\n",
        "    plt.title('Dados de teste: Valores Residuais vs Preditos')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ygClssqhosGS",
        "outputId": "e776b30a-66bc-44a5-83f7-5f2a9b70e759"
      },
      "outputs": [],
      "source": [
        "avalia_Lasso(y_train, y_test, X_train, X_train_normalized, X_test_normalized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLcSV2UuosGS"
      },
      "source": [
        "Apenas com uma regressão linear multivariável com os dados normalizados já foi possível obter um modelo capaz de explicar quase 70% da variância do processo, o que podemos considerar um bom ponto de partida considerando as perspectivas futuras com machine learning.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sRNISEvSosGS",
        "outputId": "4271a9ca-c99d-4b56-c537-d2f8bdc53d18"
      },
      "outputs": [],
      "source": [
        "avalia_Lasso(y_train, y_test, X_train, X_train_standardized, X_test_standardized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG1P14bPosGS"
      },
      "source": [
        "Da mesma fora que com os dados normalizado, com os dados padronizados obtivemos um modelo linear multivariável com capacidade de explicar aproximadamente 70% da variância do processo. É importante notar que apesar de marginal, os resultados dos dados padronizados foram ainda melhores que com os dados normalizados (maior $R^2$ / menores erros), o que abre uma perspectiva de exploração dos diversos métodos de escalonamento como um possível hiperparâmetro de ajuste dos modelos futuros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_HLVWAZosGS"
      },
      "source": [
        "### Conclusão\n",
        "\n",
        "No contexto industrial, para dados de processo de refino de petróleo, que naturalmente apresentam ruído, variabilidade operacional e complexas interações físico-químicas, explicar aproximadamente 70% da variância é considerado um bom resultado. O modelo captura as principais relações entre as variáveis de processo e a pressão de vapor, permitindo predições com precisão razoável para aplicações como monitoramento de qualidade e suporte à decisão operacional. Já a variabilidade não explicada (~30%) pode incluir fatores como dinâmica não capturada do processo, limitações de instrumentação, e flutuações nas propriedades da matéria-prima\n",
        "\n",
        "Este modelo linear LASSO representa um ponto de partida para a próxima *SPRINT*, mas já poderia ser suficiente para muitas aplicações práticas, especialmente considerando seu equilíbrio favorável entre precisão, interpretabilidade e facilidade de implementação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnglRbcl0P1m"
      },
      "source": [
        "## Equacionamento da transformação Yeo-Johnson\n",
        "\n",
        "Apesar de usarmos importação de pacotes que já disponibilizam essa transformação em poucos passos, é importante ter em mente a formulação dessas transformações, pois em caso de deploy do modelo, faz-se necessária a tranformação dos dados antes do consumo dos modelos gerados.\n",
        "\n",
        "### Fórmula geral da transformação Yeo-Johnson\n",
        "\n",
        "Transformação Direta: \n",
        "\n",
        "$$x_t = f(x, \\lambda)$$\n",
        "\n",
        "Transformação Inversa: \n",
        "\n",
        "$$x = f^{-1}(x_t, \\lambda)$$\n",
        "\n",
        "Onde:\n",
        "\n",
        "$x_t$ é o valor transformado<br>\n",
        "$x$ é o valor original<br>\n",
        "$\\lambda$ é o parâmetro da transformação Yeo-Johnson<br>\n",
        "$f^{-1}$ denota a função inversa de $f$\n",
        "\n",
        "Fórmulas Específicas\n",
        "\n",
        "Transformação Direta: $x_t = f(x, \\lambda)$\n",
        "\n",
        "Para $x \\geq 0$:\n",
        "$$f(x, \\lambda) =\n",
        "\\begin{cases}\n",
        "\\frac{(x+1)^{\\lambda} - 1}{\\lambda}, & \\text{se } \\lambda \\neq 0 \\\\\n",
        "\\log(x+1), & \\text{se } \\lambda = 0\n",
        "\\end{cases}$$\n",
        "\n",
        "Para $x < 0$:\n",
        "$$f(x, \\lambda) =\n",
        "\\begin{cases}\n",
        "-\\frac{(-x+1)^{2-\\lambda} - 1}{2-\\lambda}, & \\text{se } \\lambda \\neq 2 \\\\\n",
        "-\\log(-x+1), & \\text{se } \\lambda = 2\n",
        "\\end{cases}$$\n",
        "\n",
        "Transformação Inversa: $x = f^{-1}(x_t, \\lambda)$\n",
        "\n",
        "Para valores originalmente não-negativos (valores $x_t$ resultantes da transformação de $x \\geq 0$):\n",
        "$$f^{-1}(x_t, \\lambda) =\n",
        "\\begin{cases}\n",
        "(1 + \\lambda x_t)^{1/\\lambda} - 1, & \\text{se } \\lambda \\neq 0 \\\\\n",
        "e^{x_t} - 1, & \\text{se } \\lambda = 0\n",
        "\\end{cases}$$\n",
        "\n",
        "Para valores originalmente negativos (valores $x_t$ resultantes da transformação de $x < 0$):\n",
        "$$f^{-1}(x_t, \\lambda) =\n",
        "\\begin{cases}\n",
        "-[(1 - (2-\\lambda)x_t)^{1/(2-\\lambda)} - 1], & \\text{se } \\lambda \\neq 2 \\\\\n",
        "-(e^{-x_t} - 1), & \\text{se } \\lambda = 2\n",
        "\\end{cases}$$"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
